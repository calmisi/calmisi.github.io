<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[kvm对MSR寄存器的处理（三）]]></title>
    <url>%2Fpost%2F2019%2F1%2Fkvm%E5%AF%B9MSR%E5%AF%84%E5%AD%98%E5%99%A8%E7%9A%84%E5%A4%84%E7%90%86%EF%BC%88%E4%B8%89%EF%BC%89.html</url>
    <content type="text"><![CDATA[msrs_to_save[], emulated_msrs[], msr_based_features[]在x86.c文件中定一个了msrs_to_save[], emulated_msrs[], msr_based_features[]三个保存MSR寄存器index的数组，以及num_msrs_to_save, num_emulated_msrs, num_msr_based_features三个静态全局变量来表示以上三个数组的大小。 根据KVM的注释，msr_to_save[]数组，会在kvm.ko模块加载的时候，根据实际物理cpu的capabilities来调整；emulated_msrs[]数组的调整依赖于虚拟化的实现，而不是物理CPU feature,所以把它们放在两个数组里面，我们可以通过下一小节的描述看出差别；msr_based_features[]数组存储msr-based features相关的MSR，hypervisor使用这些MSR来查询相关的CPU features。 注意：CPU features enumeration一般是通过CPUID，即通过CPUID指令查询当前的物理CPU是否有某些feature。而MSR则是用来控制某个feature的开关。一般地，先通过CPUID指令具体的一个bit位来查询某个feature是否存在，如果存在，则set/clear MSR的bit来enable/disable这个feature。但是有一部分feature是通过MSR来enumerated的，为MSR-specific features,比如VMX相关的feature, MSR, IA32_ARCH_CAPABILITIES等。即通过MSR的bit来表示CPU是否有某些features。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394/* * List of msr numbers which we expose to userspace through KVM_GET_MSRS * and KVM_SET_MSRS, and KVM_GET_MSR_INDEX_LIST. * * This list is modified at module load time to reflect the * capabilities of the host cpu. This capabilities test skips MSRs that are * kvm-specific. Those are put in emulated_msrs; filtering of emulated_msrs * may depend on host virtualization features rather than host cpu features. */static u32 msrs_to_save[] = &#123; MSR_IA32_SYSENTER_CS, MSR_IA32_SYSENTER_ESP, MSR_IA32_SYSENTER_EIP, MSR_STAR,#ifdef CONFIG_X86_64 MSR_CSTAR, MSR_KERNEL_GS_BASE, MSR_SYSCALL_MASK, MSR_LSTAR,#endif MSR_IA32_TSC, MSR_IA32_CR_PAT, MSR_VM_HSAVE_PA, MSR_IA32_FEATURE_CONTROL, MSR_IA32_BNDCFGS, MSR_TSC_AUX, MSR_IA32_SPEC_CTRL, MSR_IA32_ARCH_CAPABILITIES, MSR_IA32_RTIT_CTL, MSR_IA32_RTIT_STATUS, MSR_IA32_RTIT_CR3_MATCH, MSR_IA32_RTIT_OUTPUT_BASE, MSR_IA32_RTIT_OUTPUT_MASK, MSR_IA32_RTIT_ADDR0_A, MSR_IA32_RTIT_ADDR0_B, MSR_IA32_RTIT_ADDR1_A, MSR_IA32_RTIT_ADDR1_B, MSR_IA32_RTIT_ADDR2_A, MSR_IA32_RTIT_ADDR2_B, MSR_IA32_RTIT_ADDR3_A, MSR_IA32_RTIT_ADDR3_B,&#125;;static unsigned num_msrs_to_save;static u32 emulated_msrs[] = &#123; MSR_KVM_SYSTEM_TIME, MSR_KVM_WALL_CLOCK, MSR_KVM_SYSTEM_TIME_NEW, MSR_KVM_WALL_CLOCK_NEW, HV_X64_MSR_GUEST_OS_ID, HV_X64_MSR_HYPERCALL, HV_X64_MSR_TIME_REF_COUNT, HV_X64_MSR_REFERENCE_TSC, HV_X64_MSR_TSC_FREQUENCY, HV_X64_MSR_APIC_FREQUENCY, HV_X64_MSR_CRASH_P0, HV_X64_MSR_CRASH_P1, HV_X64_MSR_CRASH_P2, HV_X64_MSR_CRASH_P3, HV_X64_MSR_CRASH_P4, HV_X64_MSR_CRASH_CTL, HV_X64_MSR_RESET, HV_X64_MSR_VP_INDEX, HV_X64_MSR_VP_RUNTIME, HV_X64_MSR_SCONTROL, HV_X64_MSR_STIMER0_CONFIG, HV_X64_MSR_VP_ASSIST_PAGE, HV_X64_MSR_REENLIGHTENMENT_CONTROL, HV_X64_MSR_TSC_EMULATION_CONTROL, HV_X64_MSR_TSC_EMULATION_STATUS, MSR_KVM_ASYNC_PF_EN, MSR_KVM_STEAL_TIME, MSR_KVM_PV_EOI_EN, MSR_IA32_TSC_ADJUST, MSR_IA32_TSCDEADLINE, MSR_IA32_MISC_ENABLE, MSR_IA32_MCG_STATUS, MSR_IA32_MCG_CTL, MSR_IA32_MCG_EXT_CTL, MSR_IA32_SMBASE, MSR_SMI_COUNT, MSR_PLATFORM_INFO, MSR_MISC_FEATURES_ENABLES, MSR_AMD64_VIRT_SPEC_CTRL,&#125;;static unsigned num_emulated_msrs;/* * List of msr numbers which are used to expose MSR-based features that * can be used by a hypervisor to validate requested CPU features. */static u32 msr_based_features[] = &#123; MSR_IA32_VMX_BASIC, MSR_IA32_VMX_TRUE_PINBASED_CTLS, MSR_IA32_VMX_PINBASED_CTLS, MSR_IA32_VMX_TRUE_PROCBASED_CTLS, MSR_IA32_VMX_PROCBASED_CTLS, MSR_IA32_VMX_TRUE_EXIT_CTLS, MSR_IA32_VMX_EXIT_CTLS, MSR_IA32_VMX_TRUE_ENTRY_CTLS, MSR_IA32_VMX_ENTRY_CTLS, MSR_IA32_VMX_MISC, MSR_IA32_VMX_CR0_FIXED0, MSR_IA32_VMX_CR0_FIXED1, MSR_IA32_VMX_CR4_FIXED0, MSR_IA32_VMX_CR4_FIXED1, MSR_IA32_VMX_VMCS_ENUM, MSR_IA32_VMX_PROCBASED_CTLS2, MSR_IA32_VMX_EPT_VPID_CAP, MSR_IA32_VMX_VMFUNC, MSR_F10H_DECFG, MSR_IA32_UCODE_REV, MSR_IA32_ARCH_CAPABILITIES,&#125;;static unsigned int num_msr_based_features; 调整msrs_to_save[], emulated_msrs[], msr_based_features[]在加载kvm.ko模块的时候，调用kvm_init() =&gt; kvm_arch_hardware_setup() =&gt; kvm_init_msr_list().在kvm_init_msr_list()函数中,对上面提到的三个数组进行调整。 msr_to_save[]:首先调用rdmsr_safe()去验证MSR是否在物理CPU上支持，然后对部分MSR调用具体的虚拟化的kvm_x86_ops来查询对应的MSR是否应该被支持。通过以上两步筛选后，用num_msrs_to_save变量保存过滤后的数组大小。 emulated_msrs[]:统一调用kvm_x86_ops-&gt;has_emulated_msr()函数来过滤每一个MSR，用num_emulated_msrs来保存过滤后的大小。 msr_based_features[]:使用kvm_get_msr_feature()来过滤，用num_msr_based_features来保存过滤后的大小。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879static void kvm_init_msr_list(void)&#123; u32 dummy[2]; unsigned i, j; for (i = j = 0; i &lt; ARRAY_SIZE(msrs_to_save); i++) &#123; if (rdmsr_safe(msrs_to_save[i], &amp;dummy[0], &amp;dummy[1]) &lt; 0) continue; /* * Even MSRs that are valid in the host may not be exposed * to the guests in some cases. */ switch (msrs_to_save[i]) &#123; case MSR_IA32_BNDCFGS: if (!kvm_mpx_supported()) continue; break; case MSR_TSC_AUX: if (!kvm_x86_ops-&gt;rdtscp_supported()) continue; break; case MSR_IA32_RTIT_CTL: case MSR_IA32_RTIT_STATUS: if (!kvm_x86_ops-&gt;pt_supported()) continue; break; case MSR_IA32_RTIT_CR3_MATCH: if (!kvm_x86_ops-&gt;pt_supported() || !intel_pt_validate_hw_cap(PT_CAP_cr3_filtering)) continue; break; case MSR_IA32_RTIT_OUTPUT_BASE: case MSR_IA32_RTIT_OUTPUT_MASK: if (!kvm_x86_ops-&gt;pt_supported() || (!intel_pt_validate_hw_cap(PT_CAP_topa_output) &amp;&amp; !intel_pt_validate_hw_cap(PT_CAP_single_range_output))) continue; break; case MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B: &#123; if (!kvm_x86_ops-&gt;pt_supported() || msrs_to_save[i] - MSR_IA32_RTIT_ADDR0_A &gt;= intel_pt_validate_hw_cap(PT_CAP_num_address_ranges) * 2) continue; break; &#125; default: break; &#125; if (j &lt; i) msrs_to_save[j] = msrs_to_save[i]; j++; &#125; num_msrs_to_save = j; for (i = j = 0; i &lt; ARRAY_SIZE(emulated_msrs); i++) &#123; if (!kvm_x86_ops-&gt;has_emulated_msr(emulated_msrs[i])) continue; if (j &lt; i) emulated_msrs[j] = emulated_msrs[i]; j++; &#125; num_emulated_msrs = j; for (i = j = 0; i &lt; ARRAY_SIZE(msr_based_features); i++) &#123; struct kvm_msr_entry msr; msr.index = msr_based_features[i]; if (kvm_get_msr_feature(&amp;msr)) continue; if (j &lt; i) msr_based_features[j] = msr_based_features[i]; j++; &#125; num_msr_based_features = j;&#125;]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>MSR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QEMU对MSR的处理（一）]]></title>
    <url>%2Fpost%2F2019%2F1%2FQEMU%E5%AF%B9MSR%E7%9A%84%E5%A4%84%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89.html</url>
    <content type="text"><![CDATA[在kvm-all.c里面的kvm_init()函数，会调用kvm_arch_init()函数， kvm_get_supported_msrs(); kvm_get_supported_feature_msrs(); 1. kvm_get_supported_msrs() QEMU会使用kvm_ioctl(KVM_GET_MSR_INDEX_LIST)，然后KVM会返回当前kvm模块支持的msr_to_save[]数组和emulated_msr[]数组（参见KVM对MSR寄存器的处理（三））给QEMU。 但QEMU并不保存这些MSR的内容，而是根据返回的支持的MSR来设置一些全局的静态变量来表示是否有对应的MSR。 如has_msr_star, has_msr_hsave_pa, … , has_msr_arch_capabs等，这里QEMU是记录其关心的MSR，应该有部分KVM返回的MSR是没有用的（本人没有统计）。 2. kvm_get_supported_feature_msrs() QEMU首先kvm_check_extension(KVM_CAP_GET_MSR_FEATURES)来查询KVM是否支持查询KVM_GET_MSR_FEATURE_INDEX_LIST; 然后调用kvm_ioctl(KVM_GET_MSR_FEATURE_INDEX_LIST)获取kvm的msr_based_features[]数组的大小； 然后为全局变量kvm_feature_msrs申请对需要的空间来存储从KVM获得msr_based_features[]数组； 并再一次调用kvm_ioctl(KVM_GET_MSR_FEATURE_INDEX_LIST)，最后将获取的msr_based_features[]数组存到全局变量kvm_feature_msrs变量中。 注意：我们可以看到，kvm_get_supported_msrs()获取到相应的msr数组后，没有保存对应的index,而是取其需要的msr来设置对应的全局标志变量,不一定会用到所有的KVM返回的MSR；而kvm_get_supported_feature_msrs()会获取KVM的msr_based_features[]数组，并且不做过滤，直接保存。 3. kvm_arch_get_supported_msr_feature(KVMState *s, uint32_t index)该函数负责向KVM获取指定的feature MSR的值。 检查全局变量kvm_feature_msrs是否为NULL，即为上一小节中kvm_get_supported_feature_msrs()初始化的；如果为NULL，则说明host不支持feature MSRs;那么直接return 0。 依次遍历kvm_feature_msrs中每一个MSR，找到目标index所对应的MSR，如果在kvm_feature_msrs中没有，那么直接return 0。 到了这一步，说明需要查询的feature MSR存在，即KVM支持；那么调用kvm_ioctl(KVM_GET_MSRS)获取该MSR的值。 4. x86_cpu_get_supported_feature_word(FeatureWord w, bool migratable_only)]]></content>
      <categories>
        <category>QEMU</category>
      </categories>
      <tags>
        <tag>MSR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kvm对guest中MSR寄存器的处理（二）-- rdmsr]]></title>
    <url>%2Fpost%2F2019%2F1%2Fkvm%E5%AF%B9guest%E4%B8%ADMSR%E5%AF%84%E5%AD%98%E5%99%A8%E7%9A%84%E5%A4%84%E7%90%86%EF%BC%88%E4%BA%8C%EF%BC%89-rdmsr.html</url>
    <content type="text"><![CDATA[本文为kvm操作MSR寄存器的第二部分，介绍guest中rdmsr指令的处理过程。之前在（一）中有介绍，如果对guest设置了Use MSR bitmaps VM-execution control bit,那么Read bitmap for low MSRs(0000 0000H - 0000 1FFFH)和Read bitmap for high MSRs(C000 0000H - C000 1FFFH)范围中的为0的bit对应的地址的MSR在执行rdmsr的时候不会产生VM EXIT。其他情况，运行rdmsr均会触发VM exit,最后会进入vmx.c中的handle_rdmsr()函数。 handle_rdmsr如果guest vm exit时的exit reason为31号，即为rdmsr指令产生的vm exit。具体到最后会调用handle_rdmsr()，其函数实现如下。1234567891011121314151617181920static int handle_rdmsr(struct kvm_vcpu *vcpu)&#123; u32 ecx = vcpu-&gt;arch.regs[VCPU_REGS_RCX]; struct msr_data msr_info; msr_info.index = ecx; msr_info.host_initiated = false; if (vmx_get_msr(vcpu, &amp;msr_info)) &#123; trace_kvm_msr_read_ex(ecx); kvm_inject_gp(vcpu, 0); return 1; &#125; trace_kvm_msr_read(ecx, msr_info.data); /* FIXME: handling of bits 32:63 of rax, rdx */ vcpu-&gt;arch.regs[VCPU_REGS_RAX] = msr_info.data &amp; -1u; vcpu-&gt;arch.regs[VCPU_REGS_RDX] = (msr_info.data &gt;&gt; 32) &amp; -1u; return kvm_skip_emulated_instruction(vcpu);&#125; 从vcpu-&gt;arch.regs[]数组里取出guest的RCX寄存器的时，其存放guest rdmsr指令需要读取的MSR的index. 然后将需要读取的MSR的index存入msr_info，并设置msr_info.host_initiated = false; 调用mvx_get_msr(vcpu, &amp;msr_info)在kvm中对guest的rdmsr指令进行模拟； 如果读取失败，则调用kvm_inject_gp()向guest插入一个#GP； 如果读取成功，则将模拟rdmsr后的读到的数据data存入vcpu-&gt;arch.regs[RAX]和vpu-&gt;arch.regs[RDX]，然后更改guest_rip跳过这条指令 当vm entry进入guest后，guest_rip已经指向rdmsr的下一条指令，并且rdmsr读取的值已经正常在GUEST_RAX和GUEST_RDX里面了； 注意其中trace_kvm_msr_read_ex(ecx)和trace_kvm_msr_read(ecx,msr_info.data)是给perf工具的trace用的。我们不讨论这个。 vmx_get_msr()重点是vmx_get_msr()函数，其真正完成了KVM对guest里面的rdmsr指令的模拟。如果其返回0,则表示对rdmsr的模拟是成功的，其返回非0，则表示对rdmsr指令的模拟有问题，要向guest插入一个#GP。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135 * Reads an msr value (of 'msr_index') into 'pdata'. * Returns 0 on success, non-0 otherwise. * Assumes vcpu_load() was already called. */static int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)&#123; struct vcpu_vmx *vmx = to_vmx(vcpu); struct shared_msr_entry *msr; u32 index; switch (msr_info-&gt;index) &#123;#ifdef CONFIG_X86_64 case MSR_FS_BASE: msr_info-&gt;data = vmcs_readl(GUEST_FS_BASE); break; case MSR_GS_BASE: msr_info-&gt;data = vmcs_readl(GUEST_GS_BASE); break; case MSR_KERNEL_GS_BASE: msr_info-&gt;data = vmx_read_guest_kernel_gs_base(vmx); break;#endif case MSR_EFER: return kvm_get_msr_common(vcpu, msr_info); case MSR_IA32_SPEC_CTRL: if (!msr_info-&gt;host_initiated &amp;&amp; !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL)) return 1; msr_info-&gt;data = to_vmx(vcpu)-&gt;spec_ctrl; break; case MSR_IA32_ARCH_CAPABILITIES: if (!msr_info-&gt;host_initiated &amp;&amp; !guest_cpuid_has(vcpu, X86_FEATURE_ARCH_CAPABILITIES)) return 1; msr_info-&gt;data = to_vmx(vcpu)-&gt;arch_capabilities; break; case MSR_IA32_SYSENTER_CS: msr_info-&gt;data = vmcs_read32(GUEST_SYSENTER_CS); break; case MSR_IA32_SYSENTER_EIP: msr_info-&gt;data = vmcs_readl(GUEST_SYSENTER_EIP); break; case MSR_IA32_SYSENTER_ESP: msr_info-&gt;data = vmcs_readl(GUEST_SYSENTER_ESP); break; case MSR_IA32_BNDCFGS: if (!kvm_mpx_supported() || (!msr_info-&gt;host_initiated &amp;&amp; !guest_cpuid_has(vcpu, X86_FEATURE_MPX))) return 1; msr_info-&gt;data = vmcs_read64(GUEST_BNDCFGS); break; case MSR_IA32_MCG_EXT_CTL: if (!msr_info-&gt;host_initiated &amp;&amp; !(vmx-&gt;msr_ia32_feature_control &amp; FEATURE_CONTROL_LMCE)) return 1; msr_info-&gt;data = vcpu-&gt;arch.mcg_ext_ctl; break; case MSR_IA32_FEATURE_CONTROL: msr_info-&gt;data = vmx-&gt;msr_ia32_feature_control; break; case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC: if (!nested_vmx_allowed(vcpu)) return 1; return vmx_get_vmx_msr(&amp;vmx-&gt;nested.msrs, msr_info-&gt;index, &amp;msr_info-&gt;data); case MSR_IA32_XSS: if (!vmx_xsaves_supported()) return 1; msr_info-&gt;data = vcpu-&gt;arch.ia32_xss; break; case MSR_IA32_RTIT_CTL: if (pt_mode != PT_MODE_HOST_GUEST) return 1; msr_info-&gt;data = vmx-&gt;pt_desc.guest.ctl; break; case MSR_IA32_RTIT_STATUS: if (pt_mode != PT_MODE_HOST_GUEST) return 1; msr_info-&gt;data = vmx-&gt;pt_desc.guest.status; break; case MSR_IA32_RTIT_CR3_MATCH: if ((pt_mode != PT_MODE_HOST_GUEST) || !intel_pt_validate_cap(vmx-&gt;pt_desc.caps, PT_CAP_cr3_filtering)) return 1; msr_info-&gt;data = vmx-&gt;pt_desc.guest.cr3_match; break; case MSR_IA32_RTIT_OUTPUT_BASE: if ((pt_mode != PT_MODE_HOST_GUEST) || (!intel_pt_validate_cap(vmx-&gt;pt_desc.caps, PT_CAP_topa_output) &amp;&amp; !intel_pt_validate_cap(vmx-&gt;pt_desc.caps, PT_CAP_single_range_output))) return 1; msr_info-&gt;data = vmx-&gt;pt_desc.guest.output_base; break; case MSR_IA32_RTIT_OUTPUT_MASK: if ((pt_mode != PT_MODE_HOST_GUEST) || (!intel_pt_validate_cap(vmx-&gt;pt_desc.caps, PT_CAP_topa_output) &amp;&amp; !intel_pt_validate_cap(vmx-&gt;pt_desc.caps, PT_CAP_single_range_output))) return 1; msr_info-&gt;data = vmx-&gt;pt_desc.guest.output_mask; break; case MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B: index = msr_info-&gt;index - MSR_IA32_RTIT_ADDR0_A; if ((pt_mode != PT_MODE_HOST_GUEST) || (index &gt;= 2 * intel_pt_validate_cap(vmx-&gt;pt_desc.caps, PT_CAP_num_address_ranges))) return 1; if (index % 2) msr_info-&gt;data = vmx-&gt;pt_desc.guest.addr_b[index / 2]; else msr_info-&gt;data = vmx-&gt;pt_desc.guest.addr_a[index / 2]; break; case MSR_TSC_AUX: if (!msr_info-&gt;host_initiated &amp;&amp; !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP)) return 1; /* Otherwise falls through */ default: msr = find_msr_entry(vmx, msr_info-&gt;index); if (msr) &#123; msr_info-&gt;data = msr-&gt;data; break; &#125; return kvm_get_msr_common(vcpu, msr_info); &#125; return 0;&#125; 我们可以看见，其根据msr_info-&gt;index进行switch进行不同的处理，msr_info-&gt;index的值为guest中执行rdmsr指令是RCX寄存器的值，即为要读的MSR寄存的index。 在该switch()的实现中，我们可以分为以下几类： 直接读取vmcs区域中guest MSR区域 MSR_FS_BASE (X86_64独有): vmcs_readl(GUEST_FS_BASE) MSR_GS_BASE (X86_64独有): vmcs_readl(GUEST_GS_BASE) MSR_KERNEL_GS_BASE (X86_64独有，其处理有点特殊，具体看代码): rdmsrl(MSR_KERNEL_GS_BASE, vmx-&gt;msr_guest_kernel_gs_base) MSR_IA32_SYSENTER_CS: vmcs_read32(GUEST_SYSENTER_CS) MSR_IA32_SYSENTER_RIP: vmcs_readl(GUEST_SYSENTER_EIP) MSR_IA32_SYSENTER_ESP: vmcs_readl(GUEST_SYSENTER_ESP) MSR_IA32_BNDCFGS: vmcs_read64(GUEST_BNDCFGS) 读取strcut vcpu_vmx中保存的值 MSR_IA32_SPEC_CTRL: vmx-&gt;spec_ctrl; MSR_IA32_ARCH_CAPABILITIES: vmx-&gt;arch_capabilities; MSR_IA32_FEATURE_CONTROL: vmx-&gt;msr_ia32_feature_control; MSR_IA32_RTIT_CTL: vmx-&gt;pt_desc.guest.ctl; MSR_IA32_RTIT_STATUS: vmx-&gt;pt_desc.guest.status; MSR_IA32_RTIT_CR3_MATCH: vmx-&gt;pt_desc.guest.cr3_match; MSR_IA32_RTIT_OUTPUT_BASE: vmx-&gt;pt_desc.guest.output_base; MSR_IA32_RTIT_OUTPUT_MASK: vmx-&gt;pt_desc.guest.output_mask; MSR_IA32_RTIT_ADDR0_A … MSR_IA32_RTIT_ADDR3_B: 读取strcut kvm_vcpu-&gt;arch的值 MSR_IA32_MCG_EXT_CTL: vcpu-&gt;arch.mcg_ext_ctrl; MSR_IA32_XSS: vcpu-&gt;arch.ia32_xss; 对nested模式中MSR的模拟，会调用vmx_get_vmx_msr MSR_IA32_VMX_BASIC … MSR_IA32_VMX_VMFUNC 调用find_msr_entry(struct vcpu_vmx * vmx, u32 msr)函数，查找对应的MSR在vmx-&gt;guest_msrs[]数组中是否存在。如果存在，则返回vmx-&gt;guest_msrs[]中对应MSR的值; 如果不存在，则到下一类，使用kvm_get_msr_common();保存在vmx-&gt;guest_msrs[]数组中的值，可以参考上一篇文章，kvm对guest中MSR寄存器的处理（一），其根据vmx_msr_index[]数组初始化，其中可能包括的MSR有： MSR_SYSCALL_MASK (X86_64独有), MSR_LSTAR (X86_64独有), MSR_CSTAR (X86_64独有), MSR_EFER (见下一类使用kvm_get_msr_common()), MSR_TSC_AUX, MSR_STAR 调用kvm_get_msr_common(struct kvm_vcpu * vcpu, struct msr_data * msr_data) MSR_EFER 以及以上5类不包含的MSR kvm_get_msr_common()根据上面对vmx_get_msr()函数的分析，可以发现大多数的MSR都落到了kvm_get_msr_common()这个函数，下面我们来看这个函数,该函数的实现在x86.c中。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)&#123; switch (msr_info-&gt;index) &#123; case MSR_IA32_PLATFORM_ID: case MSR_IA32_EBL_CR_POWERON: case MSR_IA32_DEBUGCTLMSR: case MSR_IA32_LASTBRANCHFROMIP: case MSR_IA32_LASTBRANCHTOIP: case MSR_IA32_LASTINTFROMIP: case MSR_IA32_LASTINTTOIP: case MSR_K8_SYSCFG: case MSR_K8_TSEG_ADDR: case MSR_K8_TSEG_MASK: case MSR_K7_HWCR: case MSR_VM_HSAVE_PA: case MSR_K8_INT_PENDING_MSG: case MSR_AMD64_NB_CFG: case MSR_FAM10H_MMIO_CONF_BASE: case MSR_AMD64_BU_CFG2: case MSR_IA32_PERF_CTL: case MSR_AMD64_DC_CFG: case MSR_F15H_EX_CFG: msr_info-&gt;data = 0; break; case MSR_F15H_PERF_CTL0 ... MSR_F15H_PERF_CTR5: case MSR_K7_EVNTSEL0 ... MSR_K7_EVNTSEL3: case MSR_K7_PERFCTR0 ... MSR_K7_PERFCTR3: case MSR_P6_PERFCTR0 ... MSR_P6_PERFCTR1: case MSR_P6_EVNTSEL0 ... MSR_P6_EVNTSEL1: if (kvm_pmu_is_valid_msr(vcpu, msr_info-&gt;index)) return kvm_pmu_get_msr(vcpu, msr_info-&gt;index, &amp;msr_info-&gt;data); msr_info-&gt;data = 0; break; case MSR_IA32_UCODE_REV: msr_info-&gt;data = vcpu-&gt;arch.microcode_version; break; case MSR_IA32_TSC: msr_info-&gt;data = kvm_scale_tsc(vcpu, rdtsc()) + vcpu-&gt;arch.tsc_offset; break; case MSR_MTRRcap: case 0x200 ... 0x2ff: return kvm_mtrr_get_msr(vcpu, msr_info-&gt;index, &amp;msr_info-&gt;data); case 0xcd: /* fsb frequency */ msr_info-&gt;data = 3; break; /* * MSR_EBC_FREQUENCY_ID * Conservative value valid for even the basic CPU models. * Models 0,1: 000 in bits 23:21 indicating a bus speed of * 100MHz, model 2 000 in bits 18:16 indicating 100MHz, * and 266MHz for model 3, or 4. Set Core Clock * Frequency to System Bus Frequency Ratio to 1 (bits * 31:24) even though these are only valid for CPU * models &gt; 2, however guests may end up dividing or * multiplying by zero otherwise. */ case MSR_EBC_FREQUENCY_ID: msr_info-&gt;data = 1 &lt;&lt; 24; break; case MSR_IA32_APICBASE: msr_info-&gt;data = kvm_get_apic_base(vcpu); break; case APIC_BASE_MSR ... APIC_BASE_MSR + 0x3ff: return kvm_x2apic_msr_read(vcpu, msr_info-&gt;index, &amp;msr_info-&gt;data); break; case MSR_IA32_TSCDEADLINE: msr_info-&gt;data = kvm_get_lapic_tscdeadline_msr(vcpu); break; case MSR_IA32_TSC_ADJUST: msr_info-&gt;data = (u64)vcpu-&gt;arch.ia32_tsc_adjust_msr; break; case MSR_IA32_MISC_ENABLE: msr_info-&gt;data = vcpu-&gt;arch.ia32_misc_enable_msr; break; case MSR_IA32_SMBASE: if (!msr_info-&gt;host_initiated) return 1; msr_info-&gt;data = vcpu-&gt;arch.smbase; break; case MSR_SMI_COUNT: msr_info-&gt;data = vcpu-&gt;arch.smi_count; break; case MSR_IA32_PERF_STATUS: /* TSC increment by tick */ msr_info-&gt;data = 1000ULL; /* CPU multiplier */ msr_info-&gt;data |= (((uint64_t)4ULL) &lt;&lt; 40); break; case MSR_EFER: msr_info-&gt;data = vcpu-&gt;arch.efer; break; case MSR_KVM_WALL_CLOCK: case MSR_KVM_WALL_CLOCK_NEW: msr_info-&gt;data = vcpu-&gt;kvm-&gt;arch.wall_clock; break; case MSR_KVM_SYSTEM_TIME: case MSR_KVM_SYSTEM_TIME_NEW: msr_info-&gt;data = vcpu-&gt;arch.time; break; case MSR_KVM_ASYNC_PF_EN: msr_info-&gt;data = vcpu-&gt;arch.apf.msr_val; break; case MSR_KVM_STEAL_TIME: msr_info-&gt;data = vcpu-&gt;arch.st.msr_val; break; case MSR_KVM_PV_EOI_EN: msr_info-&gt;data = vcpu-&gt;arch.pv_eoi.msr_val; break; case MSR_IA32_P5_MC_ADDR: case MSR_IA32_P5_MC_TYPE: case MSR_IA32_MCG_CAP: case MSR_IA32_MCG_CTL: case MSR_IA32_MCG_STATUS: case MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1: return get_msr_mce(vcpu, msr_info-&gt;index, &amp;msr_info-&gt;data, msr_info-&gt;host_initiated); case MSR_K7_CLK_CTL: /* * Provide expected ramp-up count for K7. All other * are set to zero, indicating minimum divisors for * every field. * * This prevents guest kernels on AMD host with CPU * type 6, model 8 and higher from exploding due to * the rdmsr failing. */ msr_info-&gt;data = 0x20000000; break; case HV_X64_MSR_GUEST_OS_ID ... HV_X64_MSR_SINT15: case HV_X64_MSR_CRASH_P0 ... HV_X64_MSR_CRASH_P4: case HV_X64_MSR_CRASH_CTL: case HV_X64_MSR_STIMER0_CONFIG ... HV_X64_MSR_STIMER3_COUNT: case HV_X64_MSR_REENLIGHTENMENT_CONTROL: case HV_X64_MSR_TSC_EMULATION_CONTROL: case HV_X64_MSR_TSC_EMULATION_STATUS: return kvm_hv_get_msr_common(vcpu, msr_info-&gt;index, &amp;msr_info-&gt;data, msr_info-&gt;host_initiated); break; case MSR_IA32_BBL_CR_CTL3: /* This legacy MSR exists but isn't fully documented in current * silicon. It is however accessed by winxp in very narrow * scenarios where it sets bit #19, itself documented as * a "reserved" bit. Best effort attempt to source coherent * read data here should the balance of the register be * interpreted by the guest: * * L2 cache control register 3: 64GB range, 256KB size, * enabled, latency 0x1, configured */ msr_info-&gt;data = 0xbe702111; break; case MSR_AMD64_OSVW_ID_LENGTH: if (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW)) return 1; msr_info-&gt;data = vcpu-&gt;arch.osvw.length; break; case MSR_AMD64_OSVW_STATUS: if (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW)) return 1; msr_info-&gt;data = vcpu-&gt;arch.osvw.status; break; case MSR_PLATFORM_INFO: if (!msr_info-&gt;host_initiated &amp;&amp; !vcpu-&gt;kvm-&gt;arch.guest_can_read_msr_platform_info) return 1; msr_info-&gt;data = vcpu-&gt;arch.msr_platform_info; break; case MSR_MISC_FEATURES_ENABLES: msr_info-&gt;data = vcpu-&gt;arch.msr_misc_features_enables; break; default: if (kvm_pmu_is_valid_msr(vcpu, msr_info-&gt;index)) return kvm_pmu_get_msr(vcpu, msr_info-&gt;index, &amp;msr_info-&gt;data); if (!ignore_msrs) &#123; vcpu_debug_ratelimited(vcpu, "unhandled rdmsr: 0x%x\n", msr_info-&gt;index); return 1; &#125; else &#123; if (report_ignored_msrs) vcpu_unimpl(vcpu, "ignored rdmsr: 0x%x\n", msr_info-&gt;index); msr_info-&gt;data = 0; &#125; break; &#125; return 0;&#125;EXPORT_SYMBOL_GPL(kvm_get_msr_common);]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>MSR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kvm对guest中MSR寄存器的处理（一）]]></title>
    <url>%2Fpost%2F2019%2F1%2Fkvm%E5%AF%B9guest%E4%B8%ADMSR%E5%AF%84%E5%AD%98%E5%99%A8%E7%9A%84%E5%A4%84%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89.html</url>
    <content type="text"><![CDATA[本文介绍kvm中对MSRs相关的处理，包括MSR bitmap, rdmsr和wrmsr相关的trap和处理等。 MSR bitmap如果处理器支持 1-setting of the “Use MSR bitmaps” VM-execution control, 那么VM-execution control fields会包含4个连续的MSR bitmaps的64位的物理地址，每个连续的MSR bitmaps为1 KByte的大小。如果处理器不支持1-setting of the “Use MSR bitmaps”,那么不会存在这4个连续的区域。 Read bitmap for low MSRs(locate at the MSR-bitmap address). This contains one bit for each MSR address in the range 0000 0000H to 0000 1FFFH. The bit determines whether an execution of RDMSR applied to that MSR causes a VM exit.即从 [MSR-bitmap address ~ MSR-bitmap address+1023] 共1KB(8192 bits)的大小，每个bit对应0000 0000H - 0000 1FFFH中的一个MSR。如果对应的bit为1，那么RDMSR相应地址的MSR则会VM exit. Read bitmap for high MSRs (located at the MSR-bitmap address plus 1024). This contains one bit for each MSR address in the range C000 0000H to C000 1FFFH. The bit determines whether an execution of RDMSR applied to that MSR causes a VM exit.即从 [MSR-bitmap address+1024 ~ MSR-bitmap address+2047] 共1KB(8192 bits)的大小，每个bit对应C000 0000H - C000 1FFFH中的一个MSR。如果对应的bit为1，那么RDMSR相应地址的MSR则会VM exit. Write bitmap for low MSRs (located at the MSR-bitmap address plus 2048). 同理控制地址范围0000 0000H - 0000 1FFFH的MSR的WRMSR是否会VM exit. Write bitmap for high MSRs (located at the MSR-bitmap address plus 3072). 同理控制地址范围C000 0000H - C000 1FFFH的MSR的WRMSR是否会VM exit. rdmsr &amp; wrmsr 指令在SDM.Vol3.25.1.3 Instructions That Cause VM Exits Conditionally中关于rdmsr和wrmsr指令，是这样说的：RDMSR. The RDMSR instruction causes a VM exit if any of the following are true: The “Use MSR bitmaps” VM-execution control is 0. The value of ECX is not in the ranges 0000 0000H - 0000 1FFFH and C000 0000H - C000 1FFFH. The value of ECX is in the ranges 0000 0000H - 0000 1FFFH and bit n in read bitmap for low MSRs is 1, where n is the value of ECX. The value of ECX is in the ranges C000 0000H - C000 1FFFH and bit n in read bitmap for high MSRs is 1, where n is the value of ECX &amp; 0000 1FFFH. WRMSR同上可以类推。 当guest在运行时，如果不产生VM EXIT，即运行VM non-root mdoe的时候。如果RDMSR/WRMSR不会产生VM exit,那么会直接对物理的MSR进行读写，如果产生了VM EXIT，那么会VM EXIT，到VM root mode，根据KVM code来决定如何处理，下面我们来看kvm code是如何实现的。//应该放到总结去。 KVM如何处理rdmsr VM exit.数据结构数据结构1– vmx_msr_index[]数组 定义12345678910111213/* * Though SYSCALL is only supported in 64-bit mode on Intel CPUs, kvm * will emulate SYSCALL in legacy mode if the vendor string in guest * CPUID.0:&#123;EBX,ECX,EDX&#125; is &quot;AuthenticAMD&quot; or &quot;AMDisbetter!&quot; To * support this emulation, IA32_STAR must always be included in * vmx_msr_index[], even in i386 builds. */const u32 vmx_msr_index[] = &#123; #ifdef CONFIG_X86_64 MSR_SYSCALL_MASK, MSR_LSTAR, MSR_CSTAR,#endif MSR_EFER, MSR_TSC_AUX, MSR_STAR,&#125;; 数据结构2– kvm_shared_msrs，__percpu变量 定义 123456789101112#define KVM_NR_SHARED_MSRS 16struct kvm_shared_msrs &#123; struct user_return_notifier run; bool registered; struct kvm_shared_msr_values &#123; u64 host; u64 curr; &#125; values[KVM_NR_SHARED_MSRS]; //values[16]数组的大小为16，最多可以存16个msr寄存器的值&#125;; static struct kvm_shared_msrs __percpu * shared_msrs; 创建在x86.c中的kvm_arch_init()函数中，为每个CPU分配了一个percpu变量shared_msrs，为kvm_shared_msrs结构体 12345int kvm_arch_init(void *opaque)&#123; ... shared_msrs = alloc_percpu(struct kvm_shared_msrs); ...&#125; 初始化 数据结构3– shared_msrs_global 定义在x86.c中，定义了一个结构体struct kvm_shared_msrs_global和其一个static实例，shared_msrs_global; 1234567#define KVM_NR_SHARED_MSRS 16struct kvm_shared_msrs_global &#123; int nr; u32 msrs[KVM_NR_SHARED_MSRS];&#125;static struct kvm_shared_msrs_global __read_mostly shared_msrs_global; 初始化然后在hardware_setup()函数中会根据vmx_msr_index[]调用kvm_define_shared_msr(),具体地：将vmx_msr_index[]数组存入shared_msrs_global.msrs[]数组中，其中kvm_msrs_global.nr为vmx_msr_index[]数组的大小，即记录shared_msrs_global.msrs[]数组的大小。 数据结构4– vmx-&gt;guest_msrs[] 定义vmx-&gt;guest_msrs[]为strcut vcpu_vmx结构体中定义的一个struct shared_msr_entry * guest_msrs的指针。 1234567891011121314struct vcpu_vmx &#123; ... struct shared_msr_entry * guest_msrs; int nmsrs; int save_nmsrs; bool guest_msrs_dirty; ...&#125;struct shared_msr_entry &#123; unsigned index; u64 data; u64 mask;&#125; 创建在vmx_create_vcpu()函数中，vmx-&gt;guest_msrs = kmalloc(PAGE_SIZE, GFP_KERNRL)为其分配了一个4KB的page页。 初始化在vmx.c中的vmx_vcpu_setup()函数中，初始化vmx-&gt;guest_msrs[]数组，具体地，vmx_vcpu_setup()函数，为vcpu配置VMCS的各个域，我们不关心这些，只看其中初始化vmx-&gt;guest_msrs[]数组的过程，具体看下面的for循环：vmx_msr_index[]数组里面的成员的不同的MSR的index，初始时，vmx-&gt;nmsrs=0;所以该函数根据vmx_msr_index[]定义的MSR寄存器，依次进行检查，并将其存入vmx-&gt;guest_msrs[]数组，并用vmx-&gt;nmsrs记录数组大小。首先rdmsr_safe然后将read的值wrmsr_safe,以判断KVM可以正常的读写该MSR；如果可以，就将该MSR的存入vmx-&gt;guest_msrs[]数组，具体地：vmx-&gt;guest_msrs[].index = 为该MSR在vmx_msr_index[]数组中的序号。vmx-&gt;guest_msrs[].data = 0vmx-&gt;guest_msrs[].mask = 全1； 12345678910111213141516171819202122/* * Sets up the vmcs for emulated real mode. */static void vmx_vcpu_setup(struct vcpu_vmx *vmx)&#123; ... for (i = 0; i &lt; ARRAY_SIZE(vmx_msr_index); ++i) &#123; u32 index = vmx_msr_index[i]; u32 data_low, data_high; int j = vmx-&gt;nmsrs; if (rdmsr_safe(index, &amp;data_low, &amp;data_high) &lt; 0) continue; if (wrmsr_safe(index, data_low, data_high) &lt; 0) continue; vmx-&gt;guest_msrs[j].index = i; vmx-&gt;guest_msrs[j].data = 0; vmx-&gt;guest_msrs[j].mask = -1ull; ++vmx-&gt;nmsrs; &#125; ...&#125; 对数据结构的操作点上面为每个cpu分配的struct kvm_shared_msrs shared_msrs变量在vmx_prepare_switch_to_guest()函数中会根据vmx-&gt;guest_msrs_dirty变量或者vmx-&gt;guest_cpu_state来更新。具体的条件为!vmx-&gt;loaded_cpu_state || vmx-&gt;guest_msrs_dirty，其中第一种条件为： !vmx-&gt;loaded_cpu_state,当前vmx-&gt;loaded_cpu_state为空，(loaded_cpu_state points to the VMCS whose state is loaded into the CPU registers that only need to be switched when transitioning to/from the kernel; a NULL value indicates that host state is loaded.) vmx-&gt;guest_msrs_dirty标志位，（）和vmx-&gt;save_nmsrs相关联， 在vmx.c中的setup_msrs()函数中，会根据vmx_msr_index[]数组来依次判断MSR_STAR,MSR_LSTAR,MSR_SYSCAL_MASK,MSR_EFER,MSR_TSC_AUX这5个MSR是否在vmx-&gt;guest_msrs[]如果在，则依次将它们调换到vmx-&gt;guest_msrs[]数组的前排位置去，最后设置vmx-&gt;save_nmsrs为调换的次数，以及设置vmx-&gt;guest_msrs_dirty=ture.(注意，没有发生调换的时候，即前面提到的5个MSR都不支持的时候，vmx-&gt;save_nmsrs=0，vmx-&gt;guest_msrs_dirty=true)- 具体的更新vmx-&gt;guest_msrs[]数组kvm_set_shared_msr()函数，首先取出当前cpu的percpu变量per_cpu_ptr(shared_msrs, cpu); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546void vmx_prepare_switch_to_guest(struct kvm_vcpu * vcpu）&#123; ... /* * Note that guest MSRs to be saved/restored can also be changed * when guest state is loaded. This happens when guest transitions * to/from long-mode by setting MSR_EFER.LMA. */ if (!vmx-&gt;loaded_cpu_state || vmx-&gt;guest_msrs_dirty) &#123; vmx-&gt;guest_msrs_dirty = false; for (i = 0; i &lt; vmx-&gt;save_nmsrs; ++i) kvm_set_shared_msr(vmx-&gt;guest_msrs[i].index, vmx-&gt;guest_msrs[i].data, vmx-&gt;guest_msrs[i].mask); &#125; ...&#125;// kvm_set_shared_msr()函数，首先从当前CPU取出percpu变量struct kvm_shared_msrs指针(*smsr),在kvm_arch_init()为每个CPU分配。// 首先判断要set的slot的值value和smrs-&gt;values[slot].curr的当前值是否相同，如果相同，则直接return 0；// 如果不同，则更新smrs-&gt;values[slot].curr为value,// 然后将value的值写到具体的MSR中,wrmsl_safe(shared_msrs_global.msrs[slot], value)// 然后判断当前cpu的percpu变量kvm_shared_msrs是不是已经注册了，如果没有注册则注册。int kvm_set_shared_msr(unsigned slot, u64 value, u64 mask) &#123; unsigned int cpu = smp_processor_id(); struct kvm_shared_msrs *smsr = per_cpu_ptr(shared_msrs, cpu); int err; if (((value ^ smsr-&gt;values[slot].curr) &amp; mask) == 0) return 0; smsr-&gt;values[slot].curr = value; err = wrmsrl_safe(shared_msrs_global.msrs[slot], value); if (err) return 1; if (!smsr-&gt;registered) &#123; smsr-&gt;urn.on_user_return = kvm_on_user_return; user_return_notifier_register(&amp;smsr-&gt;urn); smsr-&gt;registered = true; &#125; return 0;&#125;EXPORT_SYMBOL_GPL(kvm_set_shared_msr);]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>MSR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[size of segment]]></title>
    <url>%2Fpost%2F2018%2F10%2Fsize-of-segment.html</url>
    <content type="text"><![CDATA[在segment descriptor中，高4个bytes(4-7bytes)的bit 22为D/B字段。（this flag should always be set to 1 for 32-bit code and data segments and 0 for 16-bit code and data segments） Executable code segment:该字段为D flag，决定了该代码段中指令的有效地址的长度和操作数的长度。1表示使用32位的地址和32-bit(4字节)或者8-bit(1字节)的操作数；0表示使用16位的地址和16-bit(2字节)或者8-bit(1字节)的操作数.(当Lbit, bit 21 of the second doubleword of the segment descriptor indicates whether a code segment contains native 64-bit code. 1表示该code segment中的指令在64-bit模式执行；0表示该code segment的指令在compatibility模式执行。 如果L位置1了，那么D bit必须清除；当不在IA-32e模式，或者不是code segment时，L bit is reserved and should always be set to 0.) Stack segment (data segment pointed to by the SS register). 该字段为B（big）flag.该字段决定了堆栈指针的大小。1表示使用32-bit stack pointer,存放在32-bit的ESP寄存器中；0表示16-bit的stack pointer，存放在16-bit的SP寄存器中。如果stack segment是一个expand-down的data-segment，那么该B flag也制定了stack segment的upper bond. Expand-down data segment:该字段为B flag,指定了segment的upper bound. 1表示upper bound为FFFFFFFF(4GBytes);0表示upper bound为FFFF(64KBytes). 所以当segment为stack segment时，第2条和第3条同时生效。 Stack Alignment The stack pointer for a stack segment should be aligned on 16-bit (word) or 32-bit (double-word) boundaries, depending on the width of the stack segment. The D flag in the segment secriptor for the current code segment sets the stack-segment width. (当Dbit为1时，表示使用32-bit或8bit的操作数；当D bit为0时，表示使用16-bit和8-bit的操作数) (即，当D bit为1时，stack的width为32bits；当D bit为0时，stack的width为16bits) Adress-Size Attributes for Stack Accesses:implicit adress of the top of the stack, and they may also have an explicit memory address (for example, PUSH Array1[EBX]). The address-size attribute of the top of the stack determines whether SP or ESP is used for the stack access.Stack operations with an address-size attribute of 16 use the 16-bit SP stack pointer can can use a maximum stack address of FFFFH;Stack operations with an address-size attribute of 32 bits ues the 32-bit ESP register and can usa a maximum address of FFFFFFFFH.The default addre-size attribute for data segments used as stack is controlled by the B flag of the segment’s descriptor. The attribute of the explicit address is determined by the D flag of the current code segment and the presence or absence of the 67H address-size prefix. Stack Behavior in 64-Bit Mode 64位模式的时候，SS segment的基地址为0.Fields(base, limit, and attribute) in segment descriptor registers are ignored.SSregister的DPL is modified that it is always equal to CPL. This will be true even if it is the only field in the ss descriptor that is modified. ESP, EIP, EBP寄存器扩展为64 bits，并重命名为RSP, RIP, RBP. PUSH/POP指令以64-bit的宽度增加/减少堆栈。 When the contents of a segment register is pushed onto 64-bit stack, the pointer is automatically aligned to 64 bits (as with a stack that has a 32-bit width).]]></content>
      <categories>
        <category>Intel-SDM</category>
      </categories>
      <tags>
        <tag>Intel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CPL,RPL,DPL三者的关系，以及CPL的切换规则]]></title>
    <url>%2Fpost%2F2018%2F10%2FCPL-RPL-DPL.html</url>
    <content type="text"><![CDATA[三个概念： CPL: Current privilege Level, 当前特权级，其为当前的CS段和SS段的段寄存器的第0-1位的值，如下图所示，段寄存器中的可见部分的内容，即为segment selector的值，所以当前代码的CPL即为载入段描述符时，段选择符的RPL的值。注意：当前的CS段和SS段的段寄存器总的CPL值一定是一样的，当发生CPL切换时，对应的也会切换SS堆栈 DPL: Descriptor privilege level,为segment或者gate的特权级。其值在segment decriptor或gate descriptor的DPL区域。 RPL: Requested Privilege level，如下图所示为segment selector的最低2位。 CPL,DPL,RPL的取值为0,1,2,3.数值越大，特权级越低。当要访问数据段中的操作数的时候，数据段(data segment)对应的段选择符(segment selector)必须加载到对应的数据段寄存器(data segment registers, DS, ES, FS or GS)或者堆栈段(stack-segment register,SS)。在加载的时候，需要进行特权级检查，需要对比当前正在运行的程序的CPL,目标操作数所在段的段选择符的RPL,以及目标操作数所在段的段描述符的DPL只有当CPL &lt;= DPL &amp;&amp; RPL &lt;= DPL，目标操作数所在的段才会加载成功，否则产生一个GP,general protection. 特权级的检查(Privilege level checking)访问数据段(access data segment)当访问数据段(data segment)的时候，段选择符(segment selector)必须加载到数据段寄存器（DS、ES、FS、GS）或者堆栈寄存器SS(Stack segment是一种特殊的数据段)。 条件：CPL &lt;= DPL &amp;&amp; RPL &lt;= DPL 注意：segment selector的RPL是受软件控制的，也就是说一个CPL=3的程序可以将一个data-segment的RPL设为0,然后去访问一个data segment.那么RPL &lt;= DPL必定成立，则只用检查DPL。 访问代码段（code segment）中的数据 Load a data-segment register with a segment selector for a nonconforming, readable, code segment: CPL &lt;= DPL &amp;&amp; RPL &lt;= DPL. Load a data-segment register with a segment selector for a conforming, readable, code segment: always valid, 因为conforming的code segment的特权级总是和CPL一样的 Use a code-segment override prefix(CS) to read a readable, code segment whose selector is already loaded in the CS register: always valid,因为CS寄存器选择的code segment的DPL和CPL是一样的 访问SS register当想SS(stack segment) register中加载新的stack segment的segment selector时： CPL == DPL == RPL 访问代码段程序控制权从一个code segment转移到另一个code segment时，目标段的segment selector必须加载到code-segment register。此时会进行多种权限检查，如上面提到的CPL，DPL，RPL。以及目标代码段的type, limit等。如果检查通过，则将目标code segment的segment selector加载到CS段寄存器，程序控制权则被转移到了新加载的代码段，并且程序从EIP寄存器所指向的地址开始执行。 程序控制权的转移 通过以下指令：JMP，CALL，RET，SYSENTER，SYSEXIT，SYSCALL，SYSRET，INT n; exception和interrupt机制，以及IERT指令； JMP或CALL指令可以以下面4种方式来引用另一个code segment: 目标操作数包含指向目标code segment 的segment selector； 目标操作数指向一个call-gate descriptor,其包含一个指向目标code segment的segment selector. 目标操作数指向一个TSS,其包含指向目标code segment的segment selector; 目标操作数指向一个task gate,其指向一个TSS，TSS包含指向目标code segment的segment selector; 1.直接跳转到code segment:目标操作数包含指向目标code segment 的segment selector； JMP, CALL 和RET指令的段内跳转，不用切换code segment。 JMP, CALL 和RET指令的远程跳转，跳转到另一个code segment,需要进行特权级检查。转移程序控制权到另一个code segment,且没有使用call gate的时候： 当转移到Nonconforming code segment: CPL == DPL &amp;&amp; RPL &lt;= CPL(== DPL),并且切换到新的code segment后，CPL不变（即使RPL &lt; CPL） 当转移到Conforming code segment: CPL &gt;= DPL (RPL直接被忽略)此情形时，当程序控制权转移到conforming code segment,CPL不会改变，即使目标code segment的DPL比调用程序的CPL数值要小。因为CPL没有改变，所以也不会发生stack切换。一般Conforming segments为math libraries和exception handlers之类的代码模块。 2.使用gate descriptor进行跳转 Call gates Trap gates Interrupt gates Task gates其中Task gates用来task swithcingl; Trap and interrupt gates是特殊的call gates用来调用exception and interrupt handlers.下面主要讨论call gates. Call gatesCall gates用来在不同的特权级之间转移程序控制权，此外call gates也可以在16bits和32bits的代码段之间转移程序控制权。Call gates descriptor存放于GDT或者LDT中，但不会出现在IDT中。下图为call-gate descriptor的组成: 指明了需要被访问的code segment(segment selector) 目标code segment的程序入口(offset in segment) caller程序的特权级。（DPL域指明了call gate的特权级，which in turn is the privilege level required to access the selected procedure through the gate） 如果需要进行stack切换，指明了需要在stack之间复制的可选参数的数量.(Parm Count specifies the number of words for 16-bit call gates and doublewords for 32-bit call gates) 定义了要push到目标stack的size: 16-bit gates force 16-bit pushes and 32-bit gates forces 32-bit pushes. 指明当前call-gate是否有效。(P flag) 通过call gate访问一个code segment。为了访问call gate，CALL或者JMP指令的目标操作数需要为一个指向call gate的far pointer。如下图所示，the segment selector指明了call gate, the offset是需要的，但是不被检查和使用。会使用far pointer的segment selector在GDT或者LDT中选中一个call gate.然后使用该call gate中的segment selector和offset来组成目标线性地址。如下图所示，需要检查4个特权级： CPL （当前caller的cs的特权级） RPL （requestor’s privilege level）：指向call gate descripter的selector的RPL DPL （descriptor privilege level）：call gate descriptor的DPL DPL： call gate descriptor中的segment selector所指向的code segment descriptor中的DPL下图给出了使用JMP或者CALL指令是，上述4个特权级需要满足的关系： 如果call了一个更高特权级的nonconforming的目标code segment.那么CPL会被减小数值之目标code segment的DPL。那么就会发生stack切换。如果call或者jump了一个更高特权级的conforming的目标code segment.不会改变CPL，所以也不会发生stack切换。 总结1when access code segment When transferring program control to another code segment without going through a call gate. nonconforming: CPL == DPL &amp;&amp; RPL &lt;= CPL (RPL &lt;= CPL == DPL)。并且新的CPL不变，即使RPL比较小 conforming: CPL &gt;= DPL &amp;&amp; RPL is ignored. 并且新的CPL不变。 When transferring program control through a call gate.CPL &lt;= DPL of call gate descriptor &amp;&amp; RPL of call gate selector &lt;= DPL of call gate descriptor nonconforming code segment: DPL &lt;= CPL (CALL) DPL == CPL (JMP) conforming code segment: DPL &lt;= CPL (CALL) DPL &lt;= CPL (JMP)That means:Only CALL instructions can use call gates to transfer program control to more privileged (数值更小的) nonconforming code segment.A JMP instruction can use a call gate only to transfer program control to a nonconforming code segment with a DPL equal to the CPL.CALL and JMP instruction can both transfer program control to a more privileged conforming code segment; 总结2the change of CPL without call gate near call: CPL不变； far call: CPL不变； with call gate more privileged nonconforming destination code segment: the CPL is lowered to the DPL of the destination code segment and a stack switch occurs. more privileged conforming destination code segment: CPL不变，stack不切换。]]></content>
      <categories>
        <category>Intel-SDM</category>
      </categories>
      <tags>
        <tag>Intel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kvm源码解析之kvm-intel.ko模块初始化]]></title>
    <url>%2Fpost%2F2018%2F9%2Fkvm-module-init.html</url>
    <content type="text"><![CDATA[1. x86 arch with Intel cpu needs two modules kvm.ko and kvm-intel.koKVM是kernel-based virtual machine,但平台使用的是Intel CPU的时，需要用到内核中的2个模块：kvm.ko和kvm-intel.ko。其中kvm-intel.ko模块由vmx.c和pmu_intel.c两个文件编译。本文分析KVM源码中，系统加载这两个模块的时候，内核做了哪些工作。 kvm-intel.ko模块初始化vmx.c包含了Intel CPU对x86平台的虚拟化支持。在加载kvm-intel.ko模块的时候，根据module_init(vmx_init)首先会调用vmx_init函数1234567891011121314151617181920212223static int __init vmx_init(void)&#123; int r; ... r = kvm_init(&amp;vmx_x86_ops, sizeof(struct vcpu_vmx), __alignof__(struct vcpu_vmx), THIS_MODULE); if (r) return r; if (boot_cpu_has(X86_BUG_L1TF)) &#123; r = vmx_setup_l1d_flush(vmenrty_l1d_flush_param); if (r) &#123; vmx_exit(); return r; &#125; &#125; vmx_check_vmcs12_offsets(); return 0;&#125; 其中会调用kvm_init,并传入vmx_x86_ops,和对应vcpu的strcut vcpu_vmx,以及其对齐性。 12345678910//kvm_main.c 为kvm.ko模块提供的一个函数int kvm_init(void * opaque, unsigned vcpu_size, unsigned vcpu_align, strcut module * module)&#123; int r; int cpu; r = kvm_arch_init(opaque); ...&#125; 在kvm_init()中，分别做了以下几件事情： 调用了kvm_arch_init(),其会根据具体的architecture(mips,powerpc,s390,x86,arm)选择不同的实现.我们这里为Intel的架构，为x86架构，其对应的kvm_arch_init()函数实现在x86.c文件中。12345678910111213141516171819202122232425262728293031323334353637383940414243444546int kvm_arch_init(void * opaque)&#123; int r; struct kvm_x86_ops * ops = oqaque;// ...//do some checks to check whether the machine support KVM, and whether if enable it shared_msrs = alloc_percpu(struct kvm_shared_msrs); if (!shared_msrs) &#123; printk(); goto out; &#125; r = kvm_mmu_module_init(); if (r) goto out_free_percpu; kvm_set_mmio_spte_mask(); kvm_x86_ops = ops; kvm_mmu_set_mask_ptes(PT_USER_MASK, PT_ACCESSED_MASK, PT_DIRTY_MASK, PT64_NX_MASK, 0, PT_PRESENT_MASK, 0, sme_me_mask); kvm_timer_init(); perf_register_guest_info_callbacks(&amp;kvm_guest_cbs); if (boot_cpu_has(X86_FEATURE_XSAVE)) host_xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK); kvm_lapic_init();#ifdef CONFIG_X86_64 pvclock_gtod_register_notifier(&amp;pvclock_gtod_notifier); if (hypervisor_is_type(X86_HYPER_MS_HYPERV)) set_hv_tscchange_cb(kvm_hyperv_tsc_notifier);#endif return 0;out_free_percpu: free_percpu(shared_msrs);out: return r;&#125; 该函数主要做了以下几件事： alloc_percpu(struct kvm_shared_msrs)为每个CPU分配kvm_shared_msrs结构体 kvm_mmu_module_init() kvm_set_mmio_spte_mask() kvm_mmu_set_mask_ptes() 初始化MMU kvm_time_init() 初始化kvm的timer perf_register_guest_info_callbacks(&amp;kvm_guest_cbs); kvm_lapic_init();{ / do not patch jump label more than once per second / jump_label_rate_limit(&amp;apic_hw_disabled, HZ); jump_label_rate_limit(&amp;apic_sw_disabled, HZ);} (optional)host_xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);(optional)pvclock_gtod_register_notifier(&amp;pvclock_gtod_notifier);(optional)set_hv_tscchange_cb(kvm_hyperv_tsc_notifier); 接着在kvm_init()函数中，调用了kvm_irqfd_init() 如果没有配置CONFIG_HAVE_KVM_IRQFD，则该函数return 0. 接着调用kvm_arch_hardware_setup()函数，进行体系结构相关的硬件配置 123456789101112131415161718192021222324252627// in x86.ckvm_arch_hardware_setup()&#123; int r; r = kvm_x86_ops-&gt;hardware_setup(); if (r != 0) return r; if (kvm_has_tsc_control) &#123; /* * Make sure the user can only configure tsc_khz values that * fit into a signed integer. * A min value is not calculated because it will always * be 1 on all machines. */ u64 max = min(0x7fffffffULL, __scale_tsc(kvm_max_tsc_scaling_ratio, tsc_khz)); kvm_max_guest_tsc_khz = max; kvm_default_tsc_scaling_ratio = 1ULL &lt;&lt; kvm_tsc_scaling_ratio_frac_bits; &#125; kvm_init_msr_list(); return 0;&#125; 可以看见在kvm_arch_hardware_setup()函数中，调用了kvm_x86_ops的hardware_setup(),kvm_x86_ops为我们最开始在vmx.c文件中定义的，并通过模块的初始化函数vmx_init()-&gt;kvm_init()-&gt;kvm_arch_hardware_setup()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148static __init int hardware_setup(void)&#123; unsigned long host_bndcfgs; int r = -ENOMEM, i; rdmsrl_safe(MSR_EFER, &amp;host_efer); for (i = 0; i &lt; ARRAY_SIZE(vmx_msr_index); ++i) kvm_define_shared_msr(i, vmx_msr_index[i]); for (i = 0; i &lt; VMX_BITMAP_NR; i++) &#123; vmx_bitmap[i] = (unsigned long *)__get_free_page(GFP_KERNEL); if (!vmx_bitmap[i]) goto out; &#125; memset(vmx_vmread_bitmap, 0xff, PAGE_SIZE); memset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE); if (setup_vmcs_config(&amp;vmcs_config) &lt; 0) &#123; r = -EIO; goto out; &#125; if (boot_cpu_has(X86_FEATURE_NX)) kvm_enable_efer_bits(EFER_NX); if (boot_cpu_has(X86_FEATURE_MPX)) &#123; rdmsrl(MSR_IA32_BNDCFGS, host_bndcfgs); WARN_ONCE(host_bndcfgs, "KVM: BNDCFGS in host will be lost"); &#125; if (!cpu_has_vmx_vpid() || !cpu_has_vmx_invvpid() || !(cpu_has_vmx_invvpid_single() || cpu_has_vmx_invvpid_global())) enable_vpid = 0; if (!cpu_has_vmx_ept() || !cpu_has_vmx_ept_4levels() || !cpu_has_vmx_ept_mt_wb() || !cpu_has_vmx_invept_global()) enable_ept = 0; if (!cpu_has_vmx_ept_ad_bits() || !enable_ept) enable_ept_ad_bits = 0; if (!cpu_has_vmx_unrestricted_guest() || !enable_ept) enable_unrestricted_guest = 0; if (!cpu_has_vmx_flexpriority()) flexpriority_enabled = 0; if (!cpu_has_virtual_nmis()) enable_vnmi = 0; /* * set_apic_access_page_addr() is used to reload apic access * page upon invalidation. No need to do anything if not * using the APIC_ACCESS_ADDR VMCS field. */ if (!flexpriority_enabled) kvm_x86_ops-&gt;set_apic_access_page_addr = NULL; if (!cpu_has_vmx_tpr_shadow()) kvm_x86_ops-&gt;update_cr8_intercept = NULL; if (enable_ept &amp;&amp; !cpu_has_vmx_ept_2m_page()) kvm_disable_largepages();#if IS_ENABLED(CONFIG_HYPERV) if (ms_hyperv.nested_features &amp; HV_X64_NESTED_GUEST_MAPPING_FLUSH &amp;&amp; enable_ept) kvm_x86_ops-&gt;tlb_remote_flush = vmx_hv_remote_flush_tlb;#endif if (!cpu_has_vmx_ple()) &#123; ple_gap = 0; ple_window = 0; ple_window_grow = 0; ple_window_max = 0; ple_window_shrink = 0; &#125; if (!cpu_has_vmx_apicv()) &#123; enable_apicv = 0; kvm_x86_ops-&gt;sync_pir_to_irr = NULL; &#125; if (cpu_has_vmx_tsc_scaling()) &#123; kvm_has_tsc_control = true; kvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX; kvm_tsc_scaling_ratio_frac_bits = 48; &#125; set_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */ if (enable_ept) vmx_enable_tdp(); else kvm_disable_tdp(); if (!nested) &#123; kvm_x86_ops-&gt;get_nested_state = NULL; kvm_x86_ops-&gt;set_nested_state = NULL; &#125; /* * Only enable PML when hardware supports PML feature, and both EPT * and EPT A/D bit features are enabled -- PML depends on them to work. */ if (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml()) enable_pml = 0; if (!enable_pml) &#123; kvm_x86_ops-&gt;slot_enable_log_dirty = NULL; kvm_x86_ops-&gt;slot_disable_log_dirty = NULL; kvm_x86_ops-&gt;flush_log_dirty = NULL; kvm_x86_ops-&gt;enable_log_dirty_pt_masked = NULL; &#125; if (cpu_has_vmx_preemption_timer() &amp;&amp; enable_preemption_timer) &#123; u64 vmx_msr; rdmsrl(MSR_IA32_VMX_MISC, vmx_msr); cpu_preemption_timer_multi = vmx_msr &amp; VMX_MISC_PREEMPTION_TIMER_RATE_MASK; &#125; else &#123; kvm_x86_ops-&gt;set_hv_timer = NULL; kvm_x86_ops-&gt;cancel_hv_timer = NULL; &#125; if (!cpu_has_vmx_shadow_vmcs()) enable_shadow_vmcs = 0; if (enable_shadow_vmcs) init_vmcs_shadow_fields(); kvm_set_posted_intr_wakeup_handler(wakeup_handler); nested_vmx_setup_ctls_msrs(&amp;vmcs_config.nested, enable_apicv); kvm_mce_cap_supported |= MCG_LMCE_P; return alloc_kvm_area();out: for (i = 0; i &lt; VMX_BITMAP_NR; i++) free_page((unsigned long)vmx_bitmap[i]); return r;&#125; 总结一下，vmx的hardware_setup做了以下的工作： host_efer; 使用vmx体系中的vmx_msr_index[]数组，去初始化x86.c文件中kvm_shared_msrs_global[]数组; 为vmx_vmread_bitmap和vmx_vmwrite_bitmap申请内存空间，每个为4kB，占一个page页; setup_vmcs_config(&amp;vmcs_config)根据host cpu的cpuid和msrs初始化该平台的vmcs结构体vmcs_config和vmx_capability; 设置x86.c文件中的efer_reserved_bits； 根据vmcs_config和vmx_capability在setup_vmcs_config()设置的值修改vmx.c文件中enable_vpid, enable_ept, enable_ept_ad_bits, enable_unrestricted_guest, flexpriority_enabled, enable_vnmi, largepages_enabled的值。 如果当前vmcs_config不支持PLE，则将ple_gap, ple_window, ple_window_grow, ple_window_max, ple_window_shrink设为0; 如果vmcs_config不支持apicv,则设置enable_apicv和kvm_x86_ops-&gt;sync_pir_to_irr函数句柄; 如果vmcs_config支持tsc scaling,则设置kvm_has_tsc_control, kvm_max_tsc_scaling_ratio, kvm_tsc_scaling_ratio_frac_bits； 设置vmx_vpid_bitmap为0 如果enable_ept，则vmx_enable_tdp(),否则kvm_disable_tdp();]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kvm源码解析之kvm_main.c]]></title>
    <url>%2Fpost%2F2018%2F8%2Fkvm_main.html</url>
    <content type="text"><![CDATA[kvm_main.c中的init函数首先会调用，kvm_arch_init(opaque); kvm_arch_init这时会调用与architecture相关的对应的初始化函数，我们只关心x86平台，所以看对应的arch/x86/kvm/x86.c中的实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768int kvm_arch_init(void *opaque)&#123; int r; //该结构体为与x86平台相关的kvm需要使用的函数的指针集合 struct kvm_x86_ops *ops = opaque; //首先判断该指针是否已经被赋值，即是否已经初始化x86平台相关 if (kvm_x86_ops) &#123; printk(KERN_ERR "kvm: already loaded the other module\n"); r = -EEXIST; goto out; &#125; //调用x86平台对应的函数实现，检测cpu是否支持KVM if (!ops-&gt;cpu_has_kvm_support()) &#123; printk(KERN_ERR "kvm: no hardware support\n"); r = -EOPNOTSUPP; goto out; &#125; if (ops-&gt;disabled_by_bios()) &#123; printk(KERN_ERR "kvm: disabled by bios\n"); r = -EOPNOTSUPP; goto out; &#125; r = -ENOMEM; //为每个CPU建立shared msrs shared_msrs = alloc_percpu(struct kvm_shared_msrs); if (!shared_msrs) &#123; printk(KERN_ERR "kvm: failed to allocate percpu kvm_shared_msrs\n"); goto out; &#125; //初始化mmu r = kvm_mmu_module_init(); if (r) goto out_free_percpu; kvm_set_mmio_spte_mask(); //注意，这里给kvm_x86_ops赋值，即下次在调用该函数进行x86平台的初始化，则会提示已经加载了与x86平台的其他实现。 kvm_x86_ops = ops; kvm_mmu_set_mask_ptes(PT_USER_MASK, PT_ACCESSED_MASK, PT_DIRTY_MASK, PT64_NX_MASK, 0, PT_PRESENT_MASK, 0, sme_me_mask); kvm_timer_init(); perf_register_guest_info_callbacks(&amp;kvm_guest_cbs); if (boot_cpu_has(X86_FEATURE_XSAVE)) host_xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK); kvm_lapic_init();#ifdef CONFIG_X86_64 pvclock_gtod_register_notifier(&amp;pvclock_gtod_notifier); if (hypervisor_is_type(X86_HYPER_MS_HYPERV)) set_hv_tscchange_cb(kvm_hyperv_tsc_notifier);#endif return 0;out_free_percpu: free_percpu(shared_msrs);out: return r;&#125; kvm_irqfd_init() zalloc_cpu_mask_var() kvm_arch_hardware_setup()会调用与architecture相关的hardware setup函数，这里我们看arch/x86/kvm/x86.c 12345678910111213141516171819202122232425262728293031int kvm_arch_hardware_setup(void)&#123; int r; //在第1步中，kvm_arch_init()函数已经初始化好了kvm_x86_ops， //这里只用调用其hardware_setup()函数， //具体的该函数在vmx.c中，其为intel x86平台对vmm的硬件支持的实现。 //并且kvm模块的入口函数也是在vmx.c中，在vmx.c中有 module_init(vmx_init)， //vmx_init()中，调用kvm_main.c中的kvm_arch_init(),即该文档的起始原因。 r = kvm_x86_ops-&gt;hardware_setup(); if (r != 0) return r; if (kvm_has_tsc_control) &#123; /* * Make sure the user can only configure tsc_khz values that * fit into a signed integer. * A min value is not calculated because it will always * be 1 on all machines. */ u64 max = min(0x7fffffffULL, __scale_tsc(kvm_max_tsc_scaling_ratio, tsc_khz)); kvm_max_guest_tsc_khz = max; kvm_default_tsc_scaling_ratio = 1ULL &lt;&lt; kvm_tsc_scaling_ratio_frac_bits; &#125; kvm_init_msr_list(); //根据 msrs_to_save, emulated_msrs, msr_based_features三个数组中存的值，并查询具体的CPU的支持，并进行更新。 return 0;&#125;]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[re-init hexo-blog project from github]]></title>
    <url>%2Fpost%2F2018%2F8%2Fre-init-hexo-blog-project-from-github.html</url>
    <content type="text"><![CDATA[当换了新电脑后，如何将以前基于Hexo搭建的blog重新搭建呢。 之前的Hexo放在github上，并使用github.io展示。其中项目名字为calmisi.github.io,包含两个branch: hexo和masterhexo为默认的branch，其保存hexo项目的文件，master为展示branch，which holds the html codes that generates by hexo generate 安装Hexo环境 首先安装Node.js，去官方下载安装， 安装Hexo, npm install -g hexo-cli 安装hexo-server, npm install hexo-server --save 安装hexo-deploy-git, npm install hexo-deploy-git --save 克隆github上的blog项目 1git clone git@github.com:calmisi/calmisi.github.io.git 初始化子模块，NEXT主题以子模块的形式放在另一个仓库 123cd calmisi.github.iogit submodule initgit submodule update 初始化Hexo项目 1npm install]]></content>
      <categories>
        <category>Hexo&amp;NEXT</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Latex编译pdf后的字体嵌入问题]]></title>
    <url>%2Fpost%2F2018%2F4%2FLatex%E7%BC%96%E8%AF%91pdf%E5%90%8E%E7%9A%84%E5%AD%97%E4%BD%93%E5%B5%8C%E5%85%A5%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[最近在提交论文的camera ready版本的时候需要在IEEE PDF-eXpress中检测。结果检查结果提示Font Arial, Arial, Italic is not embedded (18x on page 3)问题。就是一些字体没有embedded。 产生的原因Latex源文件中有PDF格式的图片，在这些PDF格式的图片中，没有嵌入字体（not embedded），所以最终生成的PDF论文会提示字体没有嵌入。 有可能是用visio生成PDF文件时没有嵌入； 有可能是matlab生成的.eps格式的文件在Latex编译转化为PDF格式时，没有嵌入； 查看PDF格式的图片字体嵌入可以打开PDF格式的文档，文件 -&gt;文档属性 -&gt; 字体，其中就会显示该PDF文档中所用的所有字体了，每一个字体后面，如果注明了embedded(已嵌入)或embedded subset(已嵌入子集),就说明该字体是嵌入的。 解决方案如果我们用Latex编译论文时用了matlab生成的.eps的图 由于eps文件是ascii文件，里边只是给出字体的名称。 由于matlab图里面默认的是Helvetica字体， 当我们用latex生成pdf时会发现Helvetica字体是没有嵌入的。 将eps文件用 写字板 打开 将字体设置部分： 1234%% IncludeResource: font Helvetica/Helvetica /WindowsLatin1Encoding 120 FMSR第二行改为/ArialMT /WindowsLatin1Encoding 120 FMSR 保存，然后重新用 Latex 生成 pdf。 如果是PDF图片本身没有嵌入字体 可以用Acrobat修改为常用字体重新编译]]></content>
      <categories>
        <category>LuanQiBaZao</category>
      </categories>
      <tags>
        <tag>Latex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos安装与配置原版texlive]]></title>
    <url>%2Fpost%2F2018%2F4%2Fcentos%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%E5%8E%9F%E7%89%88texlive.html</url>
    <content type="text"><![CDATA[以前一直是在windows系统上编译论文，现在想在服务器的centos系统上修改论文终稿，于是就探索了一下。 使用yum安装尝试的第一种方法是yum安装1234yum install texlive-latex#安装texlive-latexyum install texmaker#安装tex文件编辑器 使用yum很方便，当使用texmaker编译之前论文的源文件的时候，提示好多包无法识别，没有办法，还是觉得像windows那样，安装原版TEXLIVE 安装原版texlive 下载原版texlive iso文件下载最新的texlive-2017.iso 挂载ISO镜像 12mkdir /mnt/texlivemount -t iso9660 -o ro,loop,noauto /home/xxx/Downloads/texlive.iso /mnt/texlive 安装首先需要安装依赖包，否则不能正确运行install-tl脚本， 1yum install perl-Digest-MD5 perl-Tk 然后运行install-tl脚本进行安装 12./install-tl #以命令行的方式安装./install-tl -gui #以图像化界面安装 安装完成后卸载镜像文件以及配置PATH 123456umount /mnt/texlive#编辑/etc/profile,添加export MANPATH=$&#123;MANPATH&#125;:/usr/local/texlive/texmf-dist/doc/manexport INFOPATH=$&#123;INFOPATH&#125;:/usr/local/texlive/texmf-dist/doc/infoexport PATH=$&#123;PATH&#125;:/usr/local/texlive/bin/x86_64-linux 安装后配置 更新源配置配置合适的CTAN源可以加快宏包更新的网速，以中科大的源为例:1234tlmgr option repository http://mirrors.ustc.edu.cn/CTAN/systems/texlive/tlnet#之后就可以用tlmgt命令进行网络更新，#CTAN 上的包更新很频繁，所以即便是最新版的texlive2016，其中也有大量的宏包需要更新（可能包括tlmgr程序本身）tlmgr update --self --all tlmgr 也可以GUI运行 1tlmgr --gui --gui-lang zh_CN 字体配置XeTeX 和 LuaTeX 可以直接使用系统字体。然而 texlive 自带的字体并不在系统的字体目录里面。为了让系统可以使用texlive所带的字体，需要进行如下配置。 将texlive的字体配置文件复制到系统内 123sudo cp /usr/local/texlive/texmf-var/fonts/conf/texlive-fontconfig.conf /etc/fonts/conf.d/09-texlive.conf#建议将 /etc/fonts/conf.d/09-texlive.conf包含type1字体的那行删掉，以避免在其它软件中显示成百上千的type1字体，即删掉&lt;dir&gt;/usr/local/texlive/2016/texmf-dist/fonts/type1&lt;/dir&gt; 刷新系统字体缓存 1sudo fc-cache -fsv dummy package 安装texlive2016安装之后需要“告诉”系统texlive相关软件包都安装好了。这样在系统安装依赖于tex的软件（比如R）时就不必重新下载软件仓库中的旧版 texlive 相关软件。也不会造成不同版本 tex 命令的冲突。dummy package 就是解决这样的软件依赖问题的“虚包”。 dummy package 的制作可以参考TUG上的官方说明.这里是RHEL 7系列的dummy package,下载后直接安装即可： 1rpm -ivh texlive-dummy-2012a-1.el7.*]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Texlive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NVIDIA dirver is incompatible with GNOME desktop in vncserver]]></title>
    <url>%2Fpost%2F2017%2F11%2FNVIDIA-dirver-is-incompatible-with-GNOME-desktop-in-vncserver.html</url>
    <content type="text"><![CDATA[之前一直是用的Centos自带的nouveau,但是自从接了个2k屏幕后，经常待机锁屏后无法恢复，屏幕黑屏。遂装了显卡NVIDIA的驱动。 在 Linux 操作系统下，NVIDIA 显卡的驱动需要手动安装，而且需要手动设置，其中有几步需要注意的地方，详见 NVIDIA 显卡驱动安装教程 。 安装的过程需要屏蔽nouveau模块，还要重新生成initramfs (dracut –force)。完全安装好后，显示效果好多了，待机也不黑屏了，但是出现了新问题，远程VNC连接无法现实桌面。 经过网上各种查找解决方案（like this Centos 7 Nvidia openGL breaks vncserver ），仍然没有找到有效的解决方案。 网上给出的一些建议是： NVIDIA 显卡驱动会自带 OpenGL，与 GNOME 桌面环境会冲突，造成桌面崩溃。 Linux 的桌面系统不止一种，除了 GNOME，还有很常用的 KDE 。可以使用它代替 GNOME（GNOME 桌面既美观又简洁，KDE 实在不能比）。 所以只能二选一了， 1 要么卸载 NVIDIA 驱动， 1$ nvidia-uninstall 2 要么更换桌面系统。1$ yum groupinstall &quot;KDE Plasma Workspaces&quot; 如果 Linux 系统有多个桌面环境，需要指定开机时选择哪一个1234$ vim /etc/sysconfig/desktopDESKTOP=&quot;KDE&quot; 或 DESKTOP=&quot;GNOME&quot;$ reboot]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>VNC</tag>
        <tag>Nvidia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DPDK的makefile学习]]></title>
    <url>%2Fpost%2F2017%2F11%2FDPDK%E7%9A%84makefile%E5%AD%A6%E4%B9%A0.html</url>
    <content type="text"><![CDATA[本文介绍一下DPDK的makefile架构。 DPDK的makefile架构DPDK采用GNUmakefile，并且在source_dir/mk/文件夹中有一系列预定义的文件帮助用户快捷的构建自己的DPDK app, lib等。 DPDK的编译编译DPDK的方法123#在DPDK的根目录执行：make config T=x86_64-native-linuxapp-gccmake 下面我们来看，运行make config 和make具体都干了些什么。 1.我们看根目录下的GNUmakefile1234567891011RTE_SDK := $(CURDIR)export RTE_SDK## directory list#ROOTDIRS-y := buildtools lib drivers appROOTDIRS- := testinclude $(RTE_SDK)/mk/rte.sdkroot.mk 首先将当前的根目录定义为RTE_SDK,并export;然后将需要编译的buildtools, lib, drivers, app目录定义为ROOTDIRS-y,将test目录定义为ROOTDIRS-。 然后include $(RTE_SDK)/mk/rte.sdkroot.mk 2.在rte.sdkroot.mk中，定义并export Q=‘@’ RTE_SRCDIR=$(CURDIR) BUILDING_RTE_SDK = 1 RTE_CONFIG_TEMPLATE = $(RTE_SRCDIR)/config/defconfig_$(T) 其中T为命令行传入 RTE_OUTPUT = $(O) 如果命令行有”O=”则为命令行参数，否则为默认的$(RTE_SRCDIR/build) BUILDDIR = $(RTE_OUTPUT)/build 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455MAKEFLAGS += --no-print-directory# define Q to '@' or not. $(Q) is used to prefix all shell commands to# be executed silently.Q=@ifeq '$V' '0'override V=endififdef Vifeq ("$(origin V)", "command line")Q=endifendifexport Qifeq ($(RTE_SDK),)$(error RTE_SDK is not defined)endifRTE_SRCDIR = $(CURDIR)export RTE_SRCDIRBUILDING_RTE_SDK := 1export BUILDING_RTE_SDK## We can specify the configuration template when doing the "make# config". For instance: make config T=x86_64-native-linuxapp-gcc#RTE_CONFIG_TEMPLATE :=ifdef Tifeq ("$(origin T)", "command line")RTE_CONFIG_TEMPLATE := $(RTE_SRCDIR)/config/defconfig_$(T)endifendifexport RTE_CONFIG_TEMPLATE## Default output is $(RTE_SRCDIR)/build# output files wil go in a separate directory#ifdef Oifeq ("$(origin O)", "command line")RTE_OUTPUT := $(abspath $(O))endifendifRTE_OUTPUT ?= $(RTE_SRCDIR)/buildexport RTE_OUTPUT# the directory where intermediate build files are stored, like *.o,# *.d, *.cmd, ...BUILDDIR = $(RTE_OUTPUT)/buildexport BUILDDIRexport ROOTDIRS-y ROOTDIRS- ROOTDIRS-n 3.看make config T=x86_64-native-linuxapp-gcc的流程在上一步中，我们看到在rte.sdkroot.mk中对T的定义进行了处理，对RTE_CONFIG_TEMPLATE进行了赋值，然后进入了config对应的目标123.PHONY: config defconfig showconfigs showversion showversionumconfig defconfig showconfigs showversion showversionum: $(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdkconfig.mk $@ 即采用$(RTE_SDK)/mk/rte.sdkconfig.mk进行make config操作 make config in rte.sdkconfig.mk 12345678.PHONY: configifeq ($(RTE_CONFIG_TEMPLATE),)config: notemplateelseconfig: $(RTE_OUTPUT)/include/rte_config.h $(RTE_OUTPUT)/Makefile @echo "Configuration done using" \ $(patsubst defconfig_%,%,$(notdir $(RTE_CONFIG_TEMPLATE)))endif 可以看到会执行，$(RTE_OUTPUT)/include/rte_config.h和$(RTE_OUTPUT)/Makefile 12345678910111213141516171819# generate a Makefile for this build directory# use a relative path so it will continue to work even if we move the directorySDK_RELPATH=$(shell $(RTE_SDK)/buildtools/relpath.sh $(abspath $(RTE_SRCDIR)) \ $(abspath $(RTE_OUTPUT)))OUTPUT_RELPATH=$(shell $(RTE_SDK)/buildtools/relpath.sh $(abspath $(RTE_OUTPUT)) \ $(abspath $(RTE_SRCDIR)))$(RTE_OUTPUT)/Makefile: | $(RTE_OUTPUT) $(Q)$(RTE_SDK)/buildtools/gen-build-mk.sh $(SDK_RELPATH) $(OUTPUT_RELPATH) \ &gt; $(RTE_OUTPUT)/Makefile# clean installed files, and generate a new config header file# if NODOTCONF variable is defined, don't try to rebuild .config$(RTE_OUTPUT)/include/rte_config.h: $(RTE_OUTPUT)/.config $(Q)rm -rf $(RTE_OUTPUT)/include $(RTE_OUTPUT)/app \ $(RTE_OUTPUT)/lib \ $(RTE_OUTPUT)/hostlib $(RTE_OUTPUT)/kmod $(RTE_OUTPUT)/build $(Q)mkdir -p $(RTE_OUTPUT)/include $(Q)$(RTE_SDK)/buildtools/gen-config-h.sh $(RTE_OUTPUT)/.config \ &gt; $(RTE_OUTPUT)/include/rte_config.h scripts目录下relpath.sh接受两个路径作为输入，然后获取两个路径间的相对路径。 12345678910111213141516171819202122232425262728293031$(RTE_OUTPUT): $(Q)mkdir -p $@ifdef NODOTCONF$(RTE_OUTPUT)/.config: ;else# Generate config from template, if there are duplicates keep only the last.# To do so the temp config is checked for duplicate keys with cut/sort/uniq# Then for each of those identified duplicates as long as there are more than# just one left the last match is removed.$(RTE_OUTPUT)/.config: $(RTE_CONFIG_TEMPLATE) FORCE | $(RTE_OUTPUT) $(Q)if [ "$(RTE_CONFIG_TEMPLATE)" != "" -a -f "$(RTE_CONFIG_TEMPLATE)" ]; then \ $(CPP) -undef -P -x assembler-with-cpp \ -ffreestanding \ -o $(RTE_OUTPUT)/.config_tmp $(RTE_CONFIG_TEMPLATE) ; \ config=$$(cat $(RTE_OUTPUT)/.config_tmp) ; \ echo "$$config" | awk -F '=' 'BEGIN &#123;i=1&#125; \ /^#/ &#123;pos[i++]=$$0&#125; \ !/^#/ &#123;if (!s[$$1]) &#123;pos[i]=$$0; s[$$1]=i++&#125; \ else &#123;pos[s[$$1]]=$$0&#125;&#125; END \ &#123;for (j=1; j&lt;i; j++) print pos[j]&#125;' \ &gt; $(RTE_OUTPUT)/.config_tmp ; \ if ! cmp -s $(RTE_OUTPUT)/.config_tmp $(RTE_OUTPUT)/.config; then \ cp $(RTE_OUTPUT)/.config_tmp $(RTE_OUTPUT)/.config ; \ cp $(RTE_OUTPUT)/.config_tmp $(RTE_OUTPUT)/.config.orig ; \ fi ; \ rm -f $(RTE_OUTPUT)/.config_tmp ; \ else \ $(MAKE) -rRf $(RTE_SDK)/mk/rte.sdkconfig.mk notemplate; \ fiendif CPP是Makefile内置变量，代表C语言预处理器，这里将会在RTE_OUTPUT目录下生成.config和.config.orig两个文件，最后生成了rte_config.h和Makefile文件。生成完所有依赖文件后，执行make depdirs，这个目标在rte.sdkroot.mk中： 12 3 make的流程会执行rte.sdkroot.mk中的1234# all other build targets%: $(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdkconfig.mk checkconfig $(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdkbuild.mk $@ 首先会检查是否完成了.config的生成，然后对rte.sdkbuild.mk进行make。1234567891011.PHONY: buildbuild: $(ROOTDIRS-y) @echo "Build complete [$(RTE_TARGET)]"$(ROOTDIRS-y) $(ROOTDIRS-): @[ -d $(BUILDDIR)/$@ ] || mkdir -p $(BUILDDIR)/$@ @echo "== Build $@" $(Q)$(MAKE) S=$@ -f $(RTE_SRCDIR)/$@/Makefile -C $(BUILDDIR)/$@ all @if [ $@ = drivers ]; then \ $(MAKE) -f $(RTE_SDK)/mk/rte.combinedlib.mk; \ fi]]></content>
      <categories>
        <category>Makefile</category>
        <category>DPDK</category>
      </categories>
      <tags>
        <tag>DPDK源码解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7中安装shadowsocks客户端并通过firefox进行智能代理]]></title>
    <url>%2Fpost%2F2017%2F10%2Fcentos7%E4%B8%AD%E5%AE%89%E8%A3%85shadowsocks%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%B9%B6%E9%80%9A%E8%BF%87firefox%E8%BF%9B%E8%A1%8C%E6%99%BA%E8%83%BD%E4%BB%A3%E7%90%86.html</url>
    <content type="text"><![CDATA[由于把台式机搬回寝室了，只能用实验室的服务器，服务器是Centos7的系统，有很多东西没有windows习惯。这里讲以下如何在Centos上翻墙。 shadowsocks（简称ss），在windows的客户端的配置很多，linux上却很少。 在centos7上的教程也有不少，但是他们都没讲清楚一个概念：ss-server 和 ss-local的区别。 这里推荐auooo大神的ss不完全指南 里面对于ss的原理介绍的非常浅显易懂。 我在这里说下结论：ss-server是搭建服务器用到的组件（就是你买了一个vps，要用它来翻墙。那么你要在这个vps上面搭建ss服务器，搭建好了你才能用你的笔记本啊，手机啊链接这个vps上网。） ss-local就是本地客户端。就是windows下面的那个ss。centos等linux版本上面，对应的就是ss-local。 安装ss服务现在讲讲客户端构筑步骤（服务端教程其实基本一样，都可以用同一个配置文件。只不过一个是ss-local启动，一个ss-server启动） git下载源码。这里采用源码安装。 git clone https://github.com/shadowsocks/shadowsocks-libev.git yum安装必要rpm yum install -y gcc automake autoconf libtool make build-essential curl curl-devel zlib-devel openssl-devel perl perl-devel cpio expat-devel gettext-devel git asciidoc xmlto 进入shadowsocks-libev目录，编译源码并安装 ./configure &amp;&amp; make; make install 编写配置文件 123456789101112131415mkdir ~/etc/shadowsocks/vim ~/etc/shadowsocks/sslocal.json#然后编辑如下内容，并保存&#123; "server":"ss帐号的地址", "server_port":"ss帐号的端口", "local_address":"127.0.0.1", "local_port":1080, "password":"帐号密码", "timeout":300, "method":"aes-256-cfb", "fast_open":false, "workers":1&#125; 5.添加开机自动启动的服务12345678910111213vim /etc/systemd/system/sslocal.service编辑为如下内容，并保存[Unit] Description=Shadowsocks [Service] TimeoutStartSec=0 #the_path_to为上一步创建的sslocal.json的绝对目录ExecStart=/usr/local/bin/ss-local -c the_path_to/sslocal.json [Install] WantedBy=multi-user.target 运行systemctl start sslocal启动服务 配置firefox浏览器的代理 安装foxyproxy插件，在adds-on搜索foxyproxy并安装， 设置foxyproxy， 在Proxies中点击Add New Proxy, 在General中输入名字，sslocal;并选一个颜色； 在Proxy Details中，选择 Manual Proxy Configuration, Server or IP Address设置为127.0.0.1； Port为1080； 并勾选SOCKS proxy, SOCKS v5 然后点击ok,创建该Proxy, 此时需要先将代理模式更改为sslocal,因为后面添加模式订阅可能需要翻墙， 在设置里面，配置Pattern Subscriptions， 直接点go,创建新的模式订阅 ； name 为gfwlist, description为gfwlist, URL为： https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt； 在proxies中添加刚刚创建的sslocal； Refresh Rate设置为600;格式设置为AutoProxy; Obfuscation设置为Base64; 可以在QuickAdd中开启快速添加， 最后将模式改为 “ues proxies based on pre-defined patterns and priorities”]]></content>
      <tags>
        <tag>Centos7</tag>
        <tag>Shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设置iptables的NAT转发，让内网的服务器通过NAT服务器访问外网]]></title>
    <url>%2Fpost%2F2017%2F9%2F%E8%AE%BE%E7%BD%AEiptables%E7%9A%84NAT%E8%BD%AC%E5%8F%91%EF%BC%8C%E8%AE%A9%E5%86%85%E7%BD%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%80%9A%E8%BF%87NAT%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%BF%E9%97%AE%E5%A4%96%E7%BD%91.html</url>
    <content type="text"><![CDATA[小组有了10台冷板式服务器(node1-node10)，和一台浸没式液冷服务器(node0)。但是网络中心只分配了一个公网ip给我们。我们把公网IP配置给了node0的ens5, node0的ens6为内网ip: 192.168.0.100node1-node10的IP为eth0: 192.168.0.101-192.168.0.110 为了使node1-10能够上外网，并且外网能够访问node1-10的服务，比如ssh,vnc等。我们使用iptables进行NAT转发。 iptables的介绍在Linux系统使用iptables实现防火墙、数据转发等功能。iptables有不同的表(table)，每个table有不同的链(chain)，每条chain有一个或多个规则(rule)。本文利用NAT(network address translation，网络地址转换)表来实现数据包的转发。iptables命令要用-t来指定表，如果没有指明，则使用系统缺省的表“filter”。所以使用NAT的时候，就要用“-t nat”选项了。NAT表有三条缺省的链，它们分别是PREROUTING、POSTROUTING和OUTPUT。 PREROUTING：在数据包传入时，就进到PREROUTIING链。该链执行的是修改数据包内的目的IP地址，即DNAT（变更目的IP地址）。PREROUTING只能进行DNAT。因为进行了DNAT，才能在路由表中做判断，决定送到本地或其它网口。 POSTROUTING：相对的，在POSTROUTING链后，就传出数据包，该链是整个NAT结构的最末端。执行的是修改数据包的源IP地址，即SNAT。POSTROUTING只能进行SNAT。 OUTPUT：定义对本地产生的数据包的目的NAT规则。 每个数据包都会依次经过三个不同的机制，首先是PREROUTING(DNAT)，再到路由表，最后到POSTROUTING(SNAT)。下面给出数据包流方向：文中的网络拓扑图所示的数据包，是从eth0入，eth1出。但是，无论从eth0到eth1，还是从eth1到eth0，均遵守上述的原理。就是说，SNAT和DNAT并没有规定只能在某一个网口(某一侧)。 顺便给出netfilter的完整结构图： 内网10个节点访问外网首先在node0上配置外网网卡ens5, 编辑’/etc/sysconfig/network-scripts/ifcfg-ens5’文件，123456789101112131415161718TYPE=EthernetHWADDR=e8:61:1f:12:51:18DEFROUTE=yesPEERDNS=yesPEERROUTES=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_PEERDNS=yesIPV6_PEERROUTES=yesIPV6_FAILURE_FATAL=noDEVICE=ens5BOOTPROTO="static"ONBOOT="yes"IPADDR="A.B.C.D"GATEWAY="A.B.C.D_"NETMASK="255.255.255.224" 然后配置node0上的内网网卡ens6, 编辑’/etc/sysconfig/network-scripts/ifcfg-ens6’文件，1234567891011121314151617TYPE=EthernetBOOTPROTO=staticDEFROUTE=yesPEERDNS=yesPEERROUTES=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_PEERDNS=yesIPV6_PEERROUTES=yesIPV6_FAILURE_FATAL=noDEVICE=ens6HWADDR=e8:61:1f:12:51:19IPADDR=192.168.0.100NETMASK=255.255.255.0ONBOOT=yes 注意不要配置GATEWAY 然后配置iptables1234iptables -A FORWARD -s 192.168.0.0/24 -j ACCEPTiptables -A FORWARD -d 192.168.0.0/24 -j ACCEPTiptables -t nat -A POSTROUTING -o ens5 -s 192.168.0.0/24 -j MASQUERADEiptables-save 这样node0就配置完成，然后就是配置node1-node101234route add default gw 192.168.0.100echo GATEWAY=192.168.0.100 &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0echo DNS1=114.114.114.114 &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0service network restart 需要配DNS服务器，不然只能通过IP地址访问外网，不能通过域名访问外网。 外网访问内网节点的服务这里需要用到DNAT。 比如需要访问node1的vnc 5901号窗口。我们将node0的59011映射到node1的5901运行如下命令：12345678910#设置DNAT让外网访问59011的端口的包，都转发到内网node1的5901iptables -t nat -A PREROUTING -d A.B.C.D -p tcp --dport 59011 -j DNAT --to-destination 192.168.0.101:5901#因为之前配置内网访问外网的时候，已经配置过SNAT和FORWARD了，下面的步骤不需要执行#用SNAT作源地址转换（关键），以使回应包能正确返回iptables -t nat -A POSTROUTING -d 192.168.0.101 -p tcp --dport 5901 -j SNAT --to 192.168.0.100# 还要打开FORWARD链的相关端口，特此增加iptables -A FORWARD -o end6 -d 10.1.1.27 -p tcp --dport 5901 -j ACCEPT iptables -A FORWARD -i ens6 -s 10.1.1.27 -p tcp --sport 5901 -j ACCEPT 最后iptables-save 如何同时开启TCP和UDP转发当只开启TCP转发时，用上一节的方法：1234567#设置DNAT让外网访问59011的端口的包，都转发到内网node1的5901iptables -t nat -A PREROUTING -d A.B.C.D -p tcp --dport 59011 -j DNAT --to-destination 192.168.0.101:5901#用SNAT作源地址转换（关键），以使回应包能正确返回iptables -t nat -A POSTROUTING -d 192.168.0.101 -p tcp --dport 5901 -j SNAT --to 192.168.0.100# 还要打开FORWARD链的相关端口，特此增加iptables -A FORWARD -o end6 -d 10.1.1.27 -p tcp --dport 5901 -j ACCEPT iptables -A FORWARD -i ens6 -s 10.1.1.27 -p tcp --sport 5901 -j ACCEPT 若要同时开启TCP和UDP，则12345678910#UDP iptables -t nat -A PREROUTING -d A.B.C.D -p udp --dport 59011 -j DNAT --to-destination 192.168.0.101:5901iptables -t nat -A POSTROUTING -s 192.168.0.101 -p udp --dport 5901 -j SNAT --to-source 192.168.0.100:59011iptables -A FORWARD -o end6 -d 192.168.0.101 -p tcp --dport 5901 -j ACCEPT iptables -A FORWARD -i ens6 -s 192.168.0.101 -p tcp --sport 5901 -j ACCEPT #TCPiptables -t nat -A PREROUTING -d A.B.C.D -p tcp --dport 59011 -j DNAT --to-destination 192.168.0.101:5901iptables -t nat -A POSTROUTING -s 192.168.0.101 -p udp --dport 5901 -j SNAT --to-source 192.168.0.100:59011iptables -A FORWARD -o end6 -d 192.168.0.101 -p tcp --dport 5901 -j ACCEPT iptables -A FORWARD -i ens6 -s 192.168.0.101 -p tcp --sport 5901 -j ACCEPT 对比发现，只是改了POSTROUTING链，将-d改为-s,然后将-to改为--to-source，其他不变。 可参考 iptables学习笔记：端口转发命令优化。12# 查看配置结果iptables -t nat -L]]></content>
      <categories>
        <category>Networking</category>
      </categories>
      <tags>
        <tag>Iptables</tag>
        <tag>NAT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numactl of centos]]></title>
    <url>%2Fpost%2F2017%2F9%2Fnumactl-of-centos.html</url>
    <content type="text"><![CDATA[今天编译最新的DPDK，出现了没有&lt;numa.h&gt; &lt;numaif.h&gt;等错误，最后上网查，和查看最新的DPDK文档，发现是需要libnuma-devel的库，可是用yum install libnuma-devel发现没有这个库。 google了一下，发现centos上是numactl-devel这个库， 下面有个一个 文章是介绍如何使用numactl这个工具的。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xeon CPU命名规则]]></title>
    <url>%2Fpost%2F2017%2F8%2FXeon-CPU%E5%91%BD%E5%90%8D%E8%A7%84%E5%88%99.html</url>
    <content type="text"><![CDATA[我们以新发布的E3-1220来对这个图表进行一个详细解析。 首先，Intel E3，E5，E7代表了3个不同档次的至强CPU，至强“E系列”的这种命名方式有些类似桌面上的Core i3，i5，i7；比较通俗易懂的解释就是可以对应我们的豪华汽车生产商宝马3系，5系和7系。分别对应好，更好和最好。 其次，E3-1220中的这个”1”，也就是连字符后的第一个数字，它代表处理器最多支持的并行路数，有1、2、4、8四种规格，分别代表了单路、双路、四路和八路。我们现在举例的这款E3-1220 至强CPU，连字符后的第一个数字是”1”，那么这款CPU就是一款单路的CPU，只能用于对应的单路的服务器主板上面。Intel 即将推出的E5-2400系列，E5-2600系列，相比于E3-1200系列来讲，E5代表了更高档次，更好性能，而连字符后的第一个数字为”2”，这里的2也代表了是双路的CPU，只能用于对应的双路芯片组的主板。紧接着，我们来看连字符后的第二个数字，它代表处理器封装接口形式，一共有2，4，6，8四种规格，分别是2对应Socket H2(LGA 1155)、4对应Socket B2(LGA 1356)、6对应Socket R(LGA 2011)、8对应Socket LS(LGA 1567)。我们现在举例的这款E3-1220 至强CPU，连字符后的第二个数字是”2”，2对应Socket H2(LGA 1155)，也就是说，这个CPU封装是Socket LGA 1155的。我们刚提到的Intel 即将推出的E5-2400系列，E5-2600系列至强，E5-2400系列中，连字符后的第二个数字是”4”，4对应Socket B2(LGA 1356)，那么这个CPU的封装就是Socket LGA 1356的。同理，E5-2600系列中， 6对应的是Socket LGA 2011。 然后，连字符后第三和第四位代表编号序列，一般是数字越大产品性能越高，价格也更贵。接下来，我们来看连字符后第四位数字后面的代表什么。紧跟第四位数字后的”L”代表是低功耗版，留空的话就代表是标准版。E3-1220，这个20后面留空代表是标准版，E3-1220L，20后面是”L”代表是低功耗版。连字符后面最后的数字代表修订版本，比如v2、v3、v4等等。 好了，基本上我们把新一代 Sandy Bridge 至强命名规则介绍清楚了，最后，我们再对比到以前的服务器至强CPU来一个总结：单路服务器：我们以后会接触到的新一代 Sandy Bridge 至强CPU系列会有E3-1200系列，E5-1400系列。这里需要注意的是，不像上一代产品，3系列的至强基本上就是单路服务器产品了，5系列的至强基本上就是双路的了，现在的E3-1200系列和E5-1400系列，都是属于单路服务器产品。处理器封装接口形式主要有”2”对应的LGA 1155和”4”对应的LGA 1356两种。入门级双路服务器：我们接触到的入门级双路服务器至强CPU，基本上就只有E5-2400系列，最多八核心十六线程。处理器封装接口形式为”4”对应的LGA 1356。高性能双路和四路服务器：高性能双路对应的至强CPU系列为 至强 E5-2600系列，四路对应为至强 E5-4600系列，都是”6”对应的LGA 2011封装接口，最多八核心十六线程、20MB三级缓存，四通道内存，支持PCI-E 3.0标准和40条PCI-E总线。]]></content>
      <categories>
        <category>CPU</category>
      </categories>
      <tags>
        <tag>Xeon</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dpdk_l3fwd-源码解析]]></title>
    <url>%2Fpost%2F2017%2F5%2Fdpdk-l3fwd-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html</url>
    <content type="text"><![CDATA[本文对DPDK17.02自带的example l3fwd进行走读解析。该sample application基于DPDK进行3层包转发。该application展示了使用DPDK中hash库和LPM库来实现包转发。初始化和runtime的过程很像l2fwd，主要区别就是l2fwd只是简单的将一个port的包全部转发到另一port,而l3fwd则是根据收到的包的information来决定转发到哪个port。 我们从main()函数开始进行分析。首先调用rte_eal_init(argc, argv)进行EAL环境的初始化。然后注册系统信号(signal()函数);在然后设置每个port的目的mac address，存在全局数组dest_eth_addr[]中，l3fwd默认的将经过portid号port的包的目的mac address改为02:00:00:00:00:00:portid123456/* pre-init dst MACs for all ports to 02:00:00:00:00:xx */ for (portid = 0; portid &lt; RTE_MAX_ETHPORTS; portid++) &#123; dest_eth_addr[portid] = ETHER_LOCAL_ADMIN_ADDR + ((uint64_t)portid &lt;&lt; 40); *(uint64_t *)(val_eth + portid) = dest_eth_addr[portid]; &#125; parse_args(argc, argv)然后调用parse_args(argc, argv)，解析l3fwd的参数。1234567891011121314151617181920212223242526272829303132/* display usage */static voidprint_usage(const char *prgname)&#123; printf("%s [EAL options] --" " -p PORTMASK" " [-P]" " [-E]" " [-L]" " --config (port,queue,lcore)[,(port,queue,lcore)]" " [--eth-dest=X,MM:MM:MM:MM:MM:MM]" " [--enable-jumbo [--max-pkt-len PKTLEN]]" " [--no-numa]" " [--hash-entry-num]" " [--ipv6]" " [--parse-ptype]\n\n" " -p PORTMASK: Hexadecimal bitmask of ports to configure\n" " -P : Enable promiscuous mode\n" " -E : Enable exact match\n" " -L : Enable longest prefix match (default)\n" " --config (port,queue,lcore): Rx queue configuration\n" " --eth-dest=X,MM:MM:MM:MM:MM:MM: Ethernet destination for port X\n" " --enable-jumbo: Enable jumbo frames\n" " --max-pkt-len: Under the premise of enabling jumbo,\n" " maximum packet length in decimal (64-9600)\n" " --no-numa: Disable numa awareness\n" " --hash-entry-num: Specify the hash entry number in hexadecimal to be setup\n" " --ipv6: Set if running ipv6 packets\n" " --parse-ptype: Set to use software to analyze packet type\n\n", prgname);&#125; 在parse_args中 -p: 设置enable_port_mask的值 -P: 设置promiscuous_on，是否开启port的混杂模式 -E: 设置l3fwd_em_on, 是否启用exact match -L: 设置l3fwd_lpm_on, 是否启用long prefix match --config: 调用parse_config()，这里只配置Rx 队列的信息 将(port,queue,lcore)的映射关系存入lcore_params_array数组，最后把指针传给全局变量lcore_params (port,queue,lcore)的映射表示port 的 queue号队列映射给lcore处理。 --eth-dest: 调用parse_eth_dest() parse_eth_dest()函数，根据参数，将参数传入的mac address存入peer_addr[6]数组 然后用dest指针，指向dest_eth_addr,dest = (uint8_t *)&amp;dest_eth_addr[portid];， 在for循环中dest[c] = peer_addr[c],从而配置了dest_eth_addr[]，以更改不同的port对应的目的mac address --enable-jumbo: 设置port_conf.rxmode.jumbo_frame = 1; --max-pkt-len: 设置port_conf.rxmode.max_rx_pkt_len; --no-numa: 设置numa_on = 0;即不启用numa,默认值为1 --ipv6: 设置ipv6 = 1，即启用ipv6，默认值为0 --hash-entry-num: 设置全局变量hash_entry_number，范围为[1,L3FWD_HASH_ENTRIES(1M)] --parse-ptype: 设置全局变量parse_ptype = 1，默认值为0; 当为1时，即启用rx callback软件方式分析包的type 当为0时，启用硬件分析。 后面判断如果l3fwd_lpm_on和l3fwd_em_on同时都被置1，则退出；若都没被置1，则将l3fwd_lpm_on置1，表示默认使用long prefix match;然后若是l3fwd_lpm_on，则禁用ipv6,并将hash_entery_number置为默认值4，因为ipv6和hash只有在exact match时使用。 调用init_lcore_rx_queues()函数，配置全局数组lcore_conf[]数组中每个具体的lcore所要处理的 n_rx_queue接收队列数， 以及rx_queue_list; 1234567891011121314151617struct lcore_conf lcore_conf[RTE_MAX_LCORE];struct lcore_rx_queue &#123; uint8_t port_id; uint8_t queue_id;&#125; __rte_cache_aligned;struct lcore_conf &#123; uint16_t n_rx_queue; struct lcore_rx_queue rx_queue_list[MAX_RX_QUEUE_PER_LCORE]; uint16_t n_tx_port; uint16_t tx_port_id[RTE_MAX_ETHPORTS]; uint16_t tx_queue_id[RTE_MAX_ETHPORTS]; struct mbuf_table tx_mbufs[RTE_MAX_ETHPORTS]; void *ipv4_lookup_struct; void *ipv6_lookup_struct;&#125; __rte_cache_aligned; 调用check_port_config(nb_ports)检查--config配置的port是否被enabled_port_mask所允许和是否超过了nb_ports的限制。 建立查找表然后运行setup_l3fwd_lookup_tables()，设置建立3层转发需要的转发查找表的函数调用。根据全局变量l3fwd_em_on判断是采用exact match还是Long prefix match.12345678910static voidsetup_l3fwd_lookup_tables(void)&#123; /* Setup HASH lookup functions. */ if (l3fwd_em_on) l3fwd_lkp = l3fwd_em_lkp; /* Setup LPM lookup functions. */ else l3fwd_lkp = l3fwd_lpm_lkp;&#125; 初始化各个ports12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/* initialize all ports */ for (portid = 0; portid &lt; nb_ports; portid++) &#123; /* ...*/ nb_rx_queue = get_port_n_rx_queues(portid); n_tx_queue = nb_lcores; if (n_tx_queue &gt; MAX_TX_QUEUE_PER_PORT) n_tx_queue = MAX_TX_QUEUE_PER_PORT; printf("Creating queues: nb_rxq=%d nb_txq=%u... ", nb_rx_queue, (unsigned)n_tx_queue ); ret = rte_eth_dev_configure(portid, nb_rx_queue, (uint16_t)n_tx_queue, &amp;port_conf); if (ret &lt; 0) rte_exit(EXIT_FAILURE, "Cannot configure device: err=%d, port=%d\n", ret, portid); rte_eth_macaddr_get(portid, &amp;ports_eth_addr[portid]); print_ethaddr(" Address:", &amp;ports_eth_addr[portid]); printf(", "); print_ethaddr("Destination:", (const struct ether_addr *)&amp;dest_eth_addr[portid]); printf(", "); /* * prepare src MACs for each port. */ ether_addr_copy(&amp;ports_eth_addr[portid], (struct ether_addr *)(val_eth + portid) + 1); /* init memory */ ret = init_mem(NB_MBUF); if (ret &lt; 0) rte_exit(EXIT_FAILURE, "init_mem failed\n"); /* init one TX queue per couple (lcore,port) */ queueid = 0; for (lcore_id = 0; lcore_id &lt; RTE_MAX_LCORE; lcore_id++) &#123; if (rte_lcore_is_enabled(lcore_id) == 0) continue; if (numa_on) socketid = (uint8_t)rte_lcore_to_socket_id(lcore_id); else socketid = 0; printf("txq=%u,%d,%d ", lcore_id, queueid, socketid); fflush(stdout); rte_eth_dev_info_get(portid, &amp;dev_info); txconf = &amp;dev_info.default_txconf; if (port_conf.rxmode.jumbo_frame) txconf-&gt;txq_flags = 0; ret = rte_eth_tx_queue_setup(portid, queueid, nb_txd, socketid, txconf); if (ret &lt; 0) rte_exit(EXIT_FAILURE, "rte_eth_tx_queue_setup: err=%d, " "port=%d\n", ret, portid); qconf = &amp;lcore_conf[lcore_id]; qconf-&gt;tx_queue_id[portid] = queueid; queueid++; qconf-&gt;tx_port_id[qconf-&gt;n_tx_port] = portid; qconf-&gt;n_tx_port++; &#125; printf("\n"); &#125; 首先调用get_port_n_rx_queues(portid)获取portid表示的port有几个rx queues; 设置n_tx_queue = nb_lcores;即为应用启动的时候EAL参数设置的用几个lcore; 然后调用rte_eth_dev_configure(portid, nb_rx_queue,(uint16_t)n_tx_queue, &amp;port_conf)配置该port; 注意上一步最后传入的port_conf,在参数解析的时候，可能会设置port_conf.rxmode.jumbo_frame和port_conf.rxmode.max_rx_pkt_len;所以这里有几个lcore，对应就会给每个启用的port配置几个TX queues 然后读取该port的MAC ADDRESS并输出，同时也输出该port的目的mac address，如果有用--eth-dest设置则为设置的值，否则为默认的值； 然后将该port的mac address 复制到val_eth结构里面，作为该port的源mac address存储。 然后调用init_mem(NB_MBUF)初始化内存，该函数中用for循环对每个启用的lcore进行处理， 首先根据是否启用了NUMA,来确定该lcore对应的NUMA节点号（socketid） 然后根据socketid，判断该socketid上的mempool是否创建了，已经存在的话，跳下一步；没有的话，则创建一个mempoolpktmbuf_pool[socketid] = rte_pktmbuf_pool_create(s, nb_mbuf, MEMPOOL_CACHE_SIZE, 0, RTE_MBUF_DEFAULT_BUF_SIZE, socketid);并创建该socket上的查找表l3fwd_lkp.setup(socketid) 获取该lcore的配置结构体qconf = &amp;lcore_conf[lcore_id]，分别指定qconf-&gt;ipv4_lookup_struct和qconf-&gt;ipv6_lookup_struct 然后初始化TX queue（有几个lcore，就为每个port配置几个tx queue） 首先根据是否启用NUMA,确定socketid 然后调用rte_eth_dev_info_get(portid, &amp;dev_info)读取该port的设备信息; 调用txconf = &amp;dev_info.default_txconf，获取tx的配置信息； 根据是否启用jumbo frame，配置txconf-&gt;txq_flags; 然后rte_eth_tx_queue_setup(portid, queueid, nb_txd,socketid, txconf)创建一个接收队列; 最后获取该lcore_id的lcore_conf[]配置数组，并更新tx相关的信息，比如：qconf-&gt;tx_queue_id[port]表示该lcore处理portid的那个queue;qconf-&gt;n_tx_port表示该lcore需要处理几个port的tx queue;qconf-&gt;tx_port_id[qconf-&gt;n_tx_port]表示该lcore需要处理的tx queue的port_id; 然后初始化rx queue,for循环启用的lcores. 首先获取lcore的配置信息，qconf = &amp;lcore_conf[lcore_id] 然后for循环根据配置文件中给这个lcore配置了需要处理几个rx queue,依次对每个rx queue进行配置for(queue = 0; queue &lt; qconf-&gt;n_rx_queue; ++queue) 首先读取portid和queueid信息， 然后根据是否启用numa_on，设置socketid 然后调用rte_eth_rx_queue_setup(portid, queueid, nb_rxd,socketid,NULL,pktmbuf_pool[socketid]);建立tx queue 启动各个port和运行l3fwd包转发的死循环 对各个启用的port调用rte_eth_dev_start(portid)，如果启用了混杂模式，则还需要调用rte_eth_promiscuous_enable(portid) 对各个启用的lcore，先获取其配置数组qconf= &amp;lcore_conf[lcore_id]，然后调用prepare_ptype_parser(portid, queueid),验证该lcore需要处理的portid号port的queueid队列是否可能解析包的类型； 调用rte_eal_mp_remote_launch(l3fwd_lkp.main_loop, NULL, CALL_MASTER);在每个lcore上启用l3fwd_lkp.main_loop死循环方法。 总结整个l3fwd主要做了这几件事： 首先调用rte_eal_init(argc,argv)，初始化EAL； 调用setup_l3fwd_lookup_tables（）设置全局变量l3fwd_lkp是指向exact match结构还是long prefix match结构； 然后在for循环中对每个enabled port调用rte_eth_dev_configure(portid, nb_rx_queue,(uint16_t)n_tx_queue, &amp;port_conf);配置各个port,其中设置了rx queue和tx queue的数目，还有port的配置结构体port_conf; 在同样的for循环中，对每个enabled port调用init_mem()，该函数其实和port没有关系，不知道为什么要放在port的循环中每次调用；该函数里面对enabled lcore进行循环，做了2件事： 判断该lcore属于的socket,是否已经创建了对应的pktmbuf_pool，没有则创建,并在创建的时候，调用l3fwd_lkp.setup(socketid)初始化查找表，可见查找表和socket相关； 配置lcore的配置数组lcore_conf[lcore_id]的ipv4_lookup_struct和ipv6_lookup_struct 在同样的for循环中，对每个enabled port,进行enable lcore循环， 调用rte_eth_dev_info_get(portid, &amp;dev_info)读取port的配置，然后根据读取的dev_info配置txconf, 调用rte_eth_tx_queue_setup(portid, queueid, nb_txd, socketid, txconf)建立tx queue; 说明：这里的2层循环是因为每个lcore会为每个port建立一个tx queue 最后会更新lcore的配置数组lcore_conf[lcore_id] 在新的for循环中，对enabled lcore进行循环，根据lcore的配置数组lcore_conf[lcore_id]中的n_rx_queue进行循环， 调用rte_eth_rx_queue_setup(portid, queueid, nb_rxd, socketid, NULL, pktmbuf_pool[socketid]),建立rx queue; 说明：与第5步不同，每个port的rx queue得根据应用的启动时传入的参数--config配置的lcore_conf建立 对每个enabled port进行循环调用rte_eth_dev_start(portid)和rte_eth_promiscuous_enable(portid)，分别启动每个port和设置port为混杂模式； ptype_parser的处理； 调用rte_eal_mp_remote_launch(l3fwd_lkp.main_loop, NULL, CALL_MASTER)，在每个enabled lcore上启动l3fwd_lkp.main_loop，准备对接收到的数据进行转发；]]></content>
      <categories>
        <category>DPDK源码解析</category>
      </categories>
      <tags>
        <tag>DPDK</tag>
        <tag>l3fwd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dpdk l2fwd 源码解析]]></title>
    <url>%2Fpost%2F2017%2F5%2Fdpdk-l2fwd-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html</url>
    <content type="text"><![CDATA[本文对DPDK17.02自带的example l2fwd进行走读解析。 从main()函数进行分析，首先调用rte_eal_init(argc, argv)进行EAL环境的初始化，然后执行12argc -= ret;argv += ret; 当初始化完EAL参数后，更新命令行参数，以便后面的参数被l2fwd这个特定的应用的参数解析函数l2fwd_parse_args(argc, argv)解析。 全局参数1234567891011121314/* ethernet addresses of ports */存储网络ports对应的mac addressstatic struct ether_addr l2fwd_ports_eth_addr[RTE_MAX_ETHPORTS];/* mask of enabled ports */启用的网络port的掩码，可以根据应用的参数设置static uint32_t l2fwd_enabled_port_mask = 0;/* list of enabled ports */存储不同的网络ports的转发port,即port 0转到port 1static uint32_t l2fwd_dst_ports[RTE_MAX_ETHPORTS];设置一个lcore处理几个rx队列static unsigned int l2fwd_rx_queue_per_lcore = 1; l2fwd_parse_args1234567891011121314/* display usage */static voidl2fwd_usage(const char *prgname)&#123; printf("%s [EAL options] -- -p PORTMASK [-q NQ]\n" " -p PORTMASK: hexadecimal bitmask of ports to configure\n" " -q NQ: number of queue (=ports) per lcore (default is 1)\n" " -T PERIOD: statistics will be refreshed each PERIOD seconds (0 to disable, 10 default, 86400 maximum)\n" " --[no-]mac-updating: Enable or disable MAC addresses updating (enabled by default)\n" " When enabled:\n" " - The source MAC address is replaced by the TX port MAC address\n" " - The destination MAC address is replaced by 02:00:00:00:00:TX_PORT_ID\n", prgname);&#125; 该函数给出了l2fwd支持的参数说明。 下面来看l2fwd_parse_args(argc, argv)函数， 首先调用getopt_long解析--开始的长参数，即--no-mac-updating或--mac-updating，来设置mac_updating的值，后面根据这个全局变量的值来决定转发的时候是否更新mac address; -p: 将需要使用的ports的掩码存在l2fwd_enabled_port_mask里面, 默认值为0； -q: 设置l2fwd_rx_queue_per_lcore的值，表示一个lcore可以处理几个rx queue， 默认值为1； -T: 设置timer_period，表示statistics timer_period秒输出一次 后面调用rte_pktmbuf_pool_create()创建该应用使用的内存池mempool; 然后执行12345678910111213141516171819202122232425262728/* reset l2fwd_dst_ports */ 首先重置所有port的目的转发port for (portid = 0; portid &lt; RTE_MAX_ETHPORTS; portid++) l2fwd_dst_ports[portid] = 0; last_port = 0; 依次两两成对的组成转发pair for (portid = 0; portid &lt; nb_ports; portid++) &#123; /* skip ports that are not enabled */ if ((l2fwd_enabled_port_mask &amp; (1 &lt;&lt; portid)) == 0) continue; if (nb_ports_in_mask % 2) &#123; l2fwd_dst_ports[portid] = last_port; l2fwd_dst_ports[last_port] = portid; &#125; else last_port = portid; nb_ports_in_mask++; rte_eth_dev_info_get(portid, &amp;dev_info); &#125; 当l2fwd_enabled_port_mask掩码所表示的ports数目（nb_ports_in_mask）为奇数时 if (nb_ports_in_mask % 2) &#123; printf("Notice: odd number of ports in portmask.\n"); l2fwd_dst_ports[last_port] = last_port; &#125; 设置启用的网络ports对应的转发port，该l2fwd应用应该启用偶数个ports,一次两两组成pair，相互转发数据； 根据l2fwd_rx_queue_per_lcore设置lcore_queue_conf数组，即每个lcore可以处理几个rx队列12345678910111213141516171819202122232425262728293031 rx_lcore_id = 0; qconf = NULL;/* Initialize the port/queue configuration of each logical core */ for (portid = 0; portid &lt; nb_ports; portid++) &#123; /* skip ports that are not enabled */ if ((l2fwd_enabled_port_mask &amp; (1 &lt;&lt; portid)) == 0) continue; /* get the lcore_id for this port */ // rx_lcore_id从0开始 // 如果rx_lcore_id表示的lcore没有被启用， // 或者该lcore对应的lcore_queue_conf数组的n_rx_port(分配的要处理的rx port的数目)已经达到了设置的l2fwd_rx_queue_per_lcore // 则检查下一个lcore while (rte_lcore_is_enabled(rx_lcore_id) == 0 || lcore_queue_conf[rx_lcore_id].n_rx_port == l2fwd_rx_queue_per_lcore) &#123; rx_lcore_id++; if (rx_lcore_id &gt;= RTE_MAX_LCORE) rte_exit(EXIT_FAILURE, "Not enough cores\n"); &#125; if (qconf != &amp;lcore_queue_conf[rx_lcore_id]) /* Assigned a new logical core in the loop above. */ qconf = &amp;lcore_queue_conf[rx_lcore_id]; qconf-&gt;rx_port_list[qconf-&gt;n_rx_port] = portid; qconf-&gt;n_rx_port++; printf("Lcore %u: RX port %u\n", rx_lcore_id, (unsigned) portid); &#125; 12345struct lcore_queue_conf &#123; unsigned n_rx_port; unsigned rx_port_list[MAX_RX_QUEUE_PER_LCORE];&#125; __rte_cache_aligned;struct lcore_queue_conf lcore_queue_conf[RTE_MAX_LCORE]; lcore_queue_conf结构体数组，存储对应的lcore的分配情况，比如lcore_queue_conf[i]表示第i号lcore总共需要处理lcore_queue_conf[i].n_rx_port个RX 网络port,并且这n_rx_port个网络port的编号存在lcore_queue_conf[i].rx_port_list数组里面； 初始化所有enabled ports12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/* Initialise each port */ for (portid = 0; portid &lt; nb_ports; portid++) &#123; /* skip ports that are not enabled */ if ((l2fwd_enabled_port_mask &amp; (1 &lt;&lt; portid)) == 0) &#123; printf("Skipping disabled port %u\n", (unsigned) portid); nb_ports_available--; continue; &#125; /* init port */ printf("Initializing port %u... ", (unsigned) portid); fflush(stdout); ret = rte_eth_dev_configure(portid, 1, 1, &amp;port_conf); if (ret &lt; 0) rte_exit(EXIT_FAILURE, "Cannot configure device: err=%d, port=%u\n", ret, (unsigned) portid); rte_eth_macaddr_get(portid,&amp;l2fwd_ports_eth_addr[portid]); /* init one RX queue */ fflush(stdout); ret = rte_eth_rx_queue_setup(portid, 0, nb_rxd, rte_eth_dev_socket_id(portid), NULL, l2fwd_pktmbuf_pool); if (ret &lt; 0) rte_exit(EXIT_FAILURE, "rte_eth_rx_queue_setup:err=%d, port=%u\n", ret, (unsigned) portid); /* init one TX queue on each port */ fflush(stdout); ret = rte_eth_tx_queue_setup(portid, 0, nb_txd, rte_eth_dev_socket_id(portid), NULL); if (ret &lt; 0) rte_exit(EXIT_FAILURE, "rte_eth_tx_queue_setup:err=%d, port=%u\n", ret, (unsigned) portid); /* Initialize TX buffers */ tx_buffer[portid] = rte_zmalloc_socket("tx_buffer", RTE_ETH_TX_BUFFER_SIZE(MAX_PKT_BURST), 0, rte_eth_dev_socket_id(portid)); if (tx_buffer[portid] == NULL) rte_exit(EXIT_FAILURE, "Cannot allocate buffer for tx on port %u\n", (unsigned) portid); rte_eth_tx_buffer_init(tx_buffer[portid], MAX_PKT_BURST); ret = rte_eth_tx_buffer_set_err_callback(tx_buffer[portid], rte_eth_tx_buffer_count_callback, &amp;port_statistics[portid].dropped); if (ret &lt; 0) rte_exit(EXIT_FAILURE, "Cannot set error callback for " "tx buffer on port %u\n", (unsigned) portid); /* Start device */ ret = rte_eth_dev_start(portid); if (ret &lt; 0) rte_exit(EXIT_FAILURE, "rte_eth_dev_start:err=%d, port=%u\n", ret, (unsigned) portid); printf("done: \n"); rte_eth_promiscuous_enable(portid); printf("Port %u, MAC address: %02X:%02X:%02X:%02X:%02X:%02X\n\n", (unsigned) portid, l2fwd_ports_eth_addr[portid].addr_bytes[0], l2fwd_ports_eth_addr[portid].addr_bytes[1], l2fwd_ports_eth_addr[portid].addr_bytes[2], l2fwd_ports_eth_addr[portid].addr_bytes[3], l2fwd_ports_eth_addr[portid].addr_bytes[4], l2fwd_ports_eth_addr[portid].addr_bytes[5]); /* initialize port stats */ memset(&amp;port_statistics, 0, sizeof(port_statistics)); &#125; 以上代码，在for循环中初始化每个网络port.首先根据l2fwd_enabled_port_mask来判断本次循环所要操作的portid是否被enabled，没有的话，就更新nb_ports_available的值，并进入下一次循环；否则就初始化该portid所表示的网卡port, 首先调用rte_eth_dev_configure(portid, 1, 1, &amp;port_conf)配置该portid的网卡port,其中第二2个参数是nb_rx_queue,第三个参数是nb_tx_queue，即配置该网卡一个rx队列和一个tx队列。 配置完成之后，调用rte_eth_macaddr_get(portid,&amp;l2fwd_ports_eth_addr[portid])将对应portid的port mac address读入之前定义的全局变量l2fwd_ports_eth_addr[portid]; 然后调用rte_eth_rx_queue_setup(portid, 0, nb_rxd, rte_eth_dev_socket_id(portid), NULL, l2fwd_pktmbuf_pool);从l2fwd_pktmbuf_pool内存池（main函数申请，见上面）中为receive ring申请nb_rxd个receive desciptor，其中第二个参数0表示该rx queue的rx_queue_id（该值属于[0, nb_rx_queue -1], nb_rx_queue即为第一步中rte_eth_dev_configure中配置的rx queue的数目，这里为1）；其中第5个参数为const struct rte_eth_rxconf *rx_conf,这里传入的为NULL，即不配置。 然后调用`rte_eth_tx_queue_setup(portid, 0, nb_txd, rte_eth_dev_socket_id(portid), NULL);` 注意参数基本与上一步相同，只是最后一个参数没有加pktmbuf_pool,为什么呢，请看第5步 初始化Tx buffers 12345678910111213141516/* Initialize TX buffers */ tx_buffer[portid] = rte_zmalloc_socket(&quot;tx_buffer&quot;, RTE_ETH_TX_BUFFER_SIZE(MAX_PKT_BURST), 0, rte_eth_dev_socket_id(portid)); if (tx_buffer[portid] == NULL) rte_exit(EXIT_FAILURE, &quot;Cannot allocate buffer for tx on port %u\n&quot;, (unsigned) portid); rte_eth_tx_buffer_init(tx_buffer[portid], MAX_PKT_BURST); ret = rte_eth_tx_buffer_set_err_callback(tx_buffer[portid], rte_eth_tx_buffer_count_callback, &amp;port_statistics[portid].dropped); if (ret &lt; 0) rte_exit(EXIT_FAILURE, &quot;Cannot set error callback for &quot; &quot;tx buffer on port %u\n&quot;, (unsigned) portid); 其中tx_buffer为定义为全局变量static struct rte_eth_dev_tx_buffer *tx_buffer[RTE_MAX_ETHPORTS];利用void * rte_zmalloc_socket(const char *type, size_t size, unsigned align, int socket);为该portid动态申请zero填充的空间，注意申请的size为RTE_ETH_TX_BUFFER_SIZE(MAX_PKT_BURST),该宏表示的大小为 MAX_PKT_BURST sizeof(struct rte_mbuf ) + sizeof(struct rte_eth_dev_tx_buffer)即MAX_PKT_BURST个ret_mbuf的指针(存放要发送的包)，和一个自己指向的结构体的大小(tx_buffer[i] 为struct rte_eth_dev_tx_buffer * 的指针)。 然后调用rte_eth_tx_buffer_init(tx_buffer[portid], MAX_PKT_BURST);来初始化上一步申请的tx_buffer; 然后设置错误处理回调函数 调用rte_eth_dev_start(portid)来启动该网卡port，并设置为混杂模式，输出该prot的mac address,并将port的统计数据置0 然后调用check_all_ports_link_status(nb_ports, l2fwd_enabled_port_mask);检查每个port的状态，最后调用rte_eal_mp_remote_launch(l2fwd_launch_one_lcore, NULL, CALL_MASTER)在所有核上调用l2fwd_launch_one_lcore（）函数(包括主lcore) 当lcore退出l2fwd_launch_one_lcore()死循环函数后，就检查其他lcore的状态，置ret，最后for循环stop,close各个port程序就退出了。 l2fwd_launch_one_lcore()死循环处理函数下面重点来看]]></content>
      <categories>
        <category>DPDK源码解析</category>
      </categories>
      <tags>
        <tag>l2fwd</tag>
        <tag>DPDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建shadowsocks server]]></title>
    <url>%2Fpost%2F2017%2F4%2F%E6%90%AD%E5%BB%BAshadowsocks-server.html</url>
    <content type="text"><![CDATA[自从搬了新实验室，虽然都是校内网，无奈网络差太多，LOL都打不了。 今天发现了有Proxifier这个东西，它可以将基于SOCK5的代理变为操作系统全局的。这样就可以先ssh连上服务器221（221服务器所在的机房的网络好嘛，打游戏方便）。在221的ssh上创建一个基于SOCK5的动态隧道。然后用Proxifier将这个SOCK5的代理设为全局的，就可以用221服务器那边的网络玩LOL啦。 可是实验发现基于SSH隧道的SOCK5代理，用了proxifier还是只能代理浏览器的流量，真烦心。没办法，只能尝试用SS的SOCK5代理了，首先用我的境外SS账号测试，发现基于SS的proxifier全局代理可以代理其他应用（非浏览器，如chrome）的流量。那么要用221的网打LOL,只能在221上装一个SS的server了。 安装shadowsocks servershadowsocks server为shadowsocks-libev[1]https://github.com/shadowsocks/shadowsocks-libev具体参见其README文档的Installation部分，221服务器为centos 7系统，yum安装的话，需要先下载对应的yum repo，1wget https://copr.fedorainfracloud.org/coprs/librehat/shadowsocks/repo/epel-7/librehat-shadowsocks-epel-7.repo 然后复制到/etc/yum.repos.d,然后激活，最后 install shadowsocks-libev via yum123cp ./librehat-shadowsocks-epel-7.repo /etc/yum.repos.d/su -c &apos;yum update&apos;su -c &apos;yum install shadowsocks-libev&apos; 安装完成之后呢，进行配置1234567891011vim /etc/shadowsocks-libev/config.json&#123; &quot;server&quot;:[&quot;[::0]&quot;,&quot;0.0.0.0&quot;], &quot;server_port&quot;:your_server_port, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;your_password&quot;, &quot;timeout&quot;:600, &quot;method&quot;:&quot;aes-256-cfb&quot;&#125; 最后启动服务1systemctl start shadowsocks-libev.service 用脚本安装123wget --no-check-certificate -O shadowsocks-libev.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-libev.shchmod +x shadowsocks-libev.sh./shadowsocks-libev.sh 2&gt;&amp;1 | tee shadowsocks-libev.log 安装完成后，按脚本提示配置,提示如下12345678910Congratulations, Shadowsocks-libev install completed!Your Server IP:your_server_ipYour Server Port:your_server_portYour Password:your_passwordYour Local IP:127.0.0.1Your Local Port:1080Your Encryption Method:aes-256-cfbWelcome to visit:https://teddysun.com/357.htmlEnjoy it! 本脚本安装完成后，会将 Shadowsocks-libev 加入开机自启动。 卸载的方法./shadowsocks-libev.sh uninstall 以下转自网络：http://www.360doc.com/content/17/0427/23/42433521_649228978.shtml Shadowsocks 有几种版本？区别是什么？首先要明确一点，不管 Shadowsocks 有几种版本，都分为服务端和客户端，服务端是部署在服务器（VPS）上的，客户端是在你的电脑上使用的。Shadowsocks 服务端大体上有 4 种版本，按照程序语言划分，分别为 Python ，libev ，Go ， Nodejs ，目前主流使用前 3 种。Shadowsocks 客户端几乎包括了所有的终端设备，PC ，Mac ，Android ，iOS ，Linux 等。 Shadowsocks 的最低安装需求是多少？个人建议最少 128MB 内存，因为在连接数比较多的情况下，还是占用不少内存的，如果内存不足，进程就会被系统 kill 掉，这时候就需要手动重启进程。当然，低于 128MB 也是可以安装的，Go 版是二进制安装，无需编译，非常简单快捷，libev 版运行过程中，占用内存较少，可以搭建在 Openwrt 的路由器上。自己个人使用，且连接数不是特别大的情况下，64MB 内存也基本够用了。如果你要分享给朋友们一起使用，最好还是选用大内存的。 为什么我安装（启动） Shadowsocks 失败？我只能说脚本并没有在所有的 VPS 上都测试过，所以遇到问题是在所难免的。大部分情况下，请参考《Troubleshooting》一文，自行解决。据我所知，很多人都是配置文件出了问题导致的启动失败。还有部分是改错了 iptables 导致的。在 Amazon EC2 ，百度云，青云上启动失败，连接不上怎么办？在这类云 VPS 上搭建，需要注意，配置服务器端时，应使用内网IP；Amazon EC2 缺省不允许 inbound traffic，需要在security group里配置允许连接的端口，和开通SSH client连接类似，这个在 Amazon EC2 使用指南里有说明。同样的，青云，百度云也差不多，默认不允许入网流量，网卡绑定的是内网IP，因此需要将配置文件里的 server 值改为对应的内网 IP 后再重新启动。然后在云管理界面，允许入网端口。我帮人设置了过之后，才发觉这些云和普通的 VPS 不一样，所以需要注意以上事项。 Shadowsocks 有没有控制面板？答案是有的，有人基于 PHP + MySQL 写出来一个前端控制面板，被很多人用来发布收费或免费的 Shadowsocks 服务。Github 地址如下：https://github.com/orvice/ss-panel具体怎么安装和使用，别来问我，自己研究去。 多用户怎么开启？Shadowsocks 有多种服务端程序，目前据我所知只有 Python 和 Go 版是支持在配置文件里直接设置多端口的，至于 libev 版则需要使用多个配置文件并开启多个实例才行。所谓的多用户，其实就是把不同的端口给不同的人使用，每个端口则对应不同的密码。Python 和 Go 版通过简单的修改单一配置文件，然后重启程序即可。 Shadowsocks 一键安装脚本（四合一）：系统支持：CentOS 6+，Debian 7+，Ubuntu 12+内存要求：≥128M日期 ：2017 年 01 月 21 日 关于本脚本： 一键安装 Shadowsocks-Python， ShadowsocksR， Shadowsocks-Go， Shadowsocks-libev 版（四选一）服务端； 各版本的启动脚本及配置文件名不再重合； 每次运行可安装一种版本； 支持以多次运行来安装多个版本，且各个版本可以共存（注意端口号需设成不同）； 若已安装多个版本，则卸载时也需多次运行（每次卸载一种）； Shadowsocks-Python 和 ShadowsocksR 安装后不可同时启动（因为本质上都属 Python 版）。 默认配置服务器端口：自己设定（如不设定，默认为 8989）密码：自己设定（如不设定，默认为 teddysun.com）备注：脚本默认创建单用户配置文件，如需配置多用户，请手动修改相应的配置文件后重启即可。 使用方法使用root用户登录，运行以下命令：12345678910111213141516第一行wget –no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh第二行chmod +x shadowsocks-all.sh第三行./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log安装完成后，脚本提示如下Congratulations, your_shadowsocks_version install completed!Your Server IP :your_server_ipYour Server Port :your_server_portYour Password :your_passwordYour Encryption Method:aes-256-cfbWelcome to visit:https://teddysun.com/486.htmlEnjoy it! 卸载方法：若已安装多个版本，则卸载时也需多次运行（每次卸载一种） 使用root用户登录，运行以下命令：./shadowsocks-all.sh uninstall 启动脚本：启动脚本后面的参数含义，从左至右依次为：启动，停止，重启，查看状态。1234567891011Shadowsocks-Python 版：/etc/init.d/shadowsocks-python start | stop | restart | statusShadowsocksR 版：/etc/init.d/shadowsocks-r start | stop | restart | statusShadowsocks-Go 版：/etc/init.d/shadowsocks-go start | stop | restart | statusShadowsocks-libev 版：/etc/init.d/shadowsocks-libev start | stop | restart | status Kcptun是一款加速软件VPS云端Kcptun配置软件:Xshell 安装代码:123456第一行：wget https://raw.githubusercontent.com/kuoruan/kcptun_installer/master/kcptun.sh第二行：chmod +x ./kcptun.sh第三行:./kcptun.sh 设置流程(注意点)： Kcptun 的服务端端口：请输入一个未被占用的端口，Kcptun 运行时将使用此端口。 设置加速的 IP：如果你想加速的Shadowsocks 就在运行在当前服务器上，直接回车即可。 设置需要加速的端口：即ss使用的服务器端口 当前没有软件使用此端口, 确定加速此端口?(y/n)y 设置 Kcptun 密码：如果不设置客户端就不用加 –key 这一参数(默认是 it’s a secrect ) 其余默认即可，可适当更改。 注意信息(客户端配置用)：服务器IP:45.32.60.33端口: 1139加速地址: 127.0.0.1加密方式 Crypt: none加速模式 Mode: fast 后期维护：1234561. 更新：./kcptun.sh update2. 重新配置：./kcptun.sh reconfig3. 卸载：./kcptun.sh uninstall 本地Kcptun客户端：本地 Windows 64位系统为例，首先下载 Kcptun 的 Windows 版本。 新建文件夹Kcptun下载https://github.com/xtaci/kcptun/releases/download/v20160906/kcptun-windows-amd64-20160906.tar.gz并解压到文件夹下。(已搬运) 下载Kcptun客户端配置管理工具，按照配置图完成设置(然后导入刚才解压的客户端)。 注意：Shadowsocks Android IOS等其他客户端 1、可以在关闭 KCP 协议的情况下，测试一下配置是不是正常，如果能正常联网，可以继续下一步，配置 KCP 协议。2、KCP 端口，即为加速端口。3、KCP 参数 -autoexpire 60 -key “你设的密码”]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux mmap()函数]]></title>
    <url>%2Fpost%2F2017%2F4%2Flinux-mmap-%E5%87%BD%E6%95%B0.html</url>
    <content type="text"><![CDATA[头文件：#include &lt;unistd.h&gt; #include &lt;sys/mman.h&gt; 定义函数：void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offsize); 参数说明： 参数说明start指向欲对应的内存起始地址，通常设为NULL，代表让系统自动选定地址，对应成功后该地址会返回。length代表将文件中多大的部分对应到内存。prot&nbsp;代表映射区域的保护方式，有下列组合：PROT_EXEC &nbsp;映射区域可被执行；PROT_READ &nbsp;映射区域可被读取；PROT_WRITE &nbsp;映射区域可被写入；PROT_NONE &nbsp;映射区域不能存取。flags会影响映射区域的各种特性：MAP_FIXED &nbsp;如果参数 start 所指的地址无法成功建立映射时，则放弃映射，不对地址做修正。通常不鼓励用此旗标。MAP_SHARED &nbsp;对应射区域的写入数据会复制回文件内，而且允许其他映射该文件的进程共享。MAP_PRIVATE &nbsp;对应射区域的写入操作会产生一个映射文件的复制，即私人的&quot;写入时复制&quot; (copy&nbsp;on write)对此区域作的任何修改都不会写回原来的文件内容。MAP_ANONYMOUS &nbsp;建立匿名映射，此时会忽略参数fd，不涉及文件，而且映射区域无法和其他进程共享。MAP_DENYWRITE &nbsp;只允许对应射区域的写入操作，其他对文件直接写入的操作将会被拒绝。MAP_LOCKED &nbsp;将映射区域锁定住，这表示该区域不会被置换(swap)。在调用mmap()时必须要指定MAP_SHARED 或MAP_PRIVATE。fdopen()返回的文件描述词，代表欲映射到内存的文件。offset文件映射的偏移量，通常设置为0，代表从文件最前方开始对应，offset必须是分页大小的整数倍。 返回值：若映射成功则返回映射区的内存起始地址，否则返回MAP_FAILED(-1)，错误原因存于errno 中。 错误代码： EBADF 参数fd 不是有效的文件描述词。 EACCES 存取权限有误。如果是MAP_PRIVATE 情况下文件必须可读，使用MAP_SHARED 则要有PROT_WRITE 以及该文件要能写入。 EINVAL 参数start、length 或offset 有一个不合法。 EAGAIN 文件被锁住，或是有太多内存被锁住。 ENOMEM 内存不足。 范例：利用mmap()来读取/etc/passwd 文件内容。1234567891011121314151617#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/mman.h&gt;main()&#123; int fd; void *start; struct stat sb; fd = open("/etc/passwd", O_RDONLY); /*打开/etc/passwd */ fstat(fd, &amp;sb); /* 取得文件大小 */ start = mmap(NULL, sb.st_size, PROT_READ, MAP_PRIVATE, fd, 0); if(start == MAP_FAILED) /* 判断是否映射成功 */ return; printf("%s", start); munma(start, sb.st_size); /* 解除映射 */ closed(fd);&#125; 执行结果：12345678910111213141516171819root : x : 0 : root : /root : /bin/bashbin : x : 1 : 1 : bin : /bin :daemon : x : 2 : 2 :daemon : /sbinadm : x : 3 : 4 : adm : /var/adm :lp : x :4 :7 : lp : /var/spool/lpd :sync : x : 5 : 0 : sync : /sbin : bin/sync :shutdown : x : 6 : 0 : shutdown : /sbin : /sbin/shutdownhalt : x : 7 : 0 : halt : /sbin : /sbin/haltmail : x : 8 : 12 : mail : /var/spool/mail :news : x :9 :13 : news : /var/spool/news :uucp : x :10 :14 : uucp : /var/spool/uucp :operator : x : 11 : 0 :operator : /root:games : x : 12 :100 : games :/usr/games:gopher : x : 13 : 30 : gopher : /usr/lib/gopher-data:ftp : x : 14 : 50 : FTP User : /home/ftp:nobody : x :99: 99: Nobody : /:xfs :x :100 :101 : X Font Server : /etc/xll/fs : /bin/falsegdm : x : 42 :42 : : /home/gdm: /bin/bashkids : x : 500 :500 :/home/kids : /bin/bash]]></content>
      <categories>
        <category>系统函数</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vc709 BAR空间地址&寄存器设置]]></title>
    <url>%2Fpost%2F2017%2F3%2Fvc709-BAR%E7%A9%BA%E9%97%B4%E5%9C%B0%E5%9D%80-%E5%AF%84%E5%AD%98%E5%99%A8%E8%AE%BE%E7%BD%AE.html</url>
    <content type="text"><![CDATA[本文记录Xilinx VC709这块开发板的PCI BAR空间的设置相关信息。 所有的Registers都是32 bits宽度的，从左只有依次是31位到0位； 所有没有被定义的bits都是保留的，当被读的时候默认返回0； reset之后，所有的registers都返回默认值； 所有的registers都被mapped to BAR0 DMA The Scatter Gather Packet DMA IP is provided by Northwest Logic.The DMA controller requires a 64KB register space mapped to BAR0.All DMA registers are mapped to BAR0 from 0x0000 to 0x7FFF. The address range from 0x8000 to 0xFFFF is availabel to you by way of this interface.The front-end of the DMA interfaces to the AXI4-stream interface on PCIe Endpoint IP core.The backend of the DMA provides an AXI4-stream interface as well, which connects to the user appplication side. Register address offsets from 0x0000 to 0x7FFF on BAR0 被DMA engine 自身使用 Address offset space on BAR0 from 0x800 to 0xFFFF is provided to user 0x9000 to 0x9FFF 分配给了user space registers 0xB000 to 0xEFFF 分配给了4个MACs （使用了1x5 AXI4LITE Interconnect 来route 不同的request 到对应的slave）123456789 DMA Channel | Offset from BAR0Channel 0 S2C | 0x0Channel 1 S2C | 0x100Channel 2 S2C | 0x200Channel 3 S2C | 0x300Channel 0 C2S | 0x2000Channel 1 C2S | 0x2100Channel 2 C2S | 0x2200Channel 3 C2S | 0x2300]]></content>
      <categories>
        <category>vc709 DMA 驱动</category>
      </categories>
      <tags>
        <tag>DMA</tag>
        <tag>VC709</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux内存分配方法总结]]></title>
    <url>%2Fpost%2F2017%2F3%2FLinux%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93.html</url>
    <content type="text"><![CDATA[本文转自： http://www.cnblogs.com/wenhuisun/archive/2013/05/15/3079722.html 介绍内存映射结构： 32位地址线寻址4G的内存空间，其中0-3G为用户程序所独有，3G-4G为内核占有。 struct page：整个物理内存在初始化时，每个4kb页面生成一个对应的struct page结构，这个page结构就独一无二的代表这个物理内存页面，并存放在mem_map全局数组中。 段式映射：首先根据代码段选择子cs为索引，以GDT值为起始地址的段描述表中选择出对应的段描述符，随后根据段描述符的基址，本段长度，权限信息等进行校验，校验成功后。cs:offset中的32位偏移量直接与本段基址相累加，得出最终访问地址。 0-3G与mem_map的映射方式：因linux中采用的段式映射为flat模式，所以从逻辑地址到线性地址没有变化。从段式出来进入页式，每个用户进程都独自拥有一个页目录表（pdt），运行时存放于CR3。CR3（页目录） + 前10位 =&gt; 页面表基址 + 中10位 =&gt; 页表项 + 后12位 =&gt; 物理页面地址 3G-4G与mem_map的映射方式：分为三种类型：低端内存/普通内存/高端内存。低端内存：3G-3G+16M 用于DMA pa线性映射普通内存：3G+16M-3G+896M pa线性映射 （若物理内存&lt;896M，则分界点就在3G+实际内存）高端内存：3G+896-4G 采用动态的分配方式 高端内存(假设3G+896为高端内存起址)作用：访问到1G以外的物理内存空间。线性地址共分为三段：vmalloc段/kmap段/kmap_atomic段（针对与不同的内存分配方式） 从内存分配函数的结构来看主要分为下面几个部分:a. 伙伴算法(最原始的面向页的分配方式)alloc_pages 接口： struct page alloc_page(unsigned int gfp_mask)——分配一页物理内存并返回该页物理内存的page结构指针。 struct page alloc_pages(unsigned int gfp_mask, unsigned int order)——分配 个连续的物理页并返回分配的第一个物理页的page结构指针。 &lt;释放函数：__free_page(s)&gt; 内核中定义：#define alloc_page(gfp_mask) alloc_pages(gfp_mask, 0) 最终都是调用 __alloc_pages. 其中MAX_ORDER 11，及最大分配到到页面个数为2^10（即4M）。 分配页后还不能直接用，需要得到该页对应的虚拟地址： void *page_address(struct page *page); 低端内存的映射方式：__va((unsigned long)(page - mem_map) &lt;&lt; 12) 高端内存到映射方式：struct page_address_map分配一个动态结构来管理高端内存。(内核是访问不到vma的3G以下的虚拟地址的) 具体映射由kmap / kmap_atomic执行。 get_free_page接口：(alloc_pages接口两步的替代函数) unsigned long get_free_page(unsigned int gfp_mask) unsigned long get_free_page(unsigned int gfp_mask) Unsigned long get_free_pages(unsigned int gfp_mask, unsigned int order) &lt;释放函数：free_page&gt; 与alloc_page(s)系列最大的区别是无法申请高端内存，因为它返回到是一个线性地址，而高端内存是需要额外映射才可以访问的。 b. slab高速缓存（反复分配很多同一大小内存） 注：使用较少 kmem_cache_t* xx_cache; 创建： xx_cache = kmem_cache_create(“name”, sizeof(struct xx), SLAB_HWCACHE_ALIGN, NULL, NULL); 分配： kmem_cache_alloc(xx_cache, GFP_KERNEL); 释放： kmem_cache_free(xx_cache, addr); 内存池 mempool 不使用。 c. kmalloc（最常用的分配接口） 注：必须小于128KB GFP_ATOMIC 不休眠，用于中断处理等情况 GFP_KERNEL 会休眠，一般状况使用此标记 GFP_USER 会休眠 __GFP_DMA 分配DMA内存 kmalloc/kfree d. vmalloc/vfree vmalloc采用高端内存预留的虚拟空间来收集内存碎片引起的不连续的物理内存页，是用于非连续物理内存分配。当kmalloc分配不到内存且无物理内存连续的需求时，可以使用。（优先从高端内存中查找） e. ioremap()/iounmap() ioremap()的作用是把device寄存器和内存的物理地址区域映射到内核虚拟区域，返回值为内核的虚拟地址。使用的线性地址区间也在vmmlloc段注：vmalloc()与 alloc_pages(_GFP_HIGHMEM)+kmap()；前者不连续，后者只能映射一个高端内存页面get_free_pages与alloc_pages(NORMAL)+page_address()； 两者完全等同内核地址通过 va/__pa进行中低内存的直接映射高端内存采用kmap/kmap_atomic的方式来映射 个人总结如下：a.在&lt;128kB的一般内存分配时，使用kmalloc 允许睡眠：GFP_KERNEL 不允许睡眠：GFP_ATOMICb.在&gt;128kB的内存分配时，使用get_free_pages，获取成片页面，直接返回虚拟地址（&lt;4M）（或alloc_pages + page_address）c.b失败， 如果要求分配高端内存：alloc_pages(_GFP_HIGHMEM)+kmap（仅能映射一个页面） 如果不要求内存连续： 则使用vmalloc进行分配逻辑连续的大块页面.(不建议)/分配速度较慢，访问速率较慢。d.频繁创建和销毁很多较大数据结构,使用slab.e.高端内存映射： 允许睡眠：kmap (永久映射) 不允许睡眠：kmap_atomic (临时映射)会覆盖以前到映射（不建议）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>DMA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vc709 start/stop Test 过程分析]]></title>
    <url>%2Fpost%2F2017%2F3%2Fvc709-start-stop-Test-%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[本文分析VC709板卡自带的TRD程序中对应JAVA GUI界面的start和stop按钮对应的startTest和stopTest功能。当选择performance mode的时候，GUI进入的界面可以设置四个DMA engine的包大小和选择对应的模式:performance: loopback,pktchk,pktgen;performance+raw_ethernet: loopback;然后点击start,则App程序开始发包，点stop则程序App程序停止发包。 下面具体分析点击start和stop时，GUI程序给xdma驱动发送了什么命令，对FPGA进行了怎么样的配置。 首先明确一点，startTest只用对TX engine进行配置，对应的RX Engine是不用配置的。 软件准备testCmd当软件设置好testmode后，调用StartTest(int statsfd, int engine, int testmode, int maxSize);第一个参数是xdma_stat的文件描述符，第二个参数为TX engine的编号(0,1,2,3),第三参数为上面设好的tmode,第四个参数为最大的包大小； 其中第三个参数testmode可取的值为下面宏的组合：12345678#define ENABLE_PKTCHK 0x00000100 /**&lt; Enable TX-side packet checker */#define ENABLE_PKTGEN 0x00000400 /**&lt; Enable RX-side packet generator */#define ENABLE_LOOPBACK 0x00000200 /**&lt; Enable loopback mode in test */#define ENABLE_CRISCROSS 0x00002000 /**&lt; Enable loopback mode in CRISCROSS test */#define TEST_STOP 0x00000000 /**&lt; Stop the test */#define TEST_START 0x00008000 /**&lt; Start the test */#define TEST_IN_PROGRESS 0x00004000 /**&lt; Test is in progress */ int testmode可取下面的值： ENABLE_LOOPBACK 只有loopback ENABLE_PKTCHK 只是硬件check ENABLE_PKTGEN 只是硬件generate ENABLE_PKTCHK|ENABLE_PKTGEN 硬件既check又generate 在StartTest函数中，根据第2,3,4 号参数初始化testCmd(TestCmd类型)，testCmd.Engine = engine;testCmd.TestMode = testmode;testCmd.MaxPktSize = maxSize; 并且最后testCmd.TestMode |= TEST_START，表示是start test；如果是Raw_ETH的performance，还要testCmd.TestMode |= ENABLE_CRISCROSS;最后将testCmd传给xdma的ioctl(); 总结 123456789101112131415testCmd.TestMode loopback check generate check|generateinit: 0x0000 0200 0x0000 0100 0x0000 0400 0x0000 0500 Not Raw_ETH: 0x0000 8200 0x0000 8100 0x0000 8400 0x0000 8500Raw_ETH: 0x0000 A200 0x0000 A100 0x0000 A400 0x0000 A500//即32位的寄存0000 0000 0000 0000 0000 0000 0000 0000 (从右往左第0位开始) //第8位:check位//第9位:loopback位//第10位:gen位//第14位：inprocess位//第15位：startTest位：1表示start,0 表示stop//第13位：criscross位，即raw_eth performance位 xdma驱动的ioctl()接收到命令然后看到xdma驱动的ioctl()函数:在ISTART_TEST和ISTOP_TEST的case分支时，用传入的testCmd赋值ustate,然后调用(uptr-&gt;UserSetState)(eptr, &amp;ustate, uptr-&gt;privData);这个函数才是真正的置寄存器函数。 sgusr.c中的mySetState我们看来sgusr.c中的mySetState其中要判断engien是否为TX engine ==&gt; if (privdata == 0x54545454);(说明了start test只用设置TX engine)然后RawTestMode = ustate-&gt;TestMode； 然后根据RawTestMOde来初始化testmode = 0;12345678910111213141516171819202122232425262728293031if (RawTestMode &amp; TEST_START) //见下面的解释，给testmode置值&#123; testmode = 0; if (RawTestMode &amp; ENABLE_LOOPBACK) testmode |= LOOPBACK; if (RawTestMode &amp; ENABLE_PKTCHK) testmode |= PKTCHKR; if (RawTestMode &amp; ENABLE_PKTGEN) testmode |= PKTGENR;&#125;else //TEST_STOP时，将之前in process的testmode的对应位置0，如果是loopback模式，则第1位置0；如果是CHK或者GEN，则第0位置0&#123; if (RawTestMode &amp; ENABLE_PKTCHK) testmode &amp;= ~PKTCHKR; if (RawTestMode &amp; ENABLE_PKTGEN) testmode &amp;= ~PKTGENR; if (RawTestMode &amp; ENABLE_LOOPBACK) testmode &amp;= ~LOOPBACK;&#125;/* Test start / stop conditions */#define PKTCHKR 0x00000001 /* Enable TX packet checker */#define PKTGENR 0x00000001 /* Enable RX packet generator */#define CHKR_MISMATCH 0x00000001 /* TX checker reported data mismatch */#define LOOPBACK 0x00000002 /* Enable TX data loopback onto RX */所以 loopback check generate check|generateTEST_START testmode 0x0000 0002 0x0000 0001 0x0000 0001 0x0000 0001 Important 然后开始写寄存器,即配置FPGA以改变功能sgusr.c文件中，有一个条件编译#ifdef RAW_ETH,根据没有定义RAW_ETH会执行不同的代码，从而设置不同的寄存器值，12345678910111213141516171819202122232425262728293031323334353637383940414243444546//TEST_START时： //not Raw_ETH: ndef RAW_ETH XIo_Out32 (TXbarbase + DESIGN_MODE_ADDRESS,PERF_DESIGN_MODE); (TXbarbase + 0x9004, 0x0000 0003) XIo_Out32 (TXbarbase + PKT_SIZE_ADDRESS, val); (TXbarbase + 0x9104/0x9204/0x9304/0x9404, maxpaketSize) XIo_Out32 (TXbarbase + SEQNO_WRAP_REG , seqno); (TXbarbase + 0x9110/0x9210/0x9310/0x9410, 512) //然后重置CONFIG_ADDRESS XIo_Out32 (TXbarbase + TX_CONFIG_ADDRESS, 0); (TXbarbase + 0x9108/0x9208/0x9308/0x9408, 0) //如果是PKTCHK|LOOPBACK: if (RawTestMode &amp; (ENABLE_PKTCHK | ENABLE_LOOPBACK)) XIo_Out32 (TXbarbase + TX_CONFIG_ADDRESS, testmode); (TXbarbase + 0x9108/0x9208/0x9308/0x9408, 0x00000001|0x00000002) if (RawTestMode &amp; ENABLE_PKTGEN) XIo_Out32 (TXbarbase + RX_CONFIG_ADDRESS, testmode); (TXbarbase + 0x9100/0x9200/0x9300/0x9400, 0x00000001) //Raw_ETH: XIo_Out32 (TXbarbase + PKT_SIZE_ADDRESS, val); (TXbarbase + 0x9104/0x9204/0x9304/0x9404, maxpaketSize) //然后重置CONFIG_ADDRESS XIo_Out32 (TXbarbase + TX_CONFIG_ADDRESS, 0); (TXbarbase + 0x9108/0x9208/0x9308/0x9408, 0) //如果是PKTCHK|LOOPBACK: if (RawTestMode &amp; (ENABLE_PKTCHK | ENABLE_LOOPBACK)) XIo_Out32 (TXbarbase + TX_CONFIG_ADDRESS, testmode); (TXbarbase + 0x9108/0x9208/0x9308/0x9408, 0x00000001|0x00000002) XIo_Out32 (TXbarbase + DESIGN_MODE_ADDRESS,PERF_DESIGN_MODE); (TXbarbase + 0x9004, 0x0000 0000)注意这个非Raw_ETH的值不同 if(RawTestMode &amp; ENABLE_CRISCROSS): XIo_Out32(TXbarbase+NW_PATH_OFFSET_OTHER+XXGE_RCW1_OFFSET, 0x50000000); else: XIo_Out32(TXbarbase+NW_PATH_OFFSET+XXGE_RCW1_OFFSET, 0x50000000); XIo_Out32(TXbarbase+NW_PATH_OFFSET+XXGE_TC_OFFSET, 0x50000000); //如果是PKEGEN: if (RawTestMode &amp; ENABLE_PKTGEN) XIo_Out32 (TXbarbase + RX_CONFIG_ADDRESS, testmode); (TXbarbase + 0x9100/0x9200/0x9300/0x9400, 0x00000001)//TEST_STOP时： XIo_Out32 (TXbarbase + TX_CONFIG_ADDRESS, testmode); XIo_Out32 (TXbarbase + RX_CONFIG_ADDRESS, testmode);]]></content>
      <categories>
        <category>vc709 DMA 驱动</category>
      </categories>
      <tags>
        <tag>VC709</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DMA的一些基础知识]]></title>
    <url>%2Fpost%2F2017%2F3%2FDMA%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html</url>
    <content type="text"><![CDATA[本文介绍DMA的基础知识本文转自 http://blog.csdn.net/kafeiflynn/article/details/6665743 总线地址DMA的每次数据传送(至少)需要一个内存缓冲区，它包含硬件设备要读出或写入的数据。一般而言，启动一次数据传送前，设备驱动程序必须确保DMA电路可以直接访问RAM内存单元。 现已区分三类存储器地址：逻辑地址、线性地址以及物理地址，前两个在CPU内部使用，最后一个是CPU从物理上驱动数据总线所用的存储器地址。但还有第四种存储器地址，称为总线地址(bus address)，它是除CPU之外的硬件设备驱动数据总线时所用的存储器地址。 从根本上说，内核为什么应该关心总线地址呢？这是因为在DMA操作中，数据传送不需要CPU的参与；I/O设备和DMA电路直接驱动数据总线。因此，当内核开始DMA操作时，必须把所涉及的内存缓冲区总线地址或写入DMA适当的I/O端口，或写入I/O设备适当的I/O端口。 在80x86体系结构中，总线地址与物理地址一致。然而，其他体系结构如Sun SPARC和HP Alpha都包括一个I/O存储器管理单元(IO-MMU)硬件电路，它类似微处理器分页单元，将物理地址映射为总线地址。使用DMA的所有I/O驱动程序在启动一次数据传送前必须设置好IO-MMU。不同的总线具有不同的总线地址大小，ISA的总线地址是24位长，因此在80x86体系结构中，可在物理内存的低16MB中完成DMA传送—-这就是为什么DMA使用的内存缓冲区分配在ZONE_DMA内存区中（设置了GFP_DMA标志）。原来的PCI标准定义了32位总线地址；但是，一些PCI硬件设备最初是为ISA总线设计的，因此它们仍然访问不了物理地址0x00ffffff以上的RAM内存单元。新的PCI-X标准采用64位的总线地址并允许DMA电路可以直接寻址更高的内存。 在Linux中，数据类型dma_addr_t代表一个通用的总线地址。在80x86体系结构中，dma_addr_t对应一个32位长的整数，除非内核支持PAE，在这种情况下,dma_addr_t代表一个64位整数。 pci_set_dma_mask()和dma_set_mask()辅助函数用于检查总线是否可以接收给定大小的总线地址(mask)，如果可以，则通知总线层给定的外围设备将使用该大小的总线地址。 高速缓存的一致性系统体系结构没有必要在硬件级为硬件高速缓存与DMA电路之间提供一个一致性协议，因此，执行DMA映射操作时，DMA辅助函数必须考虑硬件高速缓存。Why?假设设备驱动把一些数据填充到内存缓冲区中，然后立刻命令硬件设备利用DMA传送方式读取该数据。如果DMA访问这些物理RAM内存单元，而相应的硬件高速缓存行（CPU与RAM之间）的内容还没有写入RAM，则硬件设备读取的就是内存缓冲区中的旧值。 设备驱动开发人员可采用2种方法来处理DMA缓冲区，即两种DMA映射类型中进行选择： 一致性DMA映射CPU在RAM内存单元上所执行的每个写操作对硬件设备而言都是立即可见的。反之也一样。 流式DMA映射这种映射方式，设备驱动程序必须注意小心高速缓存一致性问题，这可以使用适当的同步辅助函数来解决，也称为“异步的”在80x86体系结构中使用DMA，不存在高速缓存一致性问题，因为设备驱动程序本身会“窥探”所访问的硬件高速缓存。因此80x86体系结构中为硬件设备所设计的驱动程序会从前述的两种DMA映射方式中选择一个：它们二者在本质上是等价的。而在MIPS、SPARC以及PowerPC的一些模型体系中，硬件设备通常不窥探硬件高速缓存，因而就会产生高速缓存一致性问题。总的来讲，为与体系结构无关的驱动程序选择一个合适的DMA映射方式是很重要的。 一般来说，如果CPU和DMA处理器以不可预知的方式去访问一个缓冲区，那么必须强制使用一致性DMA映射方式（如，SCSI适配器的command数据结构的缓冲区）。其他情形下，流式DMA映射方式更可取，因为在一些体系结构中处理一致性DMA映射是很麻烦的，并可能导致更低的系统性能。 一致性DMA映射的辅助函数通常，设备驱动程序在初始化阶段会分配内存缓冲区并建立一致性DMA映射；在卸载时释放映射和缓冲区。为分配内存缓冲区和建立一致性DMA映射，内核提供了依赖体系结构的pci_alloc_consistent()和dma_alloc_coherent()两个函数。它们均返回新缓冲区的线性地址和总线地址。在80x86体系结构中，它们返回新缓冲区的线性地址和物理地址。为了释放映射和缓冲区，内核提供了pci_free_consistent()和dma_free_coherent()两个函数。 流式DMA映射的辅助函数流式DMA映射的内存缓冲区通常在数据传送之前被映射，在传送之后被取消映射。也有可能在几次DMA传送过程中保持相同的映射，但是在这种情况下，设备驱动开发人员必须知道位于内存和外围设备之间的硬件高速缓存。为了启动一次流式DMA数据传送，驱动程序必须首先利用分区页框分配器或通用内存分配器来动态地分配内存缓冲区。然后驱动程序调用pci_map_single()或者dma_map_single()建立流式DMA映射，这两个函数接收缓冲区的线性地址作为其参数并返回相应的总线地址。为了释放该映射，驱动程序调用相应的pci_unmap_single()或dma_unmap_single()函数。 为避免高速缓存一致性问题，驱动程序在开始从RAM到设备的DMA数据传送之前，如果有必要，应该调用pci_dma_sync_single_for_device()或dma_sync_single_for_device()刷新与DMA缓冲区对应的高速缓存行。同样的，从设备到RAM的一次DMA数据传送完成之前设备驱动程序是不可以访问内存缓冲区的：相反，如果有必要，在读缓冲区之前，驱动程序应该调用pci_dma_sync_single_for_cpu()或dma_sync_single_for_cpu()使相应的硬件高速缓存行无效。在80x86体系结构中，上述函数几乎不做任何事情，因为硬件高速缓存和DMA之间的一致性是由硬件来维护的。即使是高端内存的缓冲区也可以用于DMA传送；开发人员使用pci_map_page()或dma_map_page()函数，给其传递的参数为缓冲区所在页的描述符地址和页中缓冲区的偏移地址。相应地，为了释放高端内存缓冲区的映射，开发人员使用pci_unmap_page()或dma_unmap_page()函数。 DMA设备与Linux内核内存的I/O转自 http://www.cnblogs.com/hanyan225/archive/2010/10/28/1863854.html 上节我们说到了dma_mem_alloc()函数，需要说明的是DMA的硬件使用总线地址而非物理地址，总线地址是从设备角度上看到的内存地址，物理地址是从CPU角度上看到的未经转换的内存地址(经过转换的那叫虚拟地址)。在PC上，对于ISA和PCI而言，总线即为物理地址，但并非每个平台都是如此。由于有时候接口总线是通过桥接电路被连接，桥接电路会将IO地址映射为不同的物理地址。例如，在PRep(PowerPC Reference Platform)系统中，物理地址0在设备端看起来是0X80000000，而0通常又被映射为虚拟地址0xC0000000,所以同一地址就具备了三重身份：物理地址0,总线地址0x80000000及虚拟地址0xC0000000,还有一些系统提供了页面映射机制，它能将任意的页面映射为连续的外设总线地址。内核提供了如下函数用于进行简单的虚拟地址/总线地址转换： unsigned long virt_to_bus(volatile void *address)void *bus_to_virt(unsigned long address)在使用IOMMU或反弹缓冲区的情况下，上述函数一般不会正常工作。而且，这两个函数并不建议使用。 需要说明的是设备不一定能在所有的内存地址上执行DMA操作，在这种情况下应该通过下列函数执行DMA地址掩码：int dma_set_mask(struct device *dev, u64 mask);比如，对于只能在24位地址上执行DMA操作的设备而言，就应该调用dma_set_mask(dev, 0xffffffff).DMA映射包括两个方面的工作：分配一片DMA缓冲区；为这片缓冲区产生设备可访问的地址。结合前面所讲的，DMA映射必须考虑Cache一致性问题。内核中提供了一下函数用于分配一个DMA一致性的内存区域：void *dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp)这个函数的返回值为申请到的DMA缓冲区的虚拟地址。此外，该函数还通过参数handle返回DMA缓冲区的总线地址。与之对应的释放函数为：void dma_free_coherent(struct device *dev, size_t size, void *cpu_addr, dma_addr_t handle)以下函数用于分配一个写合并(writecombinbing)的DMA缓冲区：void *dma_alloc_writecombine(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp)与之对应的是释放函数：dma_free_writecombine(),它其实就是dma_free_conherent,只不过是用了#define重命名而已。 此外，Linux内核还提供了PCI设备申请DMA缓冲区的函数pci_alloc_consistent(),原型为：void *pci_alloc_consistent(struct pci_dev *dev, size_t size, dma_addr_t *dma_addrp),对应的释放函数为：void pci_free_consistent(struct pci_dev *pdev, size_t size, void *cpu_addr, dma_addr_t dma_addr)相对于一致性DMA映射而言，流式DMA映射的接口较为复杂。对于单个已经分配的缓冲区而言，使用dma_map_single()可实现流式DMA映射：dma_addr_t dma_map_single(struct device *dev, void *buffer, size_t size, enum dma_data_direction direction),如果映射成功，返回的是总线地址，否则返回NULL.最后一个参数DMA的方向，可能取DMA_TO_DEVICE, DMA_FORM_DEVICE, DMA_BIDIRECTIONAL和DMA_NONE;与之对应的反函数是：void dma_unmap_single(struct device *dev,dma_addr_t *dma_addrp,size_t size,enum dma_data_direction direction)通常情况下，设备驱动不应该访问unmap()的流式DMA缓冲区，如果你说我就愿意这么做,我又说写什么呢，选择了权利，就选择了责任，对吧。这时可先使用如下函数获得DMA缓冲区的拥有权：void dma_sync_single_for_cpu(struct device *dev,dma_handle_t bus_addr, size_t size, enum dma_data_direction direction)在驱动访问完DMA缓冲区后，应该将其所有权还给设备，通过下面的函数:void dma_sync_single_for_device(struct device *dev,dma_handle_t bus_addr, size_t size, enum dma_data_direction direction)int dma_map_device(struct device *dev,struct scatterlist *sg, int nents,enum dma_data_direction direction)Linux系统中可以有一个相对简单的方法预先分配缓冲区，那就是同步“mem=”参数预留内存。例如，对于内存为64MB的系统，通过给其传递mem=62MB命令行参数可以使得顶部的2MB内存被预留出来作为IO内存使用，这2MB内存可以被静态映射，也可以执行ioremap().相应的函数都介绍完了:说真的，好费劲啊，我都想放弃了，可为了小王，我继续哈..在linux设备驱动中如何操作呢：像使用中断一样，在使用DMA之前，设备驱动程序需要首先向系统申请DMA通道，申请DMA通道的函数如下：int request_dma(unsigned int dmanr, const char * device_id)同样的，设备结构体指针可作为传入device_id的最佳参数。使用完DMA通道后，应该使用如下函数释放该通道：void free_dma(unsinged int dmanr)作为本篇的最后，也作为Linux设备驱动核心理论的结束篇，总结一下在Linux设备驱动中DMA相关代码的流程。如下所示：]]></content>
      <categories>
        <category>vc709 DMA 驱动</category>
      </categories>
      <tags>
        <tag>DMA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vc709 DMA 驱动程序源码理解]]></title>
    <url>%2Fpost%2F2017%2F3%2Fvc709-DMA-%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3.html</url>
    <content type="text"><![CDATA[看了一周的vc709这块板子自带的驱动程序的源码，现作点总结，以便往后回顾。该dma驱动分为2个部分，一部分为base,即xdma源代码文件夹；另一部分为user application specific部分。这样把驱动分为基础部分，和用户自定义的部分，方便了不同的用户根据自己应用的需求来定制dma功能。 user-application-specific我们先来看user application specific的部分，我选取的是xrawdata0这个驱动程序源码，该驱动很简单就是测试vc709 dma performance模式对应的应用驱动。源码文件为sguser.c，该驱动对应的一个字符设备为/dev/xraw_data0 关于字符设备的系统调用，有以下这些函数1234567static int xraw_dev_open(struct inode *in, struct file * filp);static int xraw_dev_release(struct inod *in, struct file*filp);static long xraw_dev_ioctl (struct file *filp, unsigned int cmd, unsigned long arg);static ssize_t xraw_dev_read (struct file *file, char __user * buffer, size_t length, loff_t * offset);static ssize_t xraw_dev_write (struct file *file, const char __user * buffer, size_t length, loff_t * offset);static int __init rawdata_init (void);static void __exit rawdata_cleanup (void); 并且还有对应的userFunctions调用，usr-application-specific的dma驱动实现这些函数调用，因为xdma没有实现这些，在具体的xdma是需要这些函数的123456int myInit (u64 barbase, unsigned int );int myGetRxPkt (void *, PktBuf *, unsigned int, int, unsigned int);int myPutTxPkt (void *, PktBuf *, int, unsigned int);int myPutRxPkt (void *, PktBuf *, int, unsigned int);int mySetState (void *hndl, UserState * ustate, unsigned int privdata);int myGetState (void *hndl, UserState * ustate, unsigned int privdata); user application specific驱动注册下面我们来看具体的user-application-specific的驱动是如何和xmda驱动关联起来工作的。首先我们看xrawdata_init()函数，该函数前面都是注册其对应的字符设备的工作，重点看if (xraw_DriverState &lt; POLLING)里面的内容，该段代码向xmda驱动注册2个dma engine,首先指定ufuncs的函数句柄; privData(辨别是TX还是RX dma engine); mode(是raw ethernet mode还是performance mode);最后调用handle[0] = DmaRegister (ENGINE_TX, MYBAR, &amp;ufuncs, BUFSIZE)向xdma对应的engine注册，第一个参数ENGINE_TX:要注册的DMA ENGINE号（xdma 总共有可以有64个DMA engine,但是硬件只实现了8个为0,1,2,3 for TX; 32,33,34,35 for Rx）;第二个参数MYBAR:指定是BAR0-BAR5中哪个一个，这是为0，即BAR0；第三个参数&amp;funcs：上面所说的函数调用的句柄；第四个参数BUFSIZE：该engine所处理的数据包的通常大小，这里为一个内存页的大小，即4096 B;该函数返回的handle为对应的engine的结构体的指针：Dma_engine * eptr;该结构体为xdma驱动定义， 而DmaRegister()函数在xdma_user.c中实现，即为xmda驱动的一部分，下面分析该函数都作了些什么： 首先，将传进来的第三个参数和四个参数赋给对应的Engine: eptr-&gt;user = * uptr, eptr-&gt;pktSize = pktsize; 然后，设置uptr-&gt;versionReg为BAR0的虚拟地址+0x9000; //0x9000之后的地址为user space registers 该函数调用了userFunctions-&gt;UserInit(即sgusr.c中的myInit); 然后调用了descriptor_init(eptr-&gt;dev, eptr)，初始化该DMA Engine的BDring；该函数调用了Dma_BdRingCreate()创建该Engine对应的BDring,如果是RX Engine，还需要调用DmaSetupRecvBuffers(pdev, eptr)；为BDring中的BD关联对应的pkt buffer，此时需要调用Dma_BdRingAlloc()和Dma_BdRingToHw(),具体说明可以看xdma.h中 Software Initialization 这一节，需要注意的是，该段代码只有在ETHERNET_APPMODE才会初始化RX engine. 当BDring初始化成功之后，最后调用Dma_BdRingStart()启动该dma engine 可见，大多数工作都是xdma的驱动完成的，user application驱动只用到了myInit(); user application-specific DMA write(TX)逻辑具体为sguser.c文件中的xraw_dev_write()函数，用户程序要使用DMA发送数据的时候，使用系统调用write(fd,char * buffer,size),驱动对应的调用xraw_dev_write，函数内部判断如果testmode是start并且是checker或者loopback，则可以TX，对应调用DmaSetupTransmit(handle[0],1,buffer,length)，其中handle[0]，即为该application-specific驱动注册的TX Engine,第二个参数看了函数具体代码后，发觉是没用的。DmaSetupTransmit()函数，首先为用户空间的buffer创建对应的cache pages，然后动态创建发送用的pktBuf，并用cache pages初始化，最后调用DmaSendPages_TX(hndl, pkts, allocPages)启动TX；DmaSendPages_TX()函数为xdma提供，该函数的第一参数为TX engine的handle,第二个参数为用户要发送数据的pktbuf的数组，第三个参数为该pktbuf数组的大小，也是要TX的数组占用了多少个内存page,因为一个pktbuf最多只能表示一个page的数据； PktBuf结构体对应software BD,Dma_Bd对应硬件BD,DmaSendPages_TX()函数用hndl对应的engine TX allocPages个PktBuf(存在pkts数组中),其中以PktBuf为软件BD,指定的buffer最大为一个内存Page,pkts数组可以包含多个packet,一个packet可以由n(n&gt;=1)个PktBuf表示，根据PktBuf-&gt;flags字段可以有PKT_SOP,PKT_EOP，用此间隔pkts数组中不同的packet由哪些pktbuf组成，另外还有PKT_ALL，当指向一个packet的多个pktbuf的flag设置了此字段，则DmaSendPages_TX()会依次检查组成这个packet的所有pktbuf,如果没有含PKT_EOP的pktbuf,则不发送这个packet,也就是说设置了这个字段后，TX的时候会检查包的完整性。 How to start MDA transactions RX channelRX比较简单，在初始化阶段，也就是DMA register阶段，已经为RX engine调用了DmaSetupRecvBuffers(pdev, eptr),只用等待硬件触发DMA transactions就行了。 TX channel在开始TX时，需要一些准备工作。首先应用调用Dma_BdRingAlloc()分配一个BD表，然后初始化这些BDs的data buffer address, data size, and control word, etc…；然后调用Dma_BdRingToHw()将这些BDs传递给硬件操作，具体过程参见xdma_user.c中的DmaSendPkt(void * handle, PktBuf * pkts, int numpkts)和DmaSendPages_Tx(void * handle, PktBuf ** pkts, int numpkts) 其中DmaSendPkt中用的是pci_map_single()，将要发送的包的起始地址和包长度进行映射得到总线地址（x86平台即物理地址）;DmaSendPages_Tx中用的是pci_map_page()，将要发送的包的内存页的起始地址，页内偏移和，包长度进行page映射，得到总线地址;note: pci_map_page()用在高端内存中; xdma驱动该部分驱动为dma 的base 驱动，在xdma文件中，首先我们看xdma_base.c文件，在文件最后有：12module_init(xdma_init);module_exit(xdma_cleanup);可见模块insmod的时候，调用xdma_init()函数，在xdma_init()函数中，只初始化4个自旋锁，并调用pci_register_driver(&amp;xdma_driver)；在xdma_driver结构体中，指定了probe()函数,调用pci_register_driver()会触发xdma_probe();所以载入xdma驱动的时候，主要工作都在probe()中，下面我们来看probe()函数： 1static int xdma_probe(struct pci_dev *pdev, const struct pci_device_id *ent); 首先调用pci_enable_device(pdev)使能xdma的pci设备;然后在for循环中初始化10个pktPool，使他们连成链表，并且每个都指向pktArrray[i],(pktArray[10][1999]);然后动态分配一个priData结构体类型的变量dmaData,该结构体表示了VC709开发板的pci device的私有数据:12345678910111213141516171819202122232425struct privData &#123; struct pci_dev * pdev; /**&lt; PCI device entry */ /** BAR information discovered on probe. BAR0 is understood by this driver. * Other BARs will be used as app. drivers register with this driver. */ u32 barMask; /**&lt; Bitmask for BAR information */ struct &#123; unsigned long basePAddr; /**&lt; Base address of device memory */ unsigned long baseLen; /**&lt; Length of device memory */ void __iomem * baseVAddr; /**&lt; VA - mapped address */ &#125; barInfo[MAX_BARS]; //MAX_BARS=6 u32 index; /**&lt; Which interface is this */ /** * The user driver instance data. An instance must be allocated * for each user request. The user driver request will request separately * for one C2S and one S2C DMA engine instances, if required. The DMA * driver will not allocate a pair of engine instances on its own. */ long long engineMask; /**&lt; For storing a 64-bit mask */ //因为有64个engine,每位一个mask. Dma_Engine Dma[MAX_DMA_ENGINES];/**&lt; Per-engine information */ //MAX_DMA_ENGINES=64 最多64个DMA ENGINE int userCount; /**&lt; Number of registered users */&#125;;然后初始化dmaData的barMask,engineMask和userCount,然后调用pci_set_master(pdev)将该pci设备设置设为 bus_mastering模式，后面调用了pci_request_regions，我也不知道是干什么的。然后根据系统位数，调用pci_set_dma_mask()检查总线是否可以接收给定大小的总线地址(mask)，如果可以，则通知总线层给定的外围设备将使用该大小的总线地址。后面的for循环，读取bar配置空间的数据，123456//从配置区相应寄存器得到I/O区域的内存区域长度：pci_resource_length(struct pci_dev *dev, int bar) Bar值的范围为0-5。//从配置区相应寄存器得到I/O区域的内存的相关标志：pci_resource_flags(struct pci_dev *dev, int bar) Bar值的范围为0-5。//从配置区相应寄存器得到I/O区域的基址：pci_resource_start(struct pci_dev *dev, int bar) Bar值的范围为0-5。先检查配置了哪些BAR地址空间，bar0是必须的，然后判断存在的BAR空间是不是memory-mapped，该驱动只支持memory-mapped.然后依次设置damDate结构体中各个BAR空间的basePAddr（bar基地址,是物理地址），size(bar空间长度),baseVAddr(基 虚拟地址) 然后disable中断，设置dmaData的pdev，index, 然后调用ReadDMAEngineConfiguration(pdev,dmaData)函数配置Dma Engine; 具体来查看ReadDMAEngineConfiguration(struct pci_dev * pdev, struct privData * dmaInfo)1234567891011dma Engine的配置信息都在BAR0空间的寄存器中，根据UG962可知，该trd的程序用到了8个DMA Engine，对应的offset为：DMA Channel Offset from BAR0Channel 0 S2C 0x0Channel 1 S2C 0x100Channel 2 S2C 0x200Channel 3 S2C 0x300Channel 0 C2S 0x2000Channel 1 C2S 0x2100Channel 2 C2S 0x2200Channel 3 C2S 0x2300 for循环的offset=0,i=0; offset &lt; 64 * 0x100; offset += 0x100, i++i=0,1,2,3和32,33,34,35时，if(val &amp; DMA_ENG_PRESENT_MASK)有效，调用DMA_Initialize()初始化了对应dmaData.DMA[0,1,2,3,32,33,34,35]，并reset.初始化内容有：1.首先将对应的Dma_Engine对应的指针区域用memset置零;2.设置该Engine的RegBase(寄存器基地址)为BaseAddress(即上面对应的channel的地址)，Type(是c2s还是s2c);3.设置对应的BDring的状态为XST_DMA_SG_IS_STOPPED(没有在运行),设置Engine的状态为INITIALIZED;4.设置对应的BDring的chanBase地址为BaseAddress,和BDring的IsRxChannel字段;5.最后Reset对应的Engine 调用pci_set_drvdata(pdev, dmaData)，将dmaData设置为该pci设备的私有数据，然后初始化xdma_driver对应的字符设备，然后将xdma_driver的DriverState设为INITIALIZED;然后启动轮询的计时器：12345init_timer(timer);timer-&gt;expires=jiffies+(HZ/500); //2ms之后启动timer-&gt;data=(unsigned long) pdev; //传递给function的数据，timer-&gt;function = poll_routine;add_timer(timer); 调用的函数为poll_routine(),我们来看该函数123456789101112131415161718192021222324252627282930313233343536373839404142434445static void poll_routine(unsigned long __opaque)&#123; struct pci_dev *pdev = (struct pci_dev *)__opaque; Dma_Engine * eptr; struct privData *lp; int i, offset; if(DriverState == UNREGISTERING) return; lp = pci_get_drvdata(pdev); for(i=0; i&lt;MAX_DMA_ENGINES; i++) &#123;#ifdef TH_BH_ISR /* Do housekeeping only if adequate time has elapsed since * last ISR. */ if(jiffies &lt; (LastIntr[i] + (HZ/50))) continue;#endif if(!((lp-&gt;engineMask) &amp; (1LL &lt;&lt; i))) continue; eptr = &amp;(lp-&gt;Dma[i]); if(eptr-&gt;EngineState != USER_ASSIGNED) continue; /* The spinlocks need to be handled within this function, so * don't do them here. */ PktHandler(i, eptr); &#125; /* Reschedule poll routine. Incase interrupts are enabled, the * bulk of processing should happen in the ISR. */#ifdef TH_BH_ISR offset = HZ / 50;#else offset = 0;#endif poll_timer.expires = jiffies + offset; add_timer(&amp;poll_timer);&#125; 如果DriverState为正在unregistering，则什么都不干；重点注意for循环中，为每个启用的DMA Engine调用了PktHandler(i, eptr);最后在重新调用该轮询函数(注意在probe函数中，struct timer_list * timer = &poll_timer; 当时已经指定了function)所以重点是PktHandler()1PktHandler(int eng, Dma_Engine * eptr) 首先调用if ((bd_processed = Dma_BdRingFromHw(rptr, DMA_BD_CNT, &amp;BdPtr)) &gt; 0)如果硬件产生了DMA操作，返回的BDs不为零，则进行操作，具体的操作是： BdPtr指向post-processing的Bd，即硬件操作完的BD;取消之前硬件DMA操作占用的内存page的dma映射(pci_unmap_page());调用Dma_BdRingFree()将post-processing的Bd置为free;然后调用(uptr-&gt;UserPutPkt)(eptr, ppool-&gt;pbuf, bd_processed_save, uptr-&gt;privData)将pktbuf还给application;最后检查是否为ETHERNET_APPMODE,如果是，并且对应的Engine是RX engine，则调用DmaSetupRecvBuffers(pdev, eptr)，为RX的BDring关联对应的buffer;]]></content>
      <categories>
        <category>vc709 DMA 驱动</category>
      </categories>
      <tags>
        <tag>DMA</tag>
        <tag>VC709</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xdma源码解读]]></title>
    <url>%2Fpost%2F2017%2F3%2Fxdma%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html</url>
    <content type="text"><![CDATA[A buffer descriptor defines a DMA transaction. the Dma_Bd结构体定义了BD; xdma.hxdma驱动支持以下特性： Scatter-Gather DMA (SGDMA) Interrupts Interrupts are coalesced by the driver to improve performance 32-bit addressing for Buffer Descriptors (BDs) and data buffers APIs to manage BD movement to and from the SGDMA engine Virtual memory support PCI Express support Performance measurement statistics APIs to enable DMA driver to be used by other application drivers 关于一个DMA传输，需要source address,destination address, the number of bytes to transfer.receive channel, sourec address在硬件中，不需要指定;destination address为系统buffer。transmit channel，则destination address不要指定。 A packet is defined as a series of data bytes that represent a message. SGDMA allows a packet of data to be broken up into one or more transactions. For example, take an Ethernet IP packet which consists of a 14-byte header followed by a data payload of one or more bytes. With SGDMA, the application may point a BD to the header and another BD to the payload, then transfer them as a single message. This strategy can make a TCP/IP stack more efficient by allowing it to keep packet headers and data in different memory regions instead of assembling packets into contiguous blocks of memory. Software Initialization * The driver does the following steps in order to prepare the DMA engine * to be ready to process DMA transactions: * * - DMA Initialization using Dma_Initialize() function. This step * initializes a driver instance for the given DMA engine and resets the * engine. One driver instance exists for a pair of (S2C and C2S) engines. * - BD Ring creation. A BD ring is needed per engine and can be built by * calling Dma_BdRingCreate(). A parameter passed to this function is the * number of BDs fit in a given memory range, and Dma_mBdRingCntCalc() helps * calculate the value. * - (RX channel only) Prepare BDs with attached data buffers and give them to * the RX channel. First, allocate BDs using Dma_BdRingAlloc(), then populate * data buffer address, data buffer size and the control word fields of each * allocated BD with valid values. Last call Dma_BdRingToHw() to give the * BDs to the channel. * - Enable interrupts if interrupt mode is chosen. The application is * responsible for setting up the interrupt system, which includes providing * and connecting interrupt handlers and call back functions, before * the interrupts are enabled. * - Start DMA channels: Call Dma_BdRingStart() to start a channel How to start DMA transactions * RX channel is ready to start RX transactions once the initialization (see * Initialization section above) is finished. The DMA transactions are triggered * by the user IP (like Local Link TEMAC). * * Starting TX transactions needs some work. The application calls * Dma_BdRingAlloc() to allocate a BD list, then populates necessary * attributes of each allocated BD including data buffer address, data size, * and control word, and last passes those BDs to the TX channel * (see Dma_BdRingToHw()). The added BDs will be processed as soon as the * TX channel reaches them. Software Post-Processing on completed DMA transactions * Some software post-processing is needed after DMA transactions are finished. * * If interrupts are set up and enabled, DMA channels notify the software * the finishing of DMA transactions using interrupts, Otherwise the * application could poll the channels (see Dma_BdRingFromHw()). * * - Once BDs are finished by a channel, the application first needs to fetch * them from the channel (see Dma_BdRingFromHw()). * - On TX side, the application now could free the data buffers attached to * those BDs as the data in the buffers has been transmitted. * - On RX side, the application now could use the received data in the buffers * attached to those BDs * - For both channels, those BDs need to be freed back to the Free group (see * Dma_BdRingFree()) so they are allocatable for future transactions. * - On RX side, it is the application&apos;s responsibility for having BDs ready * to receive data at any time. Otherwise the RX channel will refuse to * accept any data once it runs out of RX BDs. As we just freed those hardware * completed BDs in the previous step, it is good timing to allocate them * back (see Dma_BdRingAlloc()), prepare them, and feed them to the RX * channel again (see Dma_BdRingToHw()) Address Translation * When the BD list is setup with Dma_BdRingCreate(), a physical and * virtual address is supplied for the segment of memory containing the * descriptors. The driver will handle any translations internally. Subsequent * access of descriptors by the application is done in terms of their virtual * address. * * Any application data buffer address attached to a BD must be physical * address. The application is responsible for calculating the physical address * before assigns it to the buffer address field in the BD. Memory Barriers * The DMA hardware expects the update to its Next BD pointer register to be * the event which initiates DMA processing. Hence, memory barrier wmb() calls * have been used to ensure this. Alignment * &lt;b&gt; For BDs: &lt;/b&gt; * The Northwest Logic DMA hardware requires BDs to be aligned at 32-byte * boundaries. In addition to the this, the driver has its own alignment * requirements. It needs to store per-packet information in each BD, for * example, the buffer virtual address. In order to do this, the software * view of the BD may be larger than the hardware view of the BD. For example, * DMA_BD_SW_NUM_WORDS can be set to 16 words (64 bytes), even though * DMA_BD_HW_NUM_WORDS is 8 words (32 bytes). Due to this, the driver * gets additional space in which to store per-BD private information. * * Minimum alignment is defined by the constant DMA_BD_MINIMUM_ALIGNMENT. * This is the smallest alignment allowed by both hardware and software for them * to properly work. Other than DMA_BD_MINIMUM_ALIGNMENT, multiples of the * constant are the only valid alignments for BDs. * * If the descriptor ring is to be placed in cached memory, alignment also MUST * be at least the processor&apos;s cache-line size. If this requirement is not met * then system instability will result. This is also true if the length of a BD * is longer than one cache-line, in which case multiple cache-lines are needed * to accommodate each BD. * * Aside from the initial creation of the descriptor ring (see * Dma_BdRingCreate()), there are no other run-time checks for proper * alignment. For application data buffers: * Application data buffer alignment is taken care of by the * application-specific drivers. Reset After Stopping * This driver is designed to allow for stop-reset-start cycles of the DMA * hardware while keeping the BD list intact. When restarted after a reset, this * driver will point the DMA engine to where it left off after stopping it. * * It is possible to load an application-specific driver, run it for some * time, and then unload it. Without unloading the DMA driver as well, it * should be possible to load another instance of the application-specific * driver and it should work fine. xdma_bd.h描述Buffer Descriptor]]></content>
      <categories>
        <category>vc709 DMA 驱动</category>
      </categories>
      <tags>
        <tag>DMA</tag>
        <tag>VC709</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xraw_data driver解析]]></title>
    <url>%2Fpost%2F2017%2F3%2Fxraw-data-driver%E8%A7%A3%E6%9E%90.html</url>
    <content type="text"><![CDATA[Xilinx VC709开发板，对应的程序中有个performance mode，是查看DMA性能的。该板子的DMA驱动为xdma_v7.ko，在路径linux_driver_app/driver/xdma中。选择performance mode后，该模式只使用一个DMA engine(即engine 0)，其有2个通道分别为TX和RX，对应的设备驱动为xrawdata0.ko，在路径linux_driver_app/driver/xrawdata0中,对应的驱动源程序为sgusr.c。 xraw_data0的驱动需要向XDMA的驱动注册，以完成相应的接口，其在系统中的接口为一个字符文件/dev/xraw_data0。 下面我们具体来看sgusr.c文件的源码。 1.文件前面是各种定义，寄存器地址等。第277-278行，定义RawTestMode = TEST_STOP,即初始化时，没有开始测试。RawMinPktSize为64 Byte,RawMaxPktSize为4个内存页(4page=4*4096=32768 Byte)。 2.初始化函数static int __int rawdata_int(void) 第1457行，初始化xraw_data0的状态为 INITIALIZED,后面依次初始化3个spin_lock.第1471行，调用alloc_chrdev_region（），请求系统给xrawDev设备分配设备号。如果设备号分配成功，申请代表字符设备的struct cdev结构来初始化xrawCdev指针,成功后，初始化对应的文件操作函数到这个设备上，即初始化file_operations类型的xrawDevFileOps结构体（1489-1494行），最后第1498行调用cdev_add（）将设备加入内核。 xraw_data0作为使用DMA的应用，需要实现xdma驱动的对应功能的callback函数，想DMA注册。从第1516行开始，对应的先指定ufuncs中对应的callback，然后使用DmaRegister()将自己注册到base DMA驱动中，比如第1542行DmaRegister (ENGINE_TX, MYBAR, &amp;ufuncs, BUFSIZE))，将xraw_data0应用注册XDMA的ENGINE_TX号引擎上，对应的BAR寄存器地址为MYBAR,对应的函数调用为ufuncs,正常的包大小为BUFFSIZE.DmaRegister()的实现在xdma驱动源码的xdma_user.c中123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128/*****************************************************************************//** * This function must be called by the user driver to register itself with * the base DMA driver. After doing required checks to verify the choice * of engine and BAR register, it initializes the engine and the BD ring * associated with the engine, and enables interrupts if required. * * Only one user is supported per engine at any given time. Incase the * engine has already been registered with another user driver, an error * will be returned. * * @param engine is the DMA engine the user driver wants to use. * @param bar is the BAR register the user driver wants to use. * @param uptr is a pointer to the function callbacks in the user driver. * @param pktsize is the size of packets that the user driver will normally * use. * * @return NULL incase of any error in completing the registration. * @return Handle with which the user driver is registered. * * @note This function should not be called in an interrupt context * *****************************************************************************/void * DmaRegister(int engine, int bar, UserPtrs * uptr, int pktsize)&#123; Dma_Engine * eptr;#ifdef X86_64 u64 barbase;#else u32 barbase;#endif int result; log_verbose(KERN_INFO "User register for engine %d, BAR %d, pktsize %d\n", engine, bar, pktsize);#ifdef PM_SUPPORT if(DriverState == PM_PREPARE) &#123; log_verbose(KERN_ERR "DMA driver state %d - entering into Power Down states\n", DriverState); return NULL; &#125;#endif if(DriverState != INITIALIZED) &#123; log_verbose(KERN_ERR "DMA driver state %d - not ready\n", DriverState); return NULL; &#125; if((bar &lt; 0) || (bar &gt; 5)) &#123; log_verbose(KERN_ERR "Requested BAR %d is not valid\n", bar); return NULL; &#125; if(!((dmaData-&gt;engineMask) &amp; (1LL &lt;&lt; engine))) &#123; log_verbose(KERN_ERR "Requested engine %d does not exist\n", engine); return NULL; &#125; eptr = &amp;(dmaData-&gt;Dma[engine]);#ifdef X86_64 barbase = (dmaData-&gt;barInfo[bar].baseVAddr);#else barbase = (u32)(dmaData-&gt;barInfo[bar].baseVAddr);#endif if(eptr-&gt;EngineState != INITIALIZED) &#123; log_verbose(KERN_ERR "Requested engine %d is not free\n", engine); return NULL; &#125; /* Later, add check for reasonable packet size !!!! */ /* Later, add check for mandatory user function pointers. For optional * ones, assign a stub function pointer. This is better than doing * a NULL value check in the performance path. !!!! */ /* Copy user-supplied parameters */ eptr-&gt;user = *uptr; eptr-&gt;pktSize = pktsize; /* Specify user-specific version information register. Note that this is * in the BAR0 space. */#ifdef X86_64 uptr-&gt;versionReg = (dmaData-&gt;barInfo[0].baseVAddr) + 0x9000;#else uptr-&gt;versionReg = (u32)(dmaData-&gt;barInfo[0].baseVAddr) + 0x9000;#endif if ((uptr-&gt;UserInit)(barbase, uptr-&gt;privData) &lt; 0) &#123; log_verbose(KERN_ERR "Initialization unsuccessful\n"); return NULL; &#125; spin_lock_bh(&amp;DmaLock); /* Should inform the user of the errors !!!! */ result = descriptor_init(eptr-&gt;pdev, eptr); if (result) &#123; /* At this point, handle has not been returned to the user. * So, user refuses to prepare buffers. Will be trying again in * the poll_routine. So, do not abort here. */ printk(KERN_ERR "Cannot create BD ring, will try again later.\n"); //return NULL; &#125; /* Change the state of the engine, and increment the user count */ eptr-&gt;EngineState = USER_ASSIGNED; dmaData-&gt;userCount ++ ; /* Start the DMA engine */ if (Dma_BdRingStart(&amp;(eptr-&gt;BdRing)) == XST_FAILURE) &#123; log_normal(KERN_ERR "DmaRegister: Could not start Dma channel\n"); return NULL; &#125;#ifdef TH_BH_ISR printk("Now enabling interrupts\n"); Dma_mEngIntEnable(eptr);#endif spin_unlock_bh(&amp;DmaLock); log_verbose(KERN_INFO "Returning user handle %p\n", eptr); return eptr;&#125; 3.DMA写函数1xraw_dev_write (struct file *file, const char __user * buffer, size_t length, loff_t * offset) 其中调用了1DmaSetupTransmit(handle[0], 1, buffer, length)ENGINE_TX:只实现了ufuncs.UserPutPkt = myPutTxPkt;(用来将packet buffer还给application-specific dirver,以便reuse,对应代码里面更新TxSync)ENGINE_RX:实现了ufuncs.UserPutPkt = myPutRxPkt; 和ufuncs.UserGetPkt = myGetRxPkt; 即write对应的是handle[0],即ENGINE_TX发送。]]></content>
      <categories>
        <category>vc709 DMA 驱动</category>
      </categories>
      <tags>
        <tag>DMA</tag>
        <tag>VC709</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络中的一些common sense]]></title>
    <url>%2Fpost%2F2017%2F3%2F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9Bcommon-sense.html</url>
    <content type="text"><![CDATA[网络包需要另外20Ｂ以太网帧需要１.前导码和帧开始符preamble(8B)，2.gap（12B）两帧之前的间隔。总共20B。所以实际传输的最小帧为64B+20B=84B,10Gbps的网，传输一个64B的数据包，需要84B/10Gbps=848bit/1010^9bps=67.2ns 网络5-tuple source IP address destination IP address network or transport protocol id source port destination port]]></content>
      <categories>
        <category>Networking</category>
      </categories>
      <tags>
        <tag>5-tuple</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译click modular router遇到的一些问题]]></title>
    <url>%2Fpost%2F2017%2F3%2F%E7%BC%96%E8%AF%91click-modular-router%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[系统为Ubuntu 16.04.1 LTS #问题1：12Cant&apos;t find /usr/src/linux, so I can&apos;t compile the linuxmodule driver(You may need the --with-linux=DIR option.) 因为Ubuntu的头文件在1/usr/src/linux-`uname -r`-generic里面所以要么建立一个文件夹软连接1ln -s /usr/src/linux-headers-`uname -r` /usr/src/linux要么1./configure --with-linux=/usr/src/linux-headers-`uname -r` #问题2：12Can&apos;t find Linux System.map file in /usr/src/linux.(You may need the --with-linux=DIR and/or --with-linux-map=MAP options.) 同上，1./configure --with-linux-map=/boot/System.map-`uname -r`]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Click</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[intel XL710模式设置]]></title>
    <url>%2Fpost%2F2017%2F2%2Fintel-XL710%E6%A8%A1%E5%BC%8F%E8%AE%BE%E7%BD%AE.html</url>
    <content type="text"><![CDATA[买了块Intel 40G的网卡XL710QDA2.由于实验室的交换机是pica8的，只有4个10G的光口，为了让XL710网卡能和交换机互连，就需要设置其工作模式。 设置其工作模式需要用到Intel的QCU工具（QSFP+ Configuration Utility），官方有个说明文档：http://www.intel.com/content/dam/www/public/us/en/documents/guides/qsfp-configuration-utility-quick-usage-guide.pdf 对应的QCU软件的下载地址为：https://downloadcenter.intel.com/zh-cn/download/25851/-QSFP-Linux- 在应用QCU软件的之前，需要先更新网卡的NVM，因为NVM版本在4.42之前的不支持。这个可以用ethtool -i ethX命令查看，最好去官网下载最新的来更新网卡。我现在用的是5.05版本。 下载好QCU工具后，如linux版本解压后为qcu64e,chmod a+x ./qcu64e,给执行权限。123./qcu64e /DEVICES 列出当前系统识别的NIC的号码，第一列NIC下面的数字后面要用到./qcu64e /NIC=1 /INFO 显示NIC号为1的这块网卡的信息，./qcu64e /NIC=1 /SET 2x2x10A 设置网卡的2个40G的QSPF+的网卡分别工作在2个10GSPF+模式]]></content>
      <categories>
        <category>Networking</category>
      </categories>
      <tags>
        <tag>Intel XL710</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MTU,MSS以及Mpps]]></title>
    <url>%2Fpost%2F2017%2F1%2FMTU-MSS%E4%BB%A5%E5%8F%8AMpps.html</url>
    <content type="text"><![CDATA[转自http://blog.csdn.net/smartfox80/article/details/22508637 MSS是指应用层在一个数据包内最大能传输的字节数MTU是指IP层在一个数据包内最大能传输的字节数MTU= MSS+TCP层头部长度+IP层头部长度 对于千兆以太网，每秒能传输1000Mbit数据，即125000000B/s，每个以太网frame的固定开销有：前导码和帧开始符preamble(8B)、MAC(12B)、type(2B)、payload(46B~1500B)、CRC(4B)、gap(12B)，因此最小的以太网帧是84B，每秒可以发送1488000帧。最大的以太网帧是1538B，每秒可发送81274帧。接下来算TCP有效负荷：一个TCP segment包含IP header（20B）和TCP header(20B),还有Timestamp option（12B），因此TCP的最大吞吐量是81274*（1500-51）=117MB/s。 以太网帧结构在以太网链路上的数据包称作以太帧。以太帧起始部分由前导码和帧开始符组成。后面紧跟着一个以太网报头，以MAC地址说明目的地址和源地址。帧的中部是该帧负载的包含其他协议报头的数据包(例如IP协议)。以太帧由一个32位冗余校验码结尾。它用于检验数据传输是否出现损坏。 来自线路的二进制数据包称作一个帧。从物理线路上看到的帧，除其他信息外，还可看到前导码和帧开始符。任何物理硬件都会需要这些信息。 1个octets=1Byte 前导码和帧开始符参见：Syncword一个帧以7个字节的前导码和1个字节的帧开始符作为帧的开始。快速以太网之前，在线路上帧的这部分的位模式是10101010 10101010 10101010 10101010 10101010 10101010 10101010 10101011。由于在传输一个字节时最不重要的位最先传输(即低位最先传输)，因此其相应的16进制表示为0x55 0x55 0x55 0x55 0x55 0x55 0x55 0xD5。10/100M 网卡(MII PHY)一次传输4位(一个半字)。因此前导符会成为7组0x5+0x5,而帧开始符成为0x5+0xD。1000M网卡(GMII)一次传输8位，而10Gbit/s(XGMII) PHY芯片一次传输32位。 注意当以octet描述时，先传输7个01010101然后传输11010101。由于8位数据的低4位先发送，所以先发送帧开始符的0101，之后发送1101。 报头包含源地址和目标地址的MAC地址，以太类型字段和可选的用于说明VLAN成员关系和传输优先级的IEEE 802.1Q VLAN 标签。 帧校验码帧校验码是一个32位循环冗余校验码，以便验证帧数据是否被损坏。帧间距当一个帧发送出去之后，发送方在下次发送帧之前，需要再发送至少12个octet的空闲线路状态码。（PS:其实可以这么简单的理解，在以太网传输帧时，一帧数据最大是1538，出去以太网帧头帧尾这些附带数据外，以太网帧的最大负载就是1500字节（MTU），那么这1500个字节是来自上层协议总的数据包，在网络层中IP头有20个字节，也就是说在IP层可以接受上层协议最多的字节就是1480，在传输层中，如果使用TCP的话，除去TCP的头，还有12字节为TCP的时间戳(可选项)，在传输层可以接受上层协议的最大数据就是1480-20–12=1448字节），这1448字节就是应用层的最大负载数据，即MSS，这么一来，如果使用TCP作为传输层协议传输数据时，应用层每包的数据不要超过1448字节，不然会造成分包的操作。]]></content>
      <categories>
        <category>Networking</category>
      </categories>
      <tags>
        <tag>Networking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xlinx vc709 trd DMA驱动源码解读]]></title>
    <url>%2Fpost%2F2016%2F12%2FXlinx-vc709-trd-DMA%E9%A9%B1%E5%8A%A8%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB.html</url>
    <content type="text"><![CDATA[raw ethernet performance测试在运行Raw Ethernet Perfomance Mode的时候,安装驱动运行的脚本为/v7_xt_conn_trd/software/linux/v7_xt_conn_trd/linux_driver_app/run_raw_ethermode.sh其内容为：123456789101112131415161718192021222324252627282930#!/bin/shcompilation_error=1module_insertion_error=2compilation_clean_error=3pgrep App|xargs kill -SIGINT 1&gt;/dev/null 2&gt;&amp;1sleep 5;cd Appmake clean 1&gt;/dev/null 2&gt;&amp;1make APP_MODE=RAWETHERNET 1&gt;/dev/null 2&gt;&amp;1./App 1&gt;Applog 2&gt;&amp;1 &amp;cd ..//bin/sh remove_modules.shcd drivermake DRIVER_MODE=RAWETHERNET cleanif [ "$?" != "0" ]; then echo "Error in cleaning RAW_ETHERNET performance driver" exit $compilation_clean_error;fimake DRIVER_MODE=RAWETHERNETif [ "$?" != "0" ]; then echo "Error in compiling RAW_ETHERNET performance driver" exit $compilation_error;fisudo make DRIVER_MODE=RAWETHERNET insert if [ "$?" != "0" ]; then echo "Error in inserting RAW_ETHERNET performance driver" exit $module_insertion_error;fi可以看到6-12行代码，先判断系统中是否有App程序在运行，如果有就Kill掉。然后等待5秒后，进入App目录，重新编译App程序，编译完成后以RAWETHERNET模式运行APP。 可见App程序一直在运行，然后我们看App程序的源码，在/v7_xt_conn_trd/software/linux/v7_xt_conn_trd/linux_driver_app/App/threads.c可以发现Main()函数中的无限for循环中有这么一段代码：12345678910111213if (FD_ISSET(s, &amp;read_set))&#123; if ((n = recvfrom(s, &amp;testCmd, sizeof(testCmd), 0,(struct sockaddr *)&amp;remote, &amp;t)) == -1) &#123; perror("recvfrom"); continue ; &#125; printf("Received command for engine %d and max pkt size as %d\n",testCmd.Engine,testCmd.MaxPktSize); if(testCmd.TestMode &amp; TEST_START) StartTest(testCmd.Engine,testCmd.TestMode , testCmd.MaxPktSize); else StopTest(testCmd.Engine,testCmd.TestMode , testCmd.MaxPktSize);&#125;App程序的socket一直在监听其他进程的消息，当有revefrom()接收到消息之后，根据testCmd中的TestMode来选择开始测试或者停止测试。1int StartTest(int engine, int testmode, int maxSize) 在StartTest内部会根据engine号，testmode(start or stop)，和包的大小来调用`xlnx_thread_create(&txthread_write, &testcmd0);`, `xlnx_thread_create()`会在一个新的线程里调用`txthread_write`函数，参数为`testcmd0`, 下面我们来看`txthread_write(TestCmd *test)`函数， 其主要函数体如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152void * txthread_write(TestCmd *test)&#123; if(PacketSize % 4)&#123; chunksize = PacketSize + (4 - (PacketSize % 4)); &#125; else &#123; chunksize = PacketSize; &#125; bufferLen = BUFFER_SIZE - (BUFFER_SIZE % (chunksize * 512)); //log_verbose("thread TxWrite %d started engine is %d size %d \n", id ,test-&gt;Engine,test-&gt;MaxPktSize); buffer =(char *) valloc(bufferLen); if(!buffer) &#123; printf("Unable to allocate memory \n"); exit(-1); &#125; ... //initialize the available memory with the total memory. if (0 != initMemorySync(TxDoneSync, bufferLen)) &#123; //error perror("Bad Pointer TxDoneSync: MemorySync"); &#125; FormatBuffer(buffer,bufferLen,chunksize,PacketSize); while(1) &#123; if(0 == ReserveAvailable(TxDoneSync, chunksize)) &#123; if(PacketSent + chunksize &lt;= bufferLen ) &#123; ret=write(file_desc,buffer+PacketSent,PacketSize); if(ret &lt; PacketSize) &#123; FreeAvailable(TxDoneSync, chunksize); //TODO &#125; else &#123; PacketSent = PacketSent + chunksize; &#125; &#125; else &#123; FreeAvailable(TxDoneSync,chunksize); PacketSent = 0; &#125; &#125; ... &#125;&#125;注意25行调用了FormatBuffer(buffer,bufferLen,chunksize,PacketSize);函数，看之前的代码可以发现chunksize是PacketSize取顶刚好是4的倍数,bufferlen刚好可以512*chunksize的倍数。这个函数初始化从指针buffer开始的bufferlen个字节的内存空间，并以chunksize为单位初始化一个packet，其头2个Byte为PacketSize，往后每2B存放Packet包编号，下一个Packet，头2个字节依旧是PacketSize，后面的编号++，一直到512后就重新置0。 注意33行调用了write(file_desc,buffer+PacketSent,PacketSize)，即，向对应的/dev/xraw_data0设备文件写入生成缓存包，从buffer地址开始，一次写入PacketSize大小的包。write()对应xrawdata0/sguser.c里面的1234567891011121314static ssize_t xraw_dev_write (struct file *file, const char __user * buffer, size_t length, loff_t * offset)&#123; int ret_pack=0; if ((RawTestMode &amp; TEST_START) &amp;&amp; (RawTestMode &amp; (ENABLE_PKTCHK | ENABLE_LOOPBACK))) ret_pack = DmaSetupTransmit(handle[0], 1, buffer, length); /* * return the number of bytes sent , currently one or none */ return ret_pack;&#125;其调用了DmaSetupTransmit()函数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117static int DmaSetupTransmit(void * hndl, int num ,const char __user * buffer, size_t length) &#123; offset = offset_in_page(buffer); first = ((unsigned long)buffer &amp; PAGE_MASK) &gt;&gt; PAGE_SHIFT; last = (((unsigned long)buffer + length-1) &amp; PAGE_MASK) &gt;&gt; PAGE_SHIFT; allocPages = (last-first)+1; pkts = kmalloc( allocPages * (sizeof(PktBuf*)), GFP_KERNEL); if(pkts == NULL) &#123; printk(KERN_ERR "Error: unable to allocate memory for packets\n"); return -1; &#125; cachePages = kmalloc( (allocPages * (sizeof(struct page*))), GFP_KERNEL ); if( cachePages == NULL ) &#123; printk(KERN_ERR "Error: unable to allocate memory for cachePages\n"); kfree(pkts); return -1; &#125; memset(cachePages, 0, sizeof(allocPages * sizeof(struct page*)) ); down_read(&amp;(current-&gt;mm-&gt;mmap_sem)); status = get_user_pages(current, // current process id current-&gt;mm, // mm of current process (unsigned long)buffer, // user buffer allocPages, WRITE_TO_CARD, 0, /* don't force */ cachePages, NULL); up_read(&amp;current-&gt;mm-&gt;mmap_sem); if( status &lt; allocPages) &#123; printk(KERN_ERR ".... Error: requested pages=%d, granted pages=%d ....\n", allocPages, status); for(j=0; j&lt;status; j++) page_cache_release(cachePages[j]); kfree(pkts); kfree(cachePages); return -1; &#125; allocPages = status; // actual number of pages system gave for(j=0; j&lt; allocPages; j++) /* Packet fragments loop */ &#123; pbuf = kmalloc( (sizeof(PktBuf)), GFP_KERNEL); if(pbuf == NULL) &#123; printk(KERN_ERR "Insufficient Memory !!\n"); for(j--; j&gt;=0; j--) kfree(pkts[j]); for(j=0; j&lt;allocPages; j++) page_cache_release(cachePages[j]); kfree(pkts); kfree(cachePages); return -1; &#125; //spin_lock_bh(&amp;RawLock); pkts[j] = pbuf; // first buffer would start at some offset, need not be on page boundary if(j==0) &#123; if(j == (allocPages-1)) &#123; pbuf-&gt;size = length; &#125; else pbuf-&gt;size = ((PAGE_SIZE)-offset); &#125; else &#123; if(j == (allocPages-1)) &#123; pbuf-&gt;size = length-total; &#125; else pbuf-&gt;size = (PAGE_SIZE); &#125; pbuf-&gt;pktBuf = (unsigned char*)cachePages[j]; pbuf-&gt;pageOffset = (j == 0) ? offset : 0; // try pci_page_map pbuf-&gt;bufInfo = (unsigned char *) buffer + total; pbuf-&gt;pageAddr= (unsigned char*)cachePages[j]; pbuf-&gt;userInfo = length; pbuf-&gt;flags = PKT_ALL; if(j == 0) &#123; pbuf-&gt;flags |= PKT_SOP; &#125; if(j == (allocPages - 1) ) &#123; pbuf-&gt;flags |= PKT_EOP; &#125; total += pbuf-&gt;size; //spin_unlock_bh(&amp;RawLock); &#125; /****************************************************************/ allocPages = j; // actually used pages result = DmaSendPages_Tx (hndl, pkts,allocPages); if(result == -1) &#123; for(j=0; j&lt;allocPages; j++) &#123; page_cache_release(cachePages[j]); &#125; total = 0; &#125; kfree(cachePages); for(j=0; j&lt;allocPages; j++) &#123; kfree(pkts[j]); &#125; kfree(pkts); return total;&#125;其中hndl即为handle[0]，即/dev/xraw_data0,从用户空间buffer地址开始，传length长度的数据。注意SECTION1，offset为buffer在内存页中的偏移，first为要传的数据占用的内存第一个页的编号last为要传的数据占用的内存的最后一个页的编号allocPages为要传的数据所需要分配的内存页的个数 (PktBuf *)指向一个描述packet buffer的数据结构，其中packet buffer最大的大小为一个内存页（4K），而PktBuf指针只能指向一个内存页所以如果一个Packet占有allocPages个内存页的话，需要allocPages个指针。pkts = kmalloc( allocPages * (sizeof(PktBuf*)), GFP_KERNEL);pkts指针指向(PktBuf *)数组头，即分配了allocPages个 (PktBuf *)的指针 cachePages = kmalloc( (allocPages * (sizeof(struct page*))), GFP_KERNEL );在内核空间开辟一块连续的内存空间来存储要传的数据，大小为allocPages个内存页。12345678910down_read(&amp;(current-&gt;mm-&gt;mmap_sem));status = get_user_pages(current, // current process id current-&gt;mm, // mm of current process (unsigned long)buffer, // user buffer allocPages, WRITE_TO_CARD, 0, /* don't force */ cachePages, NULL);up_read(&amp;current-&gt;mm-&gt;mmap_sem);将buffer指向的要传的数据复制进cachePage中，这里因为buffer是用户空间的地址，而该部分功能属于驱动，是内核态的。所以需要用get_user_pages，把用户态的数据映射到内核态。 get_user_pages()接口真是个好东东，它能获取用户区进程使用内存的某个页(struct page)，然后可以在内核区通过kmap_atomic(), kmap()等函数映射到内核区线性地址，从而可以在内核区向其写入数据。get_user_pages()的函数声明如下：1int get_user_pages(struct task_struct *tsk, struct mm_struct *mm, unsigned long start, int len, int write, int force, struct page **pages, struct vm_area_struct **vmas); 其中tsk ：指定进程，如current表示当前进程mm ： 进程的内存占用结构，如current-&gt;mm，start ：要获取其页面的起始逻辑地址（也叫线性地址？），它是用户空间使用的一个地址len ：要获取的页数write ：是否要对该页进行写入 / 我不知道如果是写会做什么特别的处理 /force ：/ 不知道有什么特殊的动作 /pages ：存放获取的struct page的指针数组vms ： 返回各个页对应的struct vm_area_struct，可以传入NULL表示不获取，struct vm_area_struct应该是用于组成用户区进程内存的堆的基本元素？没仔细研究返回值：数返回实际获取的页数，貌似对每个实际获取的页都是给页计数值增1，如果实际获取的页不等于请求的页，要放弃操作则必须对已获取的页计数值减1，即page_cache_release()，相当于put_page()。其中123down_read(&amp;current-&gt;mm-&gt;mmap_sem);result = get_user_pages(current, current-&gt;mm, user_addr, data-&gt;npages, 1, 0, data-&gt;pagevec, NULL);up_read(&amp;current-&gt;mm-&gt;mmap_sem); mm-&gt;mmap_sem应该是个信号锁吧，在处理 current-&gt;mm时要先获得锁 后面的for(j=0; j&lt; allocPages; j++)循环中，每次都动态分配一个PktBuf，对其进行初始化初始化完成后，然后调用DmaSendPages_Tx (hndl, pkts,allocPages);将数据pkts指向的PktBuf *的packet buffer发送出去。 DmaSendPages_Tx(void * handle, PktBuf ** pkts, int numpkts)在xdma_user.c文件中， xdma源码解读dma驱动的源码在/v7_xt_conn_trd/software/linux/v7_xt_conn_trd/linux_driver_app/driver/xdma文件夹内,其中： xdma.h定义struct Dma_Engine(DMA Engine实例的数据结构)和struct privaData（device的私有数据的数据结构） xdma.c定义了Dma_Initialize(Dma_Engine InstancePtr, u64 BaseAddress, u32 Type)函数和Dma_Reset(Dma_Engine InstancePtr)函数，分别作为DMA Engine的初始化和重置。 打开xdma_base.c文件，xdma驱动在载入系统内核的时候，会调用xdma_probe(struct pci_dev *pdev, const struct pci_device_id *ent)函数，进行驱动的初始化。在第2890行，dmaData = kmalloc(sizeof(struct privData), GFP_KERNEL);为dmaData分配空间。dmaData的定义在214行:struct privData * dmaData = NULL;,是一个privData的指针，用来存放和DMA设备相关的信息。在xdma.h的517行，extern struct privData * dmaData;，将dmaData申明为全局变量。 然后对dmaData进行配置。第2957行开始的for循环for(i=0; i&lt;MAX_BARS; i++)，MAX_BARS=6对pcie设备的BAR空间进行配置(BAR0 - BAR5)其后，第3023行，ReadDMAEngineConfiguration(pdev, dmaData);配置设备DMA engine的信息。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475static void ReadDMAEngineConfiguration(struct pci_dev * pdev, struct privData * dmaInfo)&#123;#ifdef X86_64 u64 base, offset;#else u32 base, offset;#endif u32 val, type, dirn, num, bc; int i; int scalval=0,reg=0; Dma_Engine * eptr; /* DMA registers are in BAR0 */#ifdef X86_64 base = (dmaInfo-&gt;barInfo[0].baseVAddr);#else base = (u32)(dmaInfo-&gt;barInfo[0].baseVAddr);#endif /* Walk through the capability register of all DMA engines */ for(offset = DMA_OFFSET, i=0; offset &lt; DMA_SIZE; offset += DMA_ENGINE_PER_SIZE, i++) &#123; log_verbose(KERN_INFO "Reading engine capability from %x\n", (base+offset+REG_DMA_ENG_CAP)); val = Dma_mReadReg((base+offset), REG_DMA_ENG_CAP); log_verbose(KERN_INFO "REG_DMA_ENG_CAP returned %x\n", val); if(val &amp; DMA_ENG_PRESENT_MASK) &#123; printk(KERN_INFO "##Engine capability is %x##\n", val); scalval = (val &amp; DMA_ENG_SCAL_FACT) &gt;&gt; 30; printk(KERN_INFO "&gt;&gt; DMA engine scaling factor = 0x%x \n", scalval); reg = XIo_In32(base+PCIE_CAP_REG); XIo_Out32(base+PCIE_CAP_REG,(reg | scalval )); eptr = &amp;(dmaInfo-&gt;Dma[i]); log_verbose(KERN_INFO "DMA Engine present at offset %x: ", offset); dirn = (val &amp; DMA_ENG_DIRECTION_MASK); if(dirn == DMA_ENG_C2S) printk("C2S, "); else printk("S2C, "); type = (val &amp; DMA_ENG_TYPE_MASK); if(type == DMA_ENG_BLOCK) printk("Block DMA, "); else if(type == DMA_ENG_PACKET) printk("Packet DMA, "); else printk("Unknown DMA %x, ", type); num = (val &amp; DMA_ENG_NUMBER) &gt;&gt; DMA_ENG_NUMBER_SHIFT; printk("Eng. Number %d, ", num); bc = (val &amp; DMA_ENG_BD_MAX_BC) &gt;&gt; DMA_ENG_BD_MAX_BC_SHIFT; printk("Max Byte Count 2^%d\n", bc); if(type != DMA_ENG_PACKET) &#123; log_normal(KERN_ERR "This driver is capable of only Packet DMA\n"); continue; &#125; /* Initialise this engine's data structure. This will also * reset the DMA engine. */ Dma_Initialize(eptr, (base + offset), dirn); eptr-&gt;pdev = pdev; dmaInfo-&gt;engineMask |= (1LL &lt;&lt; i); &#125; &#125; log_verbose(KERN_INFO "Engine mask is 0x%llx\n", dmaInfo-&gt;engineMask);&#125;ReadDMAEngineConfiguration()函数会读取DMA配置寄存器的信息，]]></content>
      <categories>
        <category>vc709 DMA 驱动</category>
      </categories>
      <tags>
        <tag>Driver source code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux设备驱动，内核相关知识点]]></title>
    <url>%2Fpost%2F2016%2F12%2Flinux%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%EF%BC%8C%E5%86%85%E6%A0%B8%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9.html</url>
    <content type="text"><![CDATA[本文记录一些在开发Linux 设备驱动过程中，遇到的函数和知识点。仅供以后回顾。 函数 void *kmalloc(size_t size, int flags)kmalloc计算机语言的一种函数名，分配内存。语法，void *kmalloc(size_t size, int flags);size要分配内存的大小. 以字节为单位;flags要分配内存的类型。在设备驱动程序或者内核模块中动态开辟内存，不是用malloc，而是kmalloc,vmalloc,或者用get_free_pages直接申请页。释放内存用的是kfree,vfree，或free_pages。kmalloc函数返回的是虚拟地址(线性地址)。kmalloc特殊之处在于它分配的内存是物理上连续的,这对于要进行DMA的设备十分重要. 而用vmalloc分配的内存只是线性地址连续,物理地址不一定连续,不能直接用于DMA。kmalloc最大只能开辟128k-16，16个字节是被页描述符结构占用了。内存映射的I/O口，寄存器或者是硬件设备的RAM(如显存)一般占用F0000000以上的地址空间。在驱动程序中不能直接访问，要通过kernel函数vremap获得重新映射以后的地址。另外，很多硬件需要一块比较大的连续内存用作DMA传送。这块内存需要一直驻留在内存，不能被交换到文件中去。但是kmalloc最多只能开辟大小为32 X PAGE_SIZE的内存,一般的PAGE_SIZE=4kB,也就是128kB的大小的内存。 结构 pthread_mutex_tlinux下为了多线程同步，通常用到锁的概念。posix下抽象了一个锁类型的结构：ptread_mutex_t。通过对该结构的操作，来判断资源是否可以访问。顾名思义，加锁(lock)后，别人就无法打开，只有当锁没有关闭(unlock)的时候才能访问资源。它主要用如下5个函数进行操作。1：pthread_mutex_init(pthread_mutex_t mutex,const pthread_mutexattr_t attr);初始化锁变量mutex。attr为锁属性，NULL值为默认属性。2：pthread_mutex_lock(pthread_mutex_t mutex);加锁3：pthread_mutex_tylock(pthread_mutex_t mutex);加锁，但是与2不一样的是当锁已经在使用的时候，返回为EBUSY，而不是挂起等待。4：pthread_mutex_unlock(pthread_mutex_t mutex);释放锁5：pthread_mutex_destroy(pthread_mutex_t mutex);使用完后释放 命令 lsoflsof（list open files）是一个列出当前系统打开文件的工具。在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。所以如传输控制协议 (TCP) 和用户数据报协议 (UDP) 套接字等，系统在后台都为该应用程序分配了一个文件描述符，无论这个文件的本质如何，该文件描述符为应用程序与基础操作系统之间的交互提供了通用接口。因为应用程序打开文件的描述符列表提供了大量关于这个应用程序本身的信息，因此通过lsof工具能够查看这个列表对系统监测以及排错将是很有帮助的。 12345678910111213lsof [参数][文件]-a 列出打开文件存在的进程-c&lt;进程名&gt; 列出指定进程所打开的文件-g 列出GID号进程详情-d&lt;文件号&gt; 列出占用该文件号的进程+d&lt;目录&gt; 列出目录下被打开的文件+D&lt;目录&gt; 递归列出目录下被打开的文件-n&lt;目录&gt; 列出使用NFS的文件-i&lt;条件&gt; 列出符合条件的进程。（4、6、协议、:端口、 @ip ）-p&lt;进程号&gt; 列出指定进程号所打开的文件-u 列出UID号进程详情-h 显示帮助信息-v 显示版本信息 内存页，PAGE_MASK判定addr是否是4096(4K)倍数 123#define PAGE_SHIFT 12#define PAGE_SIZE (1UL &lt;&lt; PAGE_SHIFT)#define PAGE_MASK (~(PAGE_SIZE-1)) PAGE_SIZE = 1 0000 0000 0000 = 2^12 = 4KPAGE_MASK = ~(1 0000 0000 0000 - 1) = ~(1111 1111 1111) = 0000 0000 0000PAGE_ALIGN(addr)函数将地址addr调整为页对齐： 1#define PAGE_ALIGN(addr) (((addr)+PAGE_SIZE-1)&amp;PAGE_MASK) 如addr为0x22000001PAGE_ALIGN(addr)=(0x22000001+4096-1)&amp;0xfffff000=(0x22000001+0xfff)&amp;0xfffff000=0x22001000&amp;0xfffff000=0x22001000;同样，比如addr为0x22000003，PAGE_ALIGN(addr)后仍然是0x22001000； _IO, _IOR, _IOW, _IOWR 宏的解析与用法写设备驱动的时候，需要用ioctl()函数向设备传递控制信息。1234/* Perform the I/O control operation specified by REQUEST on FD. One argument may follow; its presence and type depend on REQUEST. Return value depends on REQUEST. Usually -1 indicates error. */extern int ioctl (int __fd, unsigned long int __request, ...) __THROW;其中long int __request是应用程序用于区别设备驱动程序请求处理内容的值。其大小4 Byte,32位。共分为4个域：bit 31~30(2位): dirbit 29~16(14位): sizebit 15~08(8位): typebit 07~00(8位): nr内核定义了 _IO() , _IOR() , _IOW() 和 _IOWR() 这 4 个宏来辅助生成上面的__request。下面分析 _IO() 的实现，其它的类似。在asm-generic/ioctl.h里可以看到这四个宏的定义：1234#define _IO(type,nr) _IOC(_IOC_NONE,(type),(nr),0)#define _IOR(type,nr,size) _IOC(_IOC_READ,(type),(nr),(_IOC_TYPECHECK(size)))#define _IOW(type,nr,size) _IOC(_IOC_WRITE,(type),(nr),(_IOC_TYPECHECK(size)))#define _IOWR(type,nr,size) _IOC(_IOC_READ|_IOC_WRITE,(type),(nr),(_IOC_TYPECHECK(size)))再看 _IOC() 的定义：123456#define _IOC(dir,type,nr,size) \ (((dir) &lt;&lt; _IOC_DIRSHIFT) | \ ((type) &lt;&lt; _IOC_TYPESHIFT) | \ ((nr) &lt;&lt; _IOC_NRSHIFT) | \ ((size) &lt;&lt; _IOC_SIZESHIFT))#define _IOC_TYPECHECK(t) (sizeof(t))可见， _IO()最后的结果由 _IOC()中的4个参数移位组合而成。在具体的看移位参数值，可以发现_IOC_TYPESHIFT = 8 ; _IOC_SIZESHIFT = 16 ; _IOC_DIRSHIFT = 30所以， (dir) &lt;&lt; 30) | \ 左移30位，得到方向（读写）属性 (type) &lt;&lt; 8) | \ 左移8位，得到魔区（magic） (nr) &lt;&lt; 0) | \ 左移0位，基（序列号）数 (size) &lt;&lt; 16 左移16位，得到“数据大小”区 这几个宏的使用格式为：_IO (魔数， 基数);_IOR (魔数， 基数， 变量型)_IOW (魔数， 基数， 变量型)_IOWR (魔数， 基数，变量型 ) 魔数(type)魔数范围为 0~255 。通常，用英文字符 “A” ~ “Z” 或者 “a” ~ “z” 来表示。设备驱动程序从传递进来的命令获取魔数，然后与自身处理的魔数相比较，如果相同则处理，不同则不处理。魔数是拒绝误使用的初步辅助状态。设备驱动 程序可以通过 _IOC_TYPE (cmd) 来获取魔数。不同的设备驱动程序最好设置不同的魔数，但并不是要求绝对，也是可以使用其他设备驱动程序已用过的魔数。设备驱动程序通过_IOC_TYPE(__request)获取魔数 基数，序列号(nr)用于区别发向同一个设备的各种不同的命令。通常，从 0开始递增，相同设备驱动程序上可以重复使用该值。例如，读取和写入命令中使用了相同的基数，设备驱动程序也能分辨出来，原因在于设备驱动程序区分命令时，使用 switch ，且直接使用命令变量request值,即dir+nr来区别一个命令。设备驱动程序通过`_IOC_NR(request)`获取nr 变量型(size) 设备驱动程序通过_IOC_SIZE(__request)获取指定ioctl()函数的第三个参数的数据大小。 1234/* Perform the I/O control operation specified by REQUEST on FD. One argument may follow; its presence and type depend on REQUEST. Return value depends on REQUEST. Usually -1 indicates error. */extern int ioctl (int __fd, unsigned long int __request, ...) __THROW; 比如第三个参数为args,即函数调用为ioctl(fd,request,args)args为ARG类型,则size对应的为：ARG。这里直接输入的是变量或者变量的类型，是因为使用了_IOC_TYPECHECK宏， 1#define _IOC_TYPECHECK(t) (sizeof(t)) 设备驱动程序通过_IOC_SIZE(__request)获取size _IO 宏该宏函数没有可传送的变量，只是用于传送命令。例如如下约定：#define TEST_DRV_RESET _IO (&#39;Q&#39;, 0)此时，省略由应用程序传送的 arg 变量或者代入 0 。在应用程序中使用该宏时，比如：ioctl (dev, TEST_DEV_RESET, 0) 或者 ioctl (dev, TEST_DRV_RESET) 。这是因为变量的有效因素是可变因素。只作为命令使用时，没有必要判 断出设备上数据的输出或输入。因此，设备驱动程序没有必要执行设备文件大开选项的相关处理。 _IOR 宏该函数用 于创建从设备读取数据的命令，例如可如下约定：#define TEST_DEV_READ _IRQ(&#39;Q&#39;, 1, int)这说明应用程序从设备读取数据的大小为 int 。下面宏用于判断传送到设备驱动程序的__request命令的读写状态：_IOC_DIR (cmd)运行该宏时，返回值的类型 如下：_IOC_NONE : 无属性_IOC_READ : 可读属性_IOC_WRITE : 可写属性_IOC_READ | _IOC_WRITE : 可读，可写属性使用该命令时，应用程序的 ioctl() 的 arg 变量值指定设备驱动程序上读取数据时的缓存(结构体)地址。 _IOW 宏用于创建设 备上写入数据的命令，其余内容与 _IOR 相同。通常，使用该命令时，ioctl() 的 arg 变量值指定设备驱动程序上写入数据时的缓存(结构体)地址。 _IOWR 宏用于创建设备上读写数据的命令。其余内 容与 _IOR 相同。通常，使用该命令时，ioctl() 的 arg 变量值指定设备驱动程序上写入或读取数据时的缓存 (结构体) 地址。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Device driver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux网络编程之sockaddr，sockaddr_in,sockaddr_un结构体详解]]></title>
    <url>%2Fpost%2F2016%2F12%2FLinux%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B9%8Bsockaddr%EF%BC%8Csockaddr-in-sockaddr-un%E7%BB%93%E6%9E%84%E4%BD%93%E8%AF%A6%E8%A7%A3.html</url>
    <content type="text"><![CDATA[sockaddr1234struct sockaddr &#123;unsigned short sa_family; /* address family, AF_xxx */char sa_data[14]; /* 14 bytes of protocol address */&#125;; sa_family是地址家族，一般都是“AF_xxx”的形式。好像通常大多用的是都是AF_INET。sa_data是14字节协议地址。此数据结构用做bind、connect、recvfrom、sendto等函数的参数，指明地址信息。 但一般编程中并不直接针对此数据结构操作，而是使用另一个与sockaddr等价的数据结构 sockaddr_insockaddr_in（在netinet/in.h中定义）：12345678910111213141516171819202122232425struct sockaddr_in &#123;short int sin_family; /* Address family */unsigned short int sin_port; /* Port number */struct in_addr sin_addr; /* Internet address */unsigned char sin_zero[8]; /* Same size as struct sockaddr */&#125;;struct in_addr &#123;unsigned long s_addr;&#125;;typedef struct in_addr &#123;union &#123; struct&#123; unsigned char s_b1, s_b2, s_b3, s_b4; &#125; S_un_b; struct &#123; unsigned short s_w1, s_w2; &#125; S_un_w; unsigned long S_addr; &#125; S_un;&#125; IN_ADDR; sin_family指代协议族，在socket编程中只能是AF_INETsin_port存储端口号（使用网络字节顺序）sin_addr存储IP地址，使用in_addr这个数据结构sin_zero是为了让sockaddr与sockaddr_in两个数据结构保持大小相同而保留的空字节。s_addr按照网络字节顺序存储IP地址 sockaddr_in和sockaddr是并列的结构，指向sockaddr_in的结构体的指针也可以指向sockadd的结构体，并代替它。也就是说，你可以使用sockaddr_in建立你所需要的信息,在最后用进行类型转换就可以了bzero((char)&amp;mysock,sizeof(mysock));//初始化mysock结构体名mysock.sa_family=AF_INET;mysock.sin_addr.s_addr=inet_addr(“192.168.0.1”);……等到要做转换的时候用：（struct sockaddr）mysock sockaddr_un进程间通信的一种方式是使用UNIX套接字，人们在使用这种方式时往往用的不是网络套接字，而是一种称为本地套接字的方式。这样做可以避免为黑客留下后门。 创建使用套接字函数socket创建，不过传递的参数与网络套接字不同。域参数应该是AF_LOCAL或者AF_UNIX，而不能用PF_INET之类。本地套接字的通讯类型应该是SOCK_STREAM或SOCK_DGRAM，协议为默认协议。例如：int sockfd;sockfd = socket(PF_LOCAL, SOCK_STREAM, 0); 绑定创建了套接字后，还必须进行绑定才能使用。不同于网络套接字的绑定，本地套接字的绑定的是struct sockaddr_un结构。struct sockaddr_un结构有两个参数：sun_family、sun_path。sun_family只能是AF_LOCAL或AF_UNIX，而sun_path是本地文件的路径。通常将文件放在/tmp目录下。例如： 1234struct sockaddr_un sun;sun.sun_family = AF_LOCAL;strcpy(sun.sun_path, filepath);bind(sockfd, (struct sockaddr*)&amp;sun, sizeof(sun)); 监听本地套接字的监听、接受连接操作与网络套接字类似。 连接连接到一个正在监听的套接字之前，同样需要填充struct sockaddr_un结构，然后调用connect函数。连接建立成功后，我们就可以像使用网络套接字一样进行发送和接受操作了。甚至还可以将连接设置为非阻塞模式，这里就不赘述了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[eclipse+CDT+JNI开发流程]]></title>
    <url>%2Fpost%2F2016%2F12%2Feclipse-CDT-JNI%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B.html</url>
    <content type="text"><![CDATA[买的Xilinx官方的板子的user application的源代码是用java写，为了测试batching,需要修改源代码。发现驱动层的调用是使用JNI。没办法就得看JNI部分的代码，正好java代码是用eclipse看的，那就直接搞了个CDT一起开发JNI。本文记录eclipse+CDT+JNI的配置开发流程。 由于driver调用是基于POSIX系统，本文开发环境是Centos 7.2eclipse： Neon 首先安装CDT插件help -&gt; install new softwares输入CDT,然后下载，安装。 JNI java project 现在开始JNI编程，首先建立一个就java project,名字为JNI_Test，创建一个名为com.example.jni的package,然后在该包中创建一个JNITest的class,内容如下： 123456789101112package com.example.jni;public class JNITest &#123;static &#123; // 调用文件名为JNI Library.dll的动态库 System.loadLibrary("libJNI Library"); &#125; public static void main(final String[] args) &#123;new JNITest().hello("world"); &#125; // native方法声明 public native void hello(String name); &#125; 然后就是要生成.h的头文件，这里提供两种方法。1：javah命令。进入JNI Test项目目录，然后进入其bin目录。执行如下命令：javah com.example.jni.JNITest则会在bin目录下生成对应的C++ .h头文件com_example_jni_JNITest.h其内容如下： 123456789101112131415161718192021/* DO NOT EDIT THIS FILE - it is machine generated */#include &lt;jni.h&gt;/* Header for class com_example_jni_JNITest */#ifndef _Included_com_example_jni_JNITest#define _Included_com_example_jni_JNITest#ifdef __cplusplusextern "C" &#123;#endif/* * Class: com_example_jni_JNITest * Method: hello * Signature: (Ljava/lang/String;)V */JNIEXPORT void JNICALL Java_com_example_jni_JNITest_hello (JNIEnv *, jobject, jstring);#ifdef __cplusplus&#125;#endif#endif 2:用eclipse来实现在external configuration tools中Program选项下新建一个Program,命名为javah,具体配置如下图： Location:填入系统jdk安装目录的bin/javah然后在第一步写的JNITest.java文件上运行该javah配置，运行成功后，刷新该项目，会发现多出一个jni文件夹，里面有我们需要的头文件com_example_jni_JNITest.h 新建一个C++ project,名字为JNI_Library注意：项目类型要选择shared library将上一步生成的com_example_jni_JNITest.h复制到该项目下面，并新建一个cpp文件，cpp文件的内容如下： 12345678910#include &lt;iostream&gt;#include "com_example_jni_JNITest.h"using namespace std;JNIEXPORT void JNICALL Java_com_example_jni_JNITest_hello(JNIEnv *env, jobject jthis, jstring data) &#123; jboolean iscopy; const char *charData = env-&gt;GetStringUTFChars(data, &amp;iscopy); cout &lt;&lt; "Hello " &lt;&lt; charData &lt;&lt; endl; env-&gt;ReleaseStringUTFChars(data, charData);&#125; 注意：会提示找不到jni.h头文件，这时候需要添加JDK的javah的include目录。在JNI_Library项目，右键-&gt;properties-&gt; C/C++ General -&gt; Paths and Symbols 此时在JNI_Library项目，右键-&gt;build project会提示错误”relocation R_X86_64_32 against .rodata&#39; can not be used when making a shared object; recompile with -fPIC&quot; 需要在编译的时候加上-fPIC`,对于使用makefile的朋友来说完全不是问题，只需在每个编译选项上加上-fPIC即可，但是不知道在eclipse什么地方加这一选项，查了半天资料，终于找到了,右键-&gt;properties-&gt;c/c++Build-&gt;Setting-&gt;Toolsetting-&gt;gcc c++compiler-&gt;optimization-&gt;otheroptimization flags,加上-fPIC即可。最后build project就可以得到.so文件。 在JNI_Testjava项目给下新建一个lib文件夹，将JNI_Libraryc++项目的Debug目录中生成的libJNI_Library.so复制进lib目录。然后右键JNI_Test-&gt;properties 注意：lib文件夹中的.so文件，名字前面需要有lib，如：libJNI_Library.so对应的java代码中为System.loadLibrary(&quot;JNI_Library&quot;);不要lib和后面的.so直接run as java application.]]></content>
      <categories>
        <category>Java &amp; eclipse</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Eclipse</tag>
        <tag>JNI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[notes for video learning of vivado PR project]]></title>
    <url>%2Fpost%2F2016%2F12%2Fnotes-for-video-learning-of-vivado-PR-project.html</url>
    <content type="text"><![CDATA[Vivado Design Suite QuickTake Video Tutorial: Partial Reconfiguration in Vivadohttps://www.xilinx.com/video/hardware/partial-reconfiguration-in-vivado.html 1.Synthesis: - Synthesise static logic and reconfigurable modules seperately - Use bottom-up or out-of-context synthesis 2.Partial Reconfiguration Control Sequence Initiation of reconfiguration implemented by the designer Off-chip microprocessor or other controller, On-chip state machine, processor or other logic Activate decoupling logic and reset Disconnect the reconfigurable region from the static region Deliver Partial Bitstream Region is automatically initialized Release decoupling logic when reconfiguration is complete. 3.Full configurations implemented in-context - Static design and reconfigurable modules stored in checkpoints. vivado development flowTcl console详细信息可以参考UG909，Chapter 3: Vivado Software Flow open_checkpoint ./Sources/synth/static/system_stub.dcp (routed checkpoint) read_xdc ./Sources/xdc/system_stub.xdc (xilinx design constraints)前2步，打开checkpoint,和top-level constriants.这些组成了static design. set_property HD.RECONFIGURABLE true [get_cells system_i/FILTER_ENGINE]在modules中选中需要设置为pr的模块(FILTER_ENGINE) read_checkpoint -cell system_i/FILTER_ENGINE ./Sources/synth/sobel/sobel_synth.dcp读入综合好的模块的checkpoint, for first partial reconfiguration. 选中对应的module(FILTER_ENGINE),在Device视图中，选择Draw pblock.为PR module设置pblock.注意：与clock boundary垂直对齐，为了应用（reset after configuration特性）。set_property RESET_AFTER_RECONFIG true [get_pblocks pblock_FILTER_ENGINE] Tools -&gt; Report -&gt; Report DRC -&gt; Partial Reconfiguration. opt_design place_design 这个时候看Device视图。会看到之前勾的pblock中有白色的boxes。那表示partition pins的位置，表示the interface between static design and reconfigurable modules. route_design write_checkpoint ./Checkpoint/sobel_fullroute_config.dcp保存为first configuration. update_design -black_box -cell system_i/FILTER_ENGINE做next configuration的时候，只用保持static design, 替换掉reconfigurable modules.update_design -black_box将清空reconfigurable partiton.这个时候，很多routes会变成黄色，表示它们only partially routed. write_checkpoint ./Checkpoint/static_routed.dcp lock_design -level routingclearing the partition之后，locked down the static design,注意现在routes变成了虚线，表明它们是fixed. read_checkpoint -cell system_i/FILTER_ENGINE ./Sources/synth/sepia/sepia_synth.dcp载入一个新的模块的checkpoint.来填充reconfiguration partition. opt_design place_design route_design write_checkpoint ./Checkpoint/sepia_fullroute_config.dcp按照第一个configuration的流程来。 close_project pr_verify ./Checkpoint/sepia_fullroute_config.dcp ./Checkpoint/sobel_fullroute_config.dcp -file verify.log比较2个routed configuration,看是否兼容。 open_checkpoint ./Checkpoint/sobel_fullroute_config.dcp write_bitstream ./Bitstram/sobel.bit最后，可以生成bitstream了。]]></content>
      <categories>
        <category>Vivado development</category>
      </categories>
      <tags>
        <tag>Vivado</tag>
        <tag>Xilinx</tag>
        <tag>Partial reconfiguration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QSFP,QSFP28等光模块及线缆]]></title>
    <url>%2Fpost%2F2016%2F12%2FQSFP-QSFP28%E7%AD%89%E5%85%89%E6%A8%A1%E5%9D%97%E5%8F%8A%E7%BA%BF%E7%BC%86.html</url>
    <content type="text"><![CDATA[本文转自知乎，详见：https://zhuanlan.zhihu.com/p/23165312https://zhuanlan.zhihu.com/p/23058741https://zhuanlan.zhihu.com/p/22050145https://zhuanlan.zhihu.com/p/22967479 1.40G高速线缆——高速通信发展的里程碑高速线缆简称DAC，是一种通信设备的连接线缆，一般用于实现短距离的互连通信，是一种极具成本效益的高速互联解决方案。高速线缆先从最初的10G发展到40G以及如今的100G 高速线缆只是经过了短短的几年，可见高速数据通信的发展多么迅猛，每一次的发展都将成为下一次的过渡。那么，接下来，我们就谈谈作为过渡阶段如今仍旧热门的40G高速线缆。 40G DAC（高速线缆）是指线缆两端都装上光模块可实现40Gbps数据传输的铜芯线缆。目前，市场上流行的40G DAC接口主要有三种类型：40G QSFP+ 转 QSFP+ DAC、40G QSFP+ 转 4SFP+ DAC以及 40G QSFP+ 转 4XFP+ DAC，线缆类型主要有两种：一种是有源铜芯线缆，一种是无源铜芯线缆。还有一点值得注意的是，其实高速线缆上面的光模块都不是真正意义上的光模块，它们是不带组件的，只能传输信号。为什么40G高速线缆的一端都是QSFP+接口？ QSFP+ 是一些计算及电信应用中使用的高性能交换机、路由器、服务器和主机总线适配器的新一代热插拔接口。这种接口是目前业界最紧凑的独立4通道的可插拔接口，支持高达40 Gbps (4通道 x 10 Gbps) 的数据速率，同时向下兼容5Gbps，2.5Gbps数据传输速率。40GBase QSFP+ 高速线缆适用于短距离QSFP设备间的堆叠与集联。 40G QSFP+ 转 QSFP+ DAC可以分为QSFP+ 转 QSFP+ 无源铜芯高速线缆和QSFP+ 转 QSFP+ 有源铜芯高速线缆。两种高速线缆都是由2个40G QSFP+光收发器（Transceiver）+ 铜芯线（Copper cable）构成。不同的是，有源线缆比无源线缆多了一个驱动芯片。用户可以利用这两种线缆实现现有的40G QSFP+ 端口（40Gbase) 到 40G QSFP+ 端口（40GBase ）的互连。40G QSFP+ 转 4SFP+ DAC只有QSFP+转 4 x SFP+ 无源铜芯高速线缆一种类型，它是由1个40G QSFP+光收发器（Transceiver）+ 铜芯线（Copper cable） + 4个10G SFP+光收发器（Transceiver）构成，主要用于为客户实现40G和10G设备（NIC/HBA/CNA,交换机设备和服务器）间的互连。用户可以利用这种线缆实现现有的40G QSFP+ 端口（40Gbase) 到 4个SFP+ 端口（10GBase ）的互连。40G QSFP+ 转 4XFP+ DAC也只有QSFP+ 转 4 x XFP 无源铜芯高速线缆一种类型，它是由1个40G QSFP+光收发器（Transceiver）+ 铜芯线（Copper cable）+ 4个10G XFP光收发器（Transceiver）构成。由于XFP是个没有DAC铜缆标准的产品，设备给到的信号补偿低，电缆本身的损耗很大，只能做短距离传输，一般只能做2m以内的距离。用户可以利用这种线缆实现现有的40G QSFP+ 端口（40Gbase) 到 4个XFP 端口（10GBase ）的互连。40G高速线缆的应用 （1）交换机与交换机之间互联（2）交换机与服务器之间互联 （3）交换机与转换器之间互联 2.想要全面了解100G光模块吗？近年来随着用户对40G和100G光模块传输链路需求的快速增长，云计算、移动宽带和IPTV用户对带宽的要求也日益提高。40G链路已经部署了几年，如今，40G光模块在数据中心里无所不在。近两年来，由于光学行业以“100G网络部署”为中心进行发展，100G光模块在数据中心市场上得以快速发展。目前市场上推出的100G光模块类型主要有：CXP光模块、CFP光模块、CFP2光模块、CFP4光模块以及QSFP28光模块。CXP光模块 CXP光模块的传输速率高达12×10Gbps，支持热插拔。“C”代表十六进制中的12，罗马数“X”代表每个通道具有10Gbps的传输速率，“P”是指支持热插拔的可插拔器。CXP光模块主要针对高速计算机市场，是CFP光模块在以太网数据中心里的补充。在技术上，CFP光模块和多模光纤一起应用于短距离数据传输。由于多模光纤市场需要高密度面板，所以其尺寸在多模光纤市场上并没有得到真正的优化。 CXP光模块长45mm、宽27mm，尺寸比XFP光模块或CFP光模块大，因此可提供更高密度的网络接口。除此之外，CXP光模块是由无线宽带贸易协会指定的铜连接器系统，可支持12个适用于100 GbE，3个适用于40 GbE通道的10G链路传输或12个10G以太网光纤通道或无线宽带信号的12×QDR链路传输。 CFP/CFP2/CFP4光模块 CFP多源协议（MSA）定义了热插拔光模块可应用于40G和100G网络传输的要求，包括下一代高速以太网（40GbE和100GbE）。CFP光模块支持在单模和多模光纤上以多种速率、协议和链路长度为要求进行传输，包括IEEE 802.3ba标准中包含的所有物理介质相关（PMD）接口。100G网络有三个PMD：100 GBASE-SR10可以传输100m、100GBASE-LR4可以传输10km、100GBASE-ER4可以传输40km。 CFP光模块是在小型可插拔光模块（SFP）接口基础上设计的，尺寸更大，支持100 Gbps数据传输。CFP光模块用的电接口在每个方向上（RX、TX）使用10 x 10 Gbps通道进行传输，因此支持10 x 10 Gbps和4 x 25 Gbps的互转。CFP光模块可以支持单个100G信号，OTU4、一个或多个40G信号，OTU3或STM-256/OC-768。 虽然CFP光模块可以实现100G数据应用，但其大尺寸不再能满足高密度数据中心的需求。在这种情况下，CFP-MSA委员会定义了其他两种形式：CFP2和CFP4光模块。下图显示了CFP，CFP2和CFP4光模块的尺寸对比： CFP2通常为100G应用程序提供两种解决方案，如下图所示： CFP4光模块是CFP2光模块宽度的一半，而CFP2光模块宽度又是CFP光模块的一半，它更适用于高密度网络应用。CFP4光模块解决方案如下所示： QSFP28光模块 同样，作为小尺寸的100G光模块，QSFP28光模块也受到越来越多的关注。顾名思义，QSFP28光模块与QSFP光模块一样，具有相同的设计理念。第一代QSFP光模块配有四个Tx和Rx端口，每个通道间的速率为10 Gbps。对于QSFP28光模块来说，QSFP光模块的每个通道可以发送和接收高达28 Gbps的数据。与CFP4光模块相比，QSFP28光模块尺寸仅艄小于CFP4光模块。虽然QSFP28光模块具有超出CFP4光模块的密度优势，但CFP4光模块较高的最大功耗使其在更长距离的光学传输上具有优势。 CPAK光模块 市场上还有另一个称为CPAK的100G光模块。CPAK光模块是今年流行的新模块类型。外观与思科（Cisco）光模块类似，但是接口采用IEEE标准，支持与其他接口兼容。 100G光模块可供选择的范围更加广泛，此外，100G AOC（有源光缆）组件也将推出市场，用于短距离互连和100G迁移应用，这对于100G光模块的挑战将是巨大的。同时，随着技术的飞速发展，100G光模块将变得更具成本效益，100G网络应用离我们越来越近。 3.100GBASE-SR4 Vs 100GBASE-SR10光模块如今，100G以太网在光通信领域变得越来越流行，在数据中心也形成了一种流行的趋势。下面，我们就短距离互联的两个主要物理层标准的几个方面，一起探讨一下100GBASE-SR10和100GBASE-SR4光模块之间的区别。 光模块元器件对比 CFP是典型的100GBASE-SR10代表元器件。它是由竞争激烈的制造商内部规定的一种多元协议（MSA）所定义，CFP出现在小型可插拔（SFP）接口之后，但是对于用10 x 10Gbps通道实现100Gbps的数据传输意义却非同凡响。 QSFP28是最新的100G以太网组成元器件，QSFP28使用4个25G的数据传输通道，其中各通道都支持最新的100/50/25G光模块和设备互联。因而，它是100GBASE-SR4最具代表性的组成元器件。结论：从外形来看，QSFP28比CFP更加流行。 尺寸对比 100GBASE-SR4和100GBASE-SR10接口的收发器分别对应于QSFP28和CFP。因此，它们具有在尺寸显著差异。如下图所示，CFP尺寸要比QSFP28大得多。 显而易见，CFP不适合于高密度的应用。相比之下，QSFP28增加了前面板的密度，降低了功耗，而且价格也相对便宜。因而，QSFP28比CFP更受欢迎。 结论：随着高密度已经成为数据中心的一种趋势，为了满足这一需求，QSFP28（100GBASE-SR4）相对于CFP（100GBASE-SR10）更具优势。 光电通道图对比 下图展示了100GBASE-SR4 和 100GBASE-SR10的基本构造:结论:100GBASE-SR4能够以较少通道数量实现各通道更高的数据传输速率，以便可以减小端口密度。 线缆和连接器类型对比 无论100GBASE-SR4还是100GBASE-SR10都采用光学激光优化多模光纤（OM3/ OM4）来传输信号。但100GBASE-SR4用一根12芯的标准QSFP MPO/ MTP连接线（4 Tx和4 Rx，每个通道提供25 Gbps的吞吐量）来连接，而100GBASE-SR10则使用2×12芯或24芯 MPO/ MTP连接线（10 Tx和10 RX，每个LAN提供吞吐量25 Gbps）来发射的同时，100GBASE-SR10还使用2×12芯 或24芯 MPO/ MTP 线缆（10 Tx和10Rx，每个通道提供吞吐量的10 Gbps）来连接。 100GBASE-SR4和100GBASE-SR10的接口光学上可分别达到4×25 Gbps和10×10 Gbps的数据传输速率，因此，它们只需使用MPO/ MTP到LC主干跳线就能轻而易举的完成25GbE/10 GbE 到 100 GbE的升级。 结论：100GBASE-SR4使用12芯MPO/ MTP跳线，而100GBASE-SR10使用两根12芯或一根24芯MPO/ MTP跳线。对于100G点到点互联来说，100GBASE-SR4更具成本效益。对于网络迁移，100GBASE-SR4也更具优势，因为它的分支更少，可大大降低线缆管理的成本。 总而言之，100GBASE-SR4提供主机板的布局优势，大大减少了布线的数量。同时，100GBASE-SR4在成本和功耗更具有竞争力，所以目前100GBASE-SR10已经慢慢退出市场。飞速光纤（Feisu.com）是一家致力于光通讯产品研发设计并提供系统解决方案的公司，供应一系列100GBASE-SR4光模块，可与各大品牌的交换机兼容。 4.100G QSFP28光模块——高性价比解决方案第一代100G光模块是CFP光模块，体积非常大，随后出现了CFP2和CFP4光模块，其中CFP4光模块是目前最新一代的100G光模块，其宽度只有CFP光模块的1/4，封装大小和QSFP+光模块的封装大小一致。而QSFP28光模块的封装大小比CFP4光模块更小，这意味着QSFP28光模块在交换机上具有更高的端口密度。QSFP28适用于4x25GE接入端口，SFP28适用于单个25GE接入端口。SFP28模块，基于SFP+的封装形式，支持25G以太网标准。SFP28能提供25Gb/s的无误码传输，在超四类多模光纤中传输距离可达到100米，并能应用于高密度的25G以太网交换机和网路接口中，促进数据中心的服务器连接。它采用流行的SFP+封装形式，为企业升级以太网连接，提供了一个更具成本效益的解决方案。 以下是几款100G QSFP28系列光模块：100G QSFP28 SR4光模块，传输距离为100m，作为新兴QSFP28家族中的一员，是100G网络短途传输中非常理想的一种解决方案。QSFP-100G-LR4光模块传输距离为10km,为目前超大型数据中心不断增加传输距离的需求提供了理想解决方案， 并将引领单模光纤在数据中心的使用。 当然，除了上述几款外，QSFP28系列产品还包括100G QSFP28有源光缆、100G QSFP28 PSM4光模块等等，这些产品在100G的发展中均发挥了重要作用。QSFP28光模块的优势 功耗 QSFP28光模块工作时的功耗通常不超过3.5W，而其他100G光模块的功耗通常在6W到24W之间。由此看来，QSFP28光模块要比其他100G光模块的功耗要低得多。 成本 现在的数据中心主要是10G网络架构，其互连解决方案主要是10GBASE-SR光模块和双工LC多模光纤跳线，如果在现有10G网络架构的基础上直接升级到40/100G网络将会节省大量的时间和成本。因此，数据中心的主要互连趋势之一是在不改变现有双工多模基础网络架构的情况下从10G网络升级到40/100G网络。在这种情况下，MPO/MTP可分支光缆无疑是10G向40/100G升级的理想解决方案。 带宽 采用最先进的100G传输技术为数据中心提供机架交换机和核心网之间的连接，比40G QSFP方案提高了150%的面板带宽密度 光模块测试 光模块使用时，测试性能是必不可少的步骤。光模块是由发射器和接收器组成，所以我们在测试的时候，一般分为4个步骤（如下图），主要分为对发射器和接收器的测试。 一、发射器部分 测试时，需要注意发射器输出波形的波长和形状，以及接收器的抖动容限和带宽，测试发射器时，需要注意： 用来测试发射器的输入信号的质量必须足够好。此外，还必须通过抖动测量和眼图测量来确认电气测量的质量。眼图测量是检查发射器输出波形的常见方法，因为眼图包含了丰富的信息，能够反映出发射器的整体性能。 发射器的输出光信号必须用眼图测试、光调制振幅和消光比等光学质量指标来测量。 二、接收器部分 与测试发射器不同的是，测试接收器时，光信号的质量必须足够差，因此，必须创造出一种代表最差信号的光压力眼图，这种最差的光信号必须通过抖动测量和光功率测试来进行校准。需要测试接收器的电子输出信号，这种测试主要有三个种类： 眼图测试，这样能保证眼图的“眼睛”处于张开状态。眼图测试通常由误码率的深度实现 抖动测试，测试不同类型的抖动 抖动跟踪和容限，测试内部时钟恢复电路对抖动的跟踪情况 总而言之，测试光模块是一项复杂的工作，但是也是保证其性能良好不可或缺的步骤。眼图测量作为一种广泛使用的测量方法，能有效测试光模块的发射器。而光模块的接收器测试则更加复杂一点，也需要更多的测试方法。 飞速光纤（http://Feisu.com）供应多种光模块，且每个光模块都经过兼容性测试，可与主流品牌如思科（Cisco）、惠普（HP）、IBM、Arista、博科（Brocade）、戴尔（DELL）、瞻博（Juniper）等完全兼容，质量有保证。更多详情，请直接访问飞速光纤官网。]]></content>
      <categories>
        <category>Networking</category>
      </categories>
      <tags>
        <tag>Optical module</tag>
        <tag>QSFP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[硬件知识的一些总结]]></title>
    <url>%2Fpost%2F2016%2F12%2F%E7%A1%AC%E4%BB%B6%E7%9F%A5%E8%AF%86%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93.html</url>
    <content type="text"><![CDATA[记录一些硬件相关的点。毕竟搞软件的来搞硬件是件很头疼的事。 1.逻辑电路中的RESET信号到底用高电平好还是低电平好？在TTL的年代，高电平的吸收电流要远小于低电平的，因此，那个时候需要让信号更多的时间处于高电平。例如RESET，只在很短的时间内有效，所以就做成/RESET，平时就处在高电平，以减小功耗。同样的，片选信号也是仅仅在选中的时候才让它为低电平，这样在不选中的时候就可以有一个较小的功耗。后来的CMOS由于是互补的，没有这个问题，但是为了和系统中的其他TTL电路共享这个RESET或者CS，因此很多都保持了低电平有效。 2.bram和dram区别选择distributed memory generator和block memorygenerator标准：Dram和bram区别： 1、bram 的输出需要时钟，dram在给出地址后既可输出数据。 2、bram有较大的存储空间，是fpga定制的ram资源；而dram是逻辑单元拼出来的，浪费LUT资源 3、dram使用更灵活方便些补充：在Xilinx Asynchronous FIFO CORE的使用时，有两种RAM可供选择，Block memory和Distributed memory。差别在于，前者是使用FPGA中的整块双口RAM资源，而后者则是拼凑起FPGA中的查找表形成。1、较大的存储应用，建议用bram；零星的小ram，一般就用dram。但这只是个一般原则，具体的使用得看整个设计中资源的冗余度和性能要求 2、dram可以是纯组合逻辑，即给出地址马上出数据，也可以加上register变成有时钟的ram。而bram一定是有时钟的。 3、如果要产生大的FIFO或timing要求较高，就用BlockRAM。否则，就可以用Distributed RAM。 块RAM是比较大块的RAM，即使用了它的一小部分，那么整个Block RAM就不能再用了。所以，当您要用的RAM是小的，时序要求不高的要用Distributed RAM，节省资源。FPGA中的资源位置是固定的，例如BRAM就是一列一列分布的，这就可能造成用户逻辑和BRAM之间的route延时比较长。举个最简单的例子，在大规模FPGA中，如果用光所有的BRAM，性能一般会下降，甚至出现route不通的情况，就是这个原因。]]></content>
      <categories>
        <category>Hardware tips</category>
      </categories>
      <tags>
        <tag>hardware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vivado remote Hardware Target]]></title>
    <url>%2Fpost%2F2016%2F12%2FVivado-remote-Hardware-Target.html</url>
    <content type="text"><![CDATA[由于FPGA开发板插在服务器上，往里面烧程序的话，就得去放服务器的房间，在服务器上的Vivado操作，那样就超级麻烦了。而且服务器配的显示器也很小，不便于开发。后来发现Vivado的Hardware manager支持remote Hardware Target. 这样就可以在自己的PC上开发，然后远程把程序烧到服务器上的FPGA板了。 首先，登陆插有FPGA板的服务器，cd到Vivado/bin/目录。运行hw_server程序。hw_server会在默认的3121号端口开启服务。 在自己的PC上打开Vivado.在右侧的导航栏flow Navigator-&gt;Program and debug -&gt; Hardware Manager -&gt;Open Target -&gt;Open new Targer.点击Next，然后选择Remote server。输入服务器的ip地址，端口号为默认的3121. 如果有防火墙，可以使用SSH tunnel.可以通过自己的PCssh -L 3121:localhost:3121 server_ip来建立与服务器的SSH隧道。其中3121:localhost:3121,第一个3121表示远程server的端口。第二个和第三个表示本机的IP和端口。注意：如果提示SSH不能绑定本地的3121端口。则说明本地的vivado还运行local JTAG server。需要先关闭本地Vivado。然后建立隧道后，进行上述第1步，第2步的时候，直接选择local。 ssh tunnel的一些说明。SSH的端口转发:本地转发Local Forward和远程转发Remote Forward SSH 端口转发自然需要 SSH 连接，而 SSH 连接是有方向的，从 SSH Client 到 SSH Server 。而我们所要访问的应用也是有方向的，应用连接的方向也是从应用的 Client 端连接到应用的 Server 端。比如需要我们要访问Internet上的Web站点时，Http应用的方向就是从我们自己这台主机(Client)到远处的Web Server。如果SSH连接和应用的连接这两个连接的方向一致，那我们就说它是本地转发。ssh -L &lt;local port&gt;:&lt;remote host&gt;:&lt;remote port&gt; &lt;SSH hostname&gt;如果SSH连接和应用的连接这两个连接的方向不同，那我们就说它是远程转发。ssh -R &lt;local port&gt;:&lt;remote host&gt;:&lt;remote port&gt; &lt;SSH hostname&gt;]]></content>
      <categories>
        <category>Vivado development</category>
      </categories>
      <tags>
        <tag>Vivado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[note of PR Vivado flow (UG909)]]></title>
    <url>%2Fpost%2F2016%2F12%2Fnote-of-PR-Vivado-flow-UG909.html</url>
    <content type="text"><![CDATA[1.Create a Partial Reconfiguration ProjectPR项目和标准的项目设计流一样。通过新建工程向导，选择target device,design sources and constraints, and set all the main project details.新建工程的时候，静态区域（static portion）应该添加source files和constraints。可以选择RTL design sources加入到每一个Reconfigurable Partition的第一个Reconfigurable Module中，或者什么都不加将其留为black boxes. 注意:只能添加一个RM的sources在工程创建的时候。the Partial Reconfiguration wizard将用来添加另外的RMs. 当工程创建好了之后。Tools &gt; Enable Partial Reconfiguration. 一旦被设置了之后，是不能撤销的。如果看不到这个选项，需要检查一下有没有有效的Partial Reconfiguration license. enable了之后，会出现3个PR-specific 菜单项和窗口tabs: A link to the Partial Reconfiguration Wizard in the Flow Navigator A Partition Definitions tab in the Sources window A Configurations tab in the Design Runs window 2.Define Reconfigurable Partitions一旦将工程设为PR工程之后，Reconfigurable Partitions可以在RTL source hierarchy中被定义。design hierarchy中的instances有如下的限制： Are defined by RTL, DCP or EDIF sources Do not contain embedded IP brought in as XCI sources Do not contain Out-of-Context (OOC) modules in the underlying RTL Do not contain parameters or generics of the port list that are evaluated at synthesis 在想要设为PR的模块上右键，选择Create Partition Definition，开始Reconfiguration Partition的创建过程。如果design sources不存在的话，这个模块就是一个black box.然后设置Partition Definition和第一个RM(Reconfigurable Module)的名字。 Note:选中的模块的实例都会变成Reconfigurable Partition.所以要将不同的实例设置不同的模块名字。 点击了OK之后。在Vivado中相应的模块实例的前面就变成了菱形，表示这是个Reconfigurable Partition.这些Design sources就被移动到了Partition Definitions tab以便单独的管理。 3.Complete the Partial Reconfiguration Project Structure设置完Reconfigurable Partitions后。可以通过Partial Reconfiguration Wizard来为不同的Reconfigurable Partitions添加另外的Reconfigurable Modules , 定义结合了MRs和static design的完整Configruations , declaring the set of runs that will be used to implement all the Configurations 可以在Flow Navigator中或者tools菜单，可以打开Partial Reconfiguration Wizard。 3.1 Edit Reconfigurable Modules如果在创建PD(Partition Definition)的时候，RTL/netlist source已经有了，那么第一列就会列出对应的Reconfigurable Module。可以点击绿色的加号按钮来创建新的RM。注意选择好正确的Partition Definition。如果netlist sources被选中了，要勾选Sources are already synthesized并且申明netlist中的Top Module.点击Ok后，会出现如下界面，可以编辑该项目的RMs 3.2 Edit ConfigurationsEach Configuration is a combination of the static logic plus one RM per RP; each Configuration is a full design image.每个Configuration都是static logic加上一个RP中的一个RM。都是一个完整的design image. 既可以手动地创建每一个Configuration,但是最简单的方法时让Vivado来自动地生成最小Configurations的集合，可以通过点击下图中automatically create configurations的链接。这将保证所有的RMs至少被包含一次。并且仅当目前还没有定义过configuration时，才能用这个方法。 If one Partition Definition has more RMs than another, a greybox RM will automatically be used for any RP that has all its RMs covered by prior Configurations. These default Configurations can be modified or renamed, and additional Configurations may be created if desired.(不懂这句。。。) Tip: Greybox modules不同于black box modules，因为它们不是空的。Greybox RMs have tie-off LUTs inserted to complete legal design connectivity in the absence of an RM and they ensure outputs do not float during operation. Vivado creates these by calling update_design -buffer_ports on selected modules. 注意当一个RM应用在不只一个Configuration中时，implementation的结果可能会不同。因为如果RM最初是在child run中implemented，那么每次都要place和route。如果RM implementation最初是在parent configuration中完成的，那么implementation的结果将被reuse。 3.3 Edit Configuration Runs当所有的Configuration都定义之后，最后要管理与之关联的Configuration Runs。同样，Vivado也可以自动创建一些列的Configuration Runs。列表中的第一个Configuration会被定义成母的，剩下的Configurations会被设置为其子configuration。 这个架构假定第一个configuration是最重要和challenging的。用户可以通过设置Parent列的值，来自行地更改父-子关系。A Parent of a synthesis run (synth_1 here) indicates the Configuration (most notably the static part) will be implemented from the synthesized netlist, and a Parent of an implementation run (impl_1 here) indicates the parent’s locked static implementation result will be used as the starting point. 注意：为了确保在电子元件上的安全工作环境，一个锁定的静态image必须在任何configuration时都能保持是一样的，只有这样bitstream generetion才能创建兼容的full和partial bitstreams。这是通过建立一个parent-child关系来实现的。 可以通过绿色的+号按钮增加新的Configuration Run.当所有的Configuration Runs都创建完成后，点击Next。然后会列出所有的新的elements.点击Finish才会真正地对工程进行更改。 In the Design Runs window, out-of-context synthesis runs are created for each Reconfigurable Module, and all Configuration Runs are generated. Relationships between parent and child runs are shown by the levels of indentation.In addition, the Configurations tab now shows the composition details of each Configuration available in the project. 4.Adding or Modifying Reconfigurable Modules or Configurations在源文件窗口的Partition Definitins tab，包含了每一个RM的源文件和constraints. 5.Implementing the PR Design.当所有必需的Configuration Runs都定义完了之后，就可以synthesize和implement了。Partial Reconfiguration design中每一个Reconfigurable Partition都需要一个Pblock.否则会出现如下错误：ERROR: [DRC 23-20] Rule violation (HDPR-30) Missing PBLOCK On Reconfigurable Cell解决办法在UG909的page58 Run Implementation in the Flow Navigator. 6.Generating Bitstreams当所有期望的Configurations都已经placed和routed后，就可以生成bitstreams了。和Implementation一下，直接点击Flow Navigator中的Generate Bitstream就好了。 Unsupported Features.The following features are not currently implemented:• IP Integrator support is not yet in place. Block Diagrams cannot be included as RMs or within RMs. Modules within Block Diagrams cannot be set as RMs.• IP is not yet supported within Reconfigurable Modules, at least for management within a single PR project. All IP should be managed in an external Manage IP project, then brought in as RTL or DCP. Alternatively, RMs containing IP can be synthesized external to the PR project and then brought in as an OOC DCP.° Even though IP can be added to RMs and set to Global synthesis, it cannot be modified or updated. A future Vivado release will support customization of IP from the Partition Definitions window.• RTL submodules within RMs must not be set to OOC synthesis. Do not nest OOC synthesis runs.• Reconfigurable Module instances must not have parameters, generics or other variables evaluated at runtime. The port list for the top level of each RM must be fixed and consistent.• Bitstream generation is not optimized for PR. Bitstreams can be generated from any implemented configuration, but will always create all full and partial bitstreams for the parent run, and partial bitstreams only for child runs. A future enhancement will allow users to pick which bitstreams are created from each configuration.• The Write Project Tcl feature is not yet supported.• Simulation is not supported from within the project. Known Limitations• Once Partitions are defined, they cannot be undone. The only way to return to a flat non-PR project is to create a new one.• Reuse of implemented Reconfigurable Modules from a child run is not supported. Only implementation results from RMs from the parent run can be reused in a child run.• Child implementation runs cannot be set active. Flow Navigator actions work on just the parent run, or the parent and all child runs, depending on the action.]]></content>
      <categories>
        <category>Virtex-7 Partial Reconfiguration</category>
      </categories>
      <tags>
        <tag>Vivado</tag>
        <tag>Xilinx</tag>
        <tag>Partial reconfiguration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux多网卡环境设置路由]]></title>
    <url>%2Fpost%2F2016%2F11%2Flinux%E5%A4%9A%E7%BD%91%E5%8D%A1%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE%E8%B7%AF%E7%94%B1.html</url>
    <content type="text"><![CDATA[在linux系统中，如果系统配置多块网卡，则需要正确的设置路由以引导不同的流量走不同的网卡。 一.查看路由route 二.默认路由设置 删除默认路由route del default此条命令会删除route显示的路由表的最上面的一条默认路由。 增加默认路由route add default gw IP(如：192.168.1.1) 三.网段路由设置 增加网段路由route add –net IP netmask MASK eth0route add –net IP netmask MASK gw IProute add –net IP/24 eth1 删除路由删除一条路由route del -net 192.168.122.0 netmask 255.255.255.0删除的时候不用写网关,会删除用route命令显示的路由表中匹配的第一条四.主机路由设置 增加一条路由route add –host 192.168.168.110 dev eth0route add –host 192.168.168.119 gw 192.168.168.1 五.永久路由设置在/etc/rc.local里添加路由设置命令：route add -net 192.168.3.0/24 dev eth0route add -net 192.168.2.0/24 gw 192.168.3.254]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Route setting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ATX电源接口定义]]></title>
    <url>%2Fpost%2F2016%2F11%2FATX%E7%94%B5%E6%BA%90%E6%8E%A5%E5%8F%A3%E5%AE%9A%E4%B9%89.html</url>
    <content type="text"><![CDATA[实验室的FPGA板的有一个外接的电源适配器，但是插在主板上再用外接的电源适配器供电就不方便了。随板子一起的有个ATX电源的转接线，是大4D的针转6pin口的。板子电源插口还提示不能直接用PCIE显卡的6pin的口直接插上去供电，必须要用它的大4D转6pin的线，后来测试了一下是因为FPGA板上的6pin口的12V线和地线和标准的PCIe显卡6pin口不一致。 问题来了。我们的服务器的ATX电源没有D型4pin的口，这下可怎么搞。于是我就查了一下ATX电源的各种接头的定义规范，准备自己DIY一个6PIN转6PIN的线，达到给FPGA板正确供电的效果。 早期电脑电源采用20Pin接口，自intel规范形成了ATX 12V 1.3版本后成为了现在使用的24Pin接口形式。并且对各针脚电压做出了定义，也就是电源的输出端与硬件的接头。下面是在网上找到的各类接头的定义规范： 主板24Pin接口定义 CPU4Pin接口定义 CPU4+4Pin接口定义 显卡6Pin接口定义 显卡6+2Pin接口定义 D口定义 SATA口定义 按照intel所定义的电源规范，所有电源厂商使用的线材需要统一规范，各电源线颜色与用途如下：红色：＋5V 主板电路、内存模块供电、光驱、硬盘等设备的信号供电 黄色：＋12V CPU、显卡供电；为标准的驱动电路供电，如光驱、硬盘的马达 橙色：＋3.3V 现在多用于 SATA 硬盘的供电，以后会有其他用途 紫色：＋5V（USB）USB设备供电，支持USB键盘鼠标的开机功能（关机后依然供电） 黑色：地线（0V） 电源供电回路的必要组成部分 绿色：PS－ON 开机信号线（当其与地线短接会启动电源） 灰色：Power Good 监测线，连接主板与电源，起到信号反馈作用]]></content>
      <categories>
        <category>LuanQiBaZao</category>
      </categories>
      <tags>
        <tag>ATX power supply</tag>
        <tag>DIY</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[eclipse中相关issue的solutions]]></title>
    <url>%2Fpost%2F2016%2F11%2Feclipse%E4%B8%AD%E7%9B%B8%E5%85%B3issue%E7%9A%84solutions.html</url>
    <content type="text"><![CDATA[本文记录在使用eclipse进行java开发时运到的问题和相应的解决方法。 错误Access restriction:The type *** is not accessible due to restriction on...解决方案当报此类error的时候，需要重新添加libraries，因为在多个不同的jar里面有classes,remove了之后重新add JRE lib会让正确的classes成为first.Solution:Project-&gt;Properties-&gt;Java buildpath-&gt;libraries,先remove掉JRE System Library,然后再Add Library重新加入系统默认的JRE.Access restriction的原因是因为这些JAR默认包含了一系列的代码访问规则（Access Rules），如果代码中引用了这些访问规则所禁止引用类，那么就会提示这个错误信息。一、既然存在访问规则，那么修改访问规则即可。打开项目的Build Path Configuration页面，打开报错的JAR包，选中Access rules条目，选择右侧的编辑按钮，添加一个访问规则即可。二、网上的另外一种解决方案：Window - preference - Java - Compiler - Errors/Warnings界面的Deprecated and restricted API下。把Forbidden reference (access rules): 的规则由默认的Error改为Warning。这种方案是修改整个Eclipse开发环境，将所有禁止访问的引用由原来的Error（默认）修改为Warning。这种规避方式比较粗暴，个人支持第一种方案]]></content>
      <categories>
        <category>Java &amp; eclipse</category>
      </categories>
      <tags>
        <tag>Eclipse</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux系统给安装的软件添加桌面启动菜单快捷方式]]></title>
    <url>%2Fpost%2F2016%2F11%2Flinux%E7%B3%BB%E7%BB%9F%E7%BB%99%E5%AE%89%E8%A3%85%E7%9A%84%E8%BD%AF%E4%BB%B6%E6%B7%BB%E5%8A%A0%E6%A1%8C%E9%9D%A2%E5%90%AF%E5%8A%A8%E8%8F%9C%E5%8D%95%E5%BF%AB%E6%8D%B7%E6%96%B9%E5%BC%8F.html</url>
    <content type="text"><![CDATA[FPGA板开发的原因，Xilinx官方给的test example是用java awt写的GUI，但是在服务器上运行的时候有错误，而且出现错误后关掉程序重新运行就会提示Another instance of GUI already running。没办法咯，只能自己看下JAVA程序的源码咯。所以就下载了个Eclipse,解压后直接用的，我想要加入centos右上角那个Application索引里面，所以就上网搜了一下。 解压下载的Eclipse package 12解压到/opt目录下tar -zxvf eclipse-java-neon-1a-linux-gtk-x86_64.tar.gz -C /opt -C /opt表示解压到/opt目录 使用符号链接，将eclipse执行文件链接到/usr/bin 1ln -s /opt/eclipse/eclipse /usr/bin/eclipse 创面一个共享的桌面启动器 1vim /usr/share/application/eclipse.desktop 编辑如下内容 12345678910[Desktop Entry]Encoding=UTF-8Name=Eclipse NeonComment=Eclipse NeonExec=/usr/bin/eclipseIcon=/opt/eclipse/icon.xpmCategories=Application;Development;Java;IDEVersion=1.0Type=ApplicationTerminal=0]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Desktop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fedora 24安装nvidia显卡驱动]]></title>
    <url>%2Fpost%2F2016%2F11%2Ffedora-24%E5%AE%89%E8%A3%85nvidia%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8.html</url>
    <content type="text"><![CDATA[生命在于折腾。真想狗带。 前言：昨天买的FPGA板到了，run了一下BIST(built-in-self-test),功能都是正常的。然后就是测试PCI-E和10G光口。噩梦就这么开始了。 最开始是服务器的电源没有D形4pin 的母口，FPGA板自带的4Pin转6pin的转接线没法接在服务器的电源上，准备直接用服务器电源的6pin口接FPGA，结果服务器插上电就闪红灯，提示电源有问题。没办法，只好用外接的电源适配器。然后把板子插上后，就准备进系统了。结果，进入centos后，在等待的时候，屏幕上方闪了一排企鹅，屏幕就黑了，没有视频输入了。懵逼。以为是系统的原因，切到WIN10系统，然后也是进不去系统。后来进bios,可以识别FPGA板。结果尝试了把uefi换成legacy重装系统，还是不行。一直折腾了到第二天，连DELL的工程师都没办法。最后，我觉得不是没进去系统，是显卡没输出信号了。然后整了个小显示器插在了DVI口上，奇迹就这么发生了。有显示了。原来是插上FPGA板后,显卡不支持2K DP口输出了。没办法，只能先用小显示器，在系统里装好Nvidia的驱动在用2K显示器咯。 安装Nvidia驱动那么接下来就是正文咯。fedora 24中安装Nvidia驱动，这个也是历经千辛万苦。 去官网下载对应的驱动一般驱动为NVIDIA-Linux-x86_64-xxx.xx.run安装驱动，只需要以root运行 ./NVIDIA-Linux-x86_64-xxx.xx.run 下面列一下遇到的问题。 1.不能在图形界面下安装Nvidia的显卡驱动 提示不能在图形界面下安装Nvidia的显卡驱动，必须退出X server才行。解决方法1：init 3 进入level 3 字符界面操作。解决方法2：Fedora 使用的init程序是systemd,所以其进入字符界面的方法是以root运行systemctl set-default multi-user.target 2.Gcc 提示我们安装驱动需要先安装gcc。解决方法：dnf install gcc 3.需要内核源码 提示没有内核源码，驱动需要和内核一起编译。解决方法：dnf install kernel-devel，需要注意的是kernel-devel需要和系统本来的kernel版本是一致的。 4.提示nvidia.io模块无法加载 该错误提示的意思是说 nvidia.ko 模块无法成功加载，是因为 nouveau 模块还在。要禁掉 nouveau 模块，只需要在 /etc/modprobe.d/ 目录下建立一个 .conf 文件，在里面写上 blacklist nouveau 即可，这件事 Nvidia 驱动的安装程序已经帮我们做了，但是依然无法阻止 nouveau 模块的加载。为什么呢？那是因为 Linux 启动时会先加载 initramfs 中的模块，如果不更新 initramfs 的话，单纯写 /etc/modprobe.d/ 目录下的配置文件也没有什么用。在 Fedora 中更新 initramfs 使用这个命令dracut --force。 最后使用init 5或者systemctl set-default graphical.target命令设置让系统开机时进入图形界面，然后reboot命令重启。 卸载Nvidia驱动装好驱动后本来以为2k屏可以愉快地工作了，结果发现我太天真了。2k屏能显示的最大的分辨率是1080p,而且刷新率只有23Hz。简直不能忍。只好卸载驱动咯。 使用./NVIDIA-Linux-x86_64-xxx.xx.run -h运行，可以看到安装程序的帮助信息。可以发现-x选项，对文件进行压缩。运行./NVIDIA-Linux-x86_64-xxx.xx.run -x后，可以发现有好多文件。然后运行nvidia-installer --uninstall命令，就可以将Nvidia 安装过程中的一些问题总结 1234ERROR: You appear to be running an X server; please exit X before installing. For further details, please see the section INSTALLING THE NVIDIA DRIVER in the README available on the Linux driver download page at www.nvidia.com. 如果init 3了，还是不行。可以用ps ax | grep X查看具体哪些进程在用Xserver,然后kill -9 ID杀掉进程。 yum install akmod-nvidia]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Nvidia</tag>
        <tag>Fedora</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala函数相关知识点]]></title>
    <url>%2Fpost%2F2016%2F11%2Fscala%E5%87%BD%E6%95%B0%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9.html</url>
    <content type="text"><![CDATA[1.scala函数可以赋值给变量 这里我们定义了一个函数fun1,然后将他赋值给了一个变量fun_v。格式为： val 变量名 = 函数名 + 空格 _这里函数名后面必须要有空格，表明是函数的原型。_代表函数的参数。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vncserver的一些设置]]></title>
    <url>%2Fpost%2F2016%2F11%2Fvncserver%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%BE%E7%BD%AE.html</url>
    <content type="text"><![CDATA[实验室有台DELL的服务器，平常都开着。服务器上部署着VNC-server,方便其他同学远程连接进行一些开发工作和实验。 刚好我的显示器有台是2k分辨率的，用VNC-viewer登录服务器后，发现能设置的最大分辨率是1920*1200。全屏的时候不能占满整个屏幕，这严重影响我操作时候的视觉美感。 设置分辨率遂在网上搜了一下怎么设置分辨率。 设置vnc server的分辨率。在开启vncserver的服务时，加上-geometry参数。如： 1vncserver -geometry 2560x1440 :2 即在5902号端口开启当前用户的vncserver服务，注意2560x1440，其中是字母x,不是符号* ， 查看系统中当前运行的vncserver进程有哪些。 1ps -ef | grep -i vnc | grep -v grep 然后用kill -9 pidkill到自己想停掉的vnc服务。因为有时候vncserver -list并不显示全自己已经开启的vncserver进程。 配置VNCserver开机自动开启一个进程。由于机器要给组里其他同学做实验用，可能经常要开关机。为了每次开机都能自动开启VNCserver的一个端口服务，方便我自己远程连接。遂决定设置开机启动2个vncserver进程，5901给root,5902给自己的user. vncserver的配置文件在 /lib/systemd/system/vncserver@.service将配置文件复制到/etc/systemd/system/目录。1cp /lib/systemd/system/vncserver@.service /etc/systemd/system/vncserver@:2.servicevncserver@:2.service对应5902号端口(即启动vncserver时的命令：vncserver :2)对应的启动脚本。 修改刚刚复制的配置文件vim /etc/systemd/system/vncserver@:2.service原始文件如下：按照下图修改： 说明： User指明该脚本是以root运行的。不然的话，后面的步骤启动该服务会提示（code=exited,status=1/failed)的错误，就是因为权限不够。将脚本中的两处修改为你需要运行这个端口的用户名。 然后运行systemctl daemon-reloadsystemctl start vncserver@:2.service就可以启动对应的5902号端口的VNCserver服务systemctl stop vncserver@:2.service是关闭5902号端口的VNCServer服务systemctl enable vncserver@:2.service设置5902号端口的VNCserver服务开机自动启动 设置root的5901号端口的vncserver服务开机自动启动的话，直接把/etc/systemd/system/vncserver@:2.servcie复制一份命名为vncserver@:1.servcie,其中的1即代表5901号端口（同在命令行启动vncserver时的命令：vncserver :1）。然后将vncserver@:1.servcie里面2处对应的地方改成root就可以了。 PS: 设置过程中，经过不断的重启测试自动启动服务是否正常。遇到如下问题。 code=exited,status=1/failed权限问题，上面给出了解决方案。 code=exited,status=2/failed是X11的临时文件的问题。去/tmp/.X11-unix删除对应的文件。几号端口有问题，就删除对应的X几文件。如果启动vncserver@:2.servcie出现status=2的问题，则删除X2文件。 设置其他用户的vncserver某个端口服务自动启动后，非root用户用vncviewer连接时，会提示1Authentication is required to set the network proxy used for downloading packages 虽然直接点击cancel就可以了，但是有这个提示还是很烦的。所以我搜了一下解决办法：disable autostart of ‘gnome-software=service’1sed -e &apos;$aX-GNOME-Autostart-enabled=false&apos; -e &apos;/X-GNOME-Autostart-enabled/d&apos; -i.bak /etc/xdg/autostart/gnome-software-service.desktop for those of us who want to edit the file manually simply add X-GNOME-Autostart-enabled=false to the end of /etc/xdg/autostart/gnome-software-service.desktop and restart vncserver. Authentication of colorAuthentication is required to create a color managed devicesolution https://bugzilla.redhat.com/show_bug.cgi?id=1149893#c13You can place a .rules file in /etc/polkit-1/rules.dI’m doing in 02-allow-colord.rules:1234567891011polkit.addRule(function(action, subject) &#123; if ((action.id == &quot;org.freedesktop.color-manager.create-device&quot; || action.id == &quot;org.freedesktop.color-manager.create-profile&quot; || action.id == &quot;org.freedesktop.color-manager.delete-device&quot; || action.id == &quot;org.freedesktop.color-manager.delete-profile&quot; || action.id == &quot;org.freedesktop.color-manager.modify-device&quot; || action.id == &quot;org.freedesktop.color-manager.modify-profile&quot;) &amp;&amp; subject.isInGroup(&quot;nwra&quot;)) &#123; return polkit.Result.YES; &#125;&#125;); Which allows users in our group to access colord. 11.30.2016 addcentos和fedora使用systemd管理配置信息。 复制/lib/systemd/system/vncserver@.service到/etc/systemd/system/vncserver@.service来创建服务。 需要将/etc/systemd/system/vncserver@.service文件中的USER改成实际用户的用户名。-geometry参数可以指定桌面分辨率的大小，默认为1024x768。 为不同的用户设置独立显示的VNC配置。可以通过配置不同的显示设备号来实现，比如在配置文件名中加入设备号3和5：systemctl start vncserver-USER_1@:3.serversystemctl start vncserver-USER_2@:5.server其中:3和:5会被SYSTEMD自动替换成配置文件中的%i。 限制VNC权限如果你只用加密传输，你可以关闭非加密传输，在service配置文件里修改如下：ExecStart=/sbin/runuser -l user -c &quot;/usr/bin/vncserver -localhost %i&quot;这样VNC server对于非加密传输将只接受本机，不再允许远程方式。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Vncserver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware中Mac os X虚拟机调整磁盘大小]]></title>
    <url>%2Fpost%2F2016%2F11%2FVMware%E4%B8%ADMac-os-X%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%B0%83%E6%95%B4%E7%A3%81%E7%9B%98%E5%A4%A7%E5%B0%8F.html</url>
    <content type="text"><![CDATA[今天想起来之前编的一个补实验室QA的iphone app还有些功能没开发完。打开了很久没用的macos虚拟机，发现XCODE有更新。果断准备先更新一波。结果提示我磁盘空间不足。我用df -hl看了下，总共有40G，已经用了30G，结果还提示我磁盘空间不足。看了Xcode真是个大家伙。没办法，只能给虚拟机扩容啦。 关闭虚拟机后，在对应的磁盘管理里，我扩展到了100G。然后重新开机。结果发现os 10.10以上的版本出现了PCI外置磁盘大小通过磁盘工具无法扩展的问题。 具体的效果就是：可以在磁盘工具中看到对应的磁盘已经扩展到目标大小（100G）,但是不能进行分区，其中唯一一个Mac os X分区也不能进行抹掉和扩展操作。原因很简单，这是盘是系统盘，所以不能在系统运行的时候进行操作。但是VMware又进不了MacOsX的恢复分区，那怎么办咯。 百度了一下，就搜到方法了：diskutilMacOsX有一个diskutil命令。 具体步骤 打开终端，输入diskuitil list; 从显示的列表中找到你需要扩展的分区。如上图，磁盘总共有107.4G，但是系统分区只有42.1G。 输入diskutil resizeVolume disk0s2 100GB其中disk0s2为对应分区的ID,100GB为目标大小（注意不要超过磁盘的总大小）。然后安静的等待一会就好啦。]]></content>
      <categories>
        <category>LuanQiBaZao</category>
      </categories>
      <tags>
        <tag>Vmware</tag>
        <tag>Macos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7修改SSH的端口号]]></title>
    <url>%2Fpost%2F2016%2F11%2Fcentos7%E4%BF%AE%E6%94%B9SSH%E7%9A%84%E7%AB%AF%E5%8F%A3%E5%8F%B7.html</url>
    <content type="text"><![CDATA[最近刚搬了实验室。新的实验室的网络是一个H3C ER2100的路由器。VPN只支持IPsec，没办法设置个人vpn。想要外网连入实验室局域网的服务器，进行远程开发的话。只能用H3C的虚拟服务器了。 比如想要访问局域网内的一台服务器（局域网的ip:192.168.1.8）,H3C路由器的WAN ip: xxx.xxx.xxx.xxx可以设置虚拟服务器为：123外部端口：22内部端口：22内部服务器IP:192.168.1.8 这样就可以通过ssh user@xxx.xxx.xxx.xxx来远程登录新实验室的服务器了。结果通过腾讯云里的主机测试，发现连不上。后来发觉可能是因为学校限制了 1-1024 号端口的外网访问。遂尝试更改服务器的ssh端口地址进行测试。 服务器的系统为centos7。更改ssh服务的端口地址的步骤为： 修改/etc/ssh/sshd_config 1234vim /etc/ssh/sshd_config#Port 22 //取消注释Port 22222 //增加此行 即开放22和22222号端口 修改SELinux使用以下命令查看当前SELinux允许的ssh端口semanage port -l | grep ssh会发现只有22号端口所以需要添加22222号端口，运行以下命令semanage port -a -t ssh_port_t -p tcp 22222最后确认是否添加成功，再次运行semanage port -l | grep ssh成功的话，会输出ssh_port tcp 22,22222 重启sshsystemctl restart sshd.service 开放22222号端口centos7防火墙换成了firewalldfirewall-cmd --permanent --add-port=22222/tcp 再次添加一个H3C路由器的虚拟服务器123外部端口：22222内部端口：22222内部服务器IP:192.168.1.8 在腾讯云主机测试ssh -p 22222 user@xxx.xxx.xxx.xxx指定ssh的端口用-p 22222，最后登录成功 插曲实验室另一个学长的服务器装的是Ubuntu 16的系统。给他设置远程访问的时候烦死了。真心不想用Ubuntu系统。搞了半天才发现防火墙是ufw。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Centos7</tag>
        <tag>Ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git开发中的常用命令]]></title>
    <url>%2Fpost%2F2016%2F11%2Fgit%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html</url>
    <content type="text"><![CDATA[分支git checkoutgit checek命令允许你切换用git branch创建的分支。查看一个分支会更新工作目录中的文件，以符合分支中的版本，它还会告诉Git记录那个分支上的新提交。 git merge1git merge --no-ff &lt;branch&gt; 将指定分支并入当前分支，但总是生成一个合并提交（即使是快速向前合并）。这可以用来记录仓库中发生的所有合并 撤销，查看历史等。git revert1git revert &lt;commit&gt; 生成一个撤销了指定的&lt;commit&gt;引入的修改的新提交，然后应用到当前分支。 撤销(revert)应该用在你想要项目历史中移除一整个提交的时候。比如，你在追踪一个bug,然后你发现它是由一个提交造成的，这时候撤销就很有用。与其说自己去修复它，然后提交一个新的快照，不如用git revert，它帮你做了所有的事情。 撤销(revert)和重设(reset)对比理解这一点很重要–git revert回滚了『单独一个提交』——它没有移除后面的提交，然后回到项目之前的状态。 撤销和重设相比有两个重要的优点。首先，它不会改变项目历史，对那些已经发布到共享仓库的提交来说这是一个安全的操作。其次，git revert可以针对历史中任何一个提交，而git reset只能从当前提交向前回溯。比如，你想用git reset重设一个旧的提交，你不得不移除那个提交后的所有提交，再移除那个提交，然后重新提交后面的所有提交。不用说，这并不是一个优雅的回滚方案。 例子下面这个例子是git revert一个简单的演示。它提交了一个快照，然后立即撤销这个操作。 1234567# 编辑一些跟踪的文件# 提交一份快照git commit -m &quot;Make some changes that will be undone&quot;# 撤销刚刚的提交git revert HEAD 这个操作可以用下图可视化： git reset和git checkout一样，git reset有很多种用法。它可以被用来移除提交快照，尽管它通常被用来撤销缓存区和工作目录的修改。不管是哪种情况，它应该只被用于 本地 修改——你永远不应该重设和其他开发者共享的快照。 用法1git reset &lt;file&gt; 从缓存区移除特定文件，但不改变工作目录。它会取消这个文件的缓存，而不覆盖任何更改。1git reset 重设缓冲区，匹配最近的一次提交，但工作目录不变。它会取消 所有 文件的缓存，而不会覆盖任何修改，给你了一个重设缓存快照的机会。1git reset --hard 重设缓冲区和工作目录，匹配最近的一次提交。除了取消缓存之外，–hard 标记告诉Git还要重写所有工作目录中的更改。换句话说：它清除了所有未提交的更改，所以在使用前确定你想扔掉你所有本地的开发。1git reset &lt;commit&gt; 将当前分支的末端移到，将缓存区重设到这个提交，但不改变工作目录。所有之后的更改会保留在工作目录中，这允许你用更干净、原子性的快照重新提交项目历史。1git reset --hard &lt;commit&gt; 将当前分支的末端移到，将缓存区和工作目录都重设到这个提交。它不仅清除了未提交的更改，同时还清除了之后的所有提交。 git cleangit clean命令将未跟踪的文件从你的工作目录中移除。它只是提供了一条捷径，因为用git status查看哪些文件还未跟踪然后手动移除它们也很方便。和一般的rm命令一样，git clean是无法撤消的，所以在删除未跟踪的文件之前想清楚，你是否真的要这么做。 git clean命令经常和git reset –hard一起使用。记住，reset只影响被跟踪的文件，所以还需要一个单独的命令来清理未被跟踪的文件。这个两个命令相结合，你就可以将工作目录回到之前特定提交时的状态。 用法1git clean -n 执行一次git clean的『演习』。它会告诉你那些文件在命令执行后会被移除，而不是真的删除它。 1git clean -f 移除当前目录下未被跟踪的文件。-f(强制)标记是必需的，除非clean.requireForce配置项被设为了false (默认为true)。它 不会 删除 .gitignore中指定的未跟踪的文件。 1git clean -f &lt;path&gt; 移除未跟踪的文件，但限制在某个路径下。 1git clean -df 移除未跟踪的文件，以及目录。 1git clean -xf 移除当前目录下未跟踪的文件，以及Git一般忽略的文件。 讨论如果你在本地仓库中作死之后想要毁尸灭迹，git reset –hard和git clean -f是你最好的选择。运行这两个命令使工作目录和最近的提交相匹配，让你在干净的状态下继续工作。git clean命令对于build后清理工作目录十分有用。比如，它可以轻易地删除C编译器生成的.o和.exe二进制文件。这通常是打包发布前需要的一步。-x命令在这种情况下特别方便。请牢记，和git reset一样， git clean是仅有的几个可以永久删除提交的命令之一，所以要小心使用。事实上，它太容易丢掉重要的修改了，以至于Git厂商 强制 你用-f标志来进行最基本的操作。这可以避免你用一个git clean就不小心删除了所有东西。 例子下面的例子清除了工作目录中的所有更改，包括新建还没加入缓存的文件。它假设你已经提交了一些快照，准备开始一些新的实验。 123456789# 编辑了一些文件# 新增了一些文件# 『糟糕』# 将跟踪的文件回滚回去git reset --hard# 移除未跟踪的文件git clean -df 在执行了reset/clean的流程之后，工作目录和缓存区和最近一次提交看上去一模一样，而git status会认为这是一个干净的工作目录。你可以重新来过了。注意，不像git reset的第二个栗子，新的文件没有被加入到仓库中。因此，它们不会受到git reset –hard的影响，需要git clean来删除它们。 git checkout1git checkout &lt;commit&gt; &lt;file&gt; 查看文件之前的版本。它将工作目录中的文件变成中那个文件的拷贝，并将它加入缓存区。1git checkout &lt;commit&gt;更新工作目录中的所有文件，使得和某个特定提交中的文件一致。你可以将提交的哈希字串，或是标签作为参数。这会使你处在分离HEAD的状态。 讨论 版本控制系统背后的思想就是『安全』地储存项目的拷贝，这样你永远不用担心什么时候不可复原地破坏了你的代码库。当你建立了项目历史之后，git checkout是一种便捷的方式，来将保存的快照『加载』到你的开发机器上去。检出之前的提交是一个只读操作。在查看旧版本的时候绝不会损坏你的仓库。你项目『当前』的状态在 master上不会变化。在开发的正常阶段，HEAD一般指向master或是其他的本地分支，但当你检出之前提交的时候，HEAD就不再指向一个分支了——它直接指向一个提交。这被称为『分离HEAD』状态。在另一方面，检出旧文件不影响你仓库的当前状态。你可以在新的快照中像其他文件一样重新提交旧版本。所以，在效果上，git checkout的这个用法可以用来将单个文件回滚到旧版本 。 例子 这个栗子假定你开始了一个疯狂的实验，但你不确定你是否想要保留它。为了帮助你决定，你想看一看你开始实验之前的项目状态。首先，你需要找到你想要看的那个版本的ID。1git log --oneline假设你的项目历史看上去和下面一样：12345b7119f2 继续做些丧心病狂的事872fa7e 做些丧心病狂的事a1e8fb5 对hello.py做了一些修改435b61d 创建hello.py9773e52 初始导入你可以这样使用git checkout来查看『对hello.py做了一些修改』这个提交：1git checkout a1e8fb5这让你的工作目录和a1e8fb5提交所处的状态完全一致。你可以查看文件，编译项目，运行测试，甚至编辑文件而不需要考虑是否会影响项目的当前状态。你所做的一切 都不会 被保存到仓库中。为了继续开发，你需要回到你项目的『当前』状态：1git checkout master这里假定了你默认在master分支上开发，我们会在以后的分支模型中详细讨论。一旦你回到master分支之后，你可以使用 git revert或git reset来回滚任何不想要的更改。 检出文件 如果你只对某个文件感兴趣，你也可以用git checkout来获取它的一个旧版本。比如说，如果你只想从之前的提交中查看hello.py文件，你可以使用下面的命令：1git checkout a1e8fb5 hello.py记住，和检出提交不同，这里 确实 会影响你项目的当前状态。旧的文件版本会显示为『需要提交的更改』，允许你回滚到文件之前的版本。如果你不想保留旧的版本，你可以用下面的命令检出到最近的版本：1git checkout HEAD hello.py]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用git时的一些技巧、问题和解决方案]]></title>
    <url>%2Fpost%2F2016%2F10%2F%E4%BD%BF%E7%94%A8git%E6%97%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%80%E5%B7%A7%E3%80%81%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.html</url>
    <content type="text"><![CDATA[本文描述一些使用git时，需要用到的一些技巧。以及可能遇到的问题和相应的解决方案。 可能遇到的问题1.warning: LF will be replaced by CRLF CRLF: Carriage-Return Line-Feed回车换行就是回车(CR, ASCII 13, \r) 换行(LF, ASCII 10, \n)。这两个ACSII字符不会在屏幕有任何输出，但在Windows中广泛使用来标识一行的结束。而在Linux/UNIX系统中只有换行符。也就是说在windows中的换行符为 CRLF，而在linux下的换行符为：LF当原始文件为Linux系统中生成的文件时，其中为LF，当在windows系统执行git指令时，系统提示：LF 将被转换成 CRLF。 解决方法：配置git参数core.autocrlf为false1$ git config --global core.autocrlf false git中的一些技巧1.使用分支在Git中，分支是你日常开发流程中的一部分。当你想要添加一个新的功能或是修复一个bug时——不管bug是大是小——你都应该新建一个分支来封装你的修改。这确保了不稳定的代码永远不会被提交到主代码库中，它同时给了你机会，在并入主分支前清理你feature分支的历史。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Hexo和Next的一些技巧]]></title>
    <url>%2Fpost%2F2016%2F10%2F%E4%BD%BF%E7%94%A8Hexo%E5%92%8CNext%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%80%E5%B7%A7.html</url>
    <content type="text"><![CDATA[本文描述一些关于Hexo和NEXT的技巧，大多数为自己在网上搜索所得，整理于此，方便自己查看。 1. Hexo设置阅读全文Hexo的首页中，文章是会显示全文的，这样看起来太冗长了。首页应该显示文章的部分截取，然后提供“阅读全文”功能，点击跳转到具体的文章显示页。我在网上搜索到了3种方法。 在文章中使用&lt;!-- more --&gt;手动进行截取这种方法可以根据文章的内容，自己在合适的位置添加&lt;!-- more --&gt;标签，则首页显示的文章截取到该标签之前。注意：Next主题中，需要将主题配置文件_config.yml中的 scorll_to_more设置为true; 在文章的front-matter中添加description，并提供具体的description描述。这种方法只会在首页列表中显示文章的摘要内容，进入文章详情后不会再显示。 1234567---titile: firstArticledate: tag:categories:description: it is the desciption of this article, and it will display at homepage. When you clcik 'read more', it won't showed at the whole article.--- 自动形成摘要，在主题配置文件中设置默认截取的长度为150字符，可以根据需要自行设置 123auto_excerpt:enable: truelength: 150 建议使用第1种方法，这样不仅可以精确的控制不同的文章需要显示的摘要的内容，还可以让Hexo中的插件更好地识别。并且第一种方法的优先级比第三种方法的高。 2.Next主题解决首页菜单(导航栏)中tags和categories需要手动创建页面在NEXT主题中，可以设置菜单。需要编辑主题配置文件_config.yml，对应的字段为menu。1234567menu: home: / archives: /archives #about: /about #categories: /categories tags: /tags #commonweal: /404.html其中:前面代表menu中选项的字段，后面表示该字段对应的链接为网站的那个目录。其中tags和categories取消注释后，Hexo generate后会提示没有页面的。这是因为我们只是创建了这个链接，而没有创建页面，默认情况下home和archieves页面是不用自己创建的，所以tags和categories需要我们自己创建。 创建分类在Hexo站点的根目录下执行1hexo new page &quot;tags&quot; 修改type字段执行完，在Hexo根目录下的source文件夹中会多出一个tags文件夹，里面默认有一个index.md的文件，这就是我们创建的导航栏的tags页面默认这个页面是空的，我们需要修改其front-matter以让Hexo识别。编辑index.md添加type和comments字段 12type: tagscomments: false Hexo会根据type字段来生成对应的页面的。同理，categoris页面需要设置对应的type为categories]]></content>
      <categories>
        <category>Hexo&amp;NEXT</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署spark调试开发环境]]></title>
    <url>%2Fpost%2F2016%2F10%2F%E9%83%A8%E7%BD%B2spark%E8%B0%83%E8%AF%95%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83.html</url>
    <content type="text"><![CDATA[本文摘取自网络，非本人原创 记录一下关于 spark 开发的相关环境配置, 以及与 intellij 的集成. 本文针对的是需要了解 spark 源码 的情况下的开发环境配置. 如果只是需要写 spark job, 而并不想 trace 到源码里面去看运行上下文, 那么有很多资料讲这个的了: 无非是下载 spark 的 jar, 新建一个 scala/python/R 项目, 把这个 jar 设置成依赖就可以开始写了. 具体怎么运行 spark job, 在官方文档中已经写得很清楚了. 本文记录的 不是 后一种情况由于实验室工作需要于是开始学习 spark 的源码. 之前都是东一坨西一块地配置, 好了也不知道为什么, 没好也只会到处找相关 blog, 然后关闭项目再从头 import 试试. 大部分能找到的中文资料还是比较没用, 或者只能干掉一两件事, 于是坐下来好好看了看 maven 的入门文档 (因为 spark 开发组偏向于 maven 做项目管理, 感觉钦定的比较🐵), 从头到尾地把大部分入门 spark 开发的 intellij 配置给手动操作了一遍, 目前算是一篇milestone文档.可能的前置要求 能够翻墙(最好能够翻墙)Windows 没有充分测试, 建议 POSIX 环境脑补能力配置代码阅读环境 符号跳转获取 spark 源码从这里选择一个镜像, 下载源码 (我用的是spark-1.6.1.tgz). 编译spark源码这里可以配置国内的Maven镜像，提高Mevan获取速度。以下是OSChina的镜像1234567891011121314 &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-osc&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus osc&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-osc-thirdparty&lt;/id&gt; &lt;mirrorOf&gt;thirdparty&lt;/mirrorOf&gt; &lt;name&gt;Nexus osc thirdparty&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/repositories/thirdparty/&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 以下是阿里云的镜像12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 然后用mvn buildbuild/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package 执行一次干净的从头编译, 包括所有 spark submodules, 不运行测试 在官方文档里面有更详细的介绍 配置Intellij IDEA编译完Spark，在导入Intellij IDEA之前，先配置好IDEA. a. 配置SBTFILE–&gt;other settings–&gt;Build,Execution,Deployment–&gt;Build Tools–&gt;SBT JVM–&gt;Custom选择你自己的Java 安装目录 Launcher(sbt-launch.jar)–&gt;Custom选择你自己的sbt-launch.jar b. 配置MavenFile–&gt;Build,Execution,Deployment–&gt;Build Tools–&gt;Maven 修改Maven home directory, 设置成你的安装目录 修改User settings file,设置成你的安装目录下面conf中settings.xml File–&gt;Build,Execution,Deployment–&gt;Build Tools–&gt;Maven–&gt;Runner VM Options 加入如下参数-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m JRE设置成自己想要的Java版本 c. 配置ScalaFile–&gt;Settings–&gt;Build,Execution,Deployment–&gt;Compiler–&gt;Scala Compiler Incrementality type设置成SBT. File–&gt;Settings–&gt;Languages &amp; Frameworks–&gt;Scala Compile Server 选中 Use external compile server for scala JVM SDK 设置成自己需要的。 导入Intellij导入 Intellij 项目 在 Intellij 什么项目也没有打开的小页面, 选择 Import Project -&gt; /path/to/spark-1.6.1/ -&gt; 单选 Import project from external model[Maven] -&gt; 勾选 Search for projects recursively -&gt; Next -&gt; Next -&gt; Next -&gt; Next -&gt; Finish配置依赖Maven 等待 Intellij 下方状态条显示 Index 和 Resolve Dependencies 等工作结束, 查看左边Project视图最下方的External Libraries, 展开后应该只有JDK 在Project视图中, 右键点击根目录 (spark-1.6.1) 下的pom.xml文件, 选择 Maven -&gt; Reimport, 完成后在External Libraries内会找到大量 spark 项目的 maven 依赖 如果没有执行之前的编译操作, 这一步大概并不能找到 maven 依赖Scala 随便打开一个 scala 文件, 比如examples/src/main/scala/org/apache/spark/examples/SparkPi, 编辑器右上角会提示设置 scala sdk, 建议设置为2.10.5版本 如果下拉框里面没有 scala sdk, 那么创建一个, 如果创建窗口里面仍然没有, 点击Download下载一个, 注意版本别太高, 行为未知到这个地方, Intellij 已经解析了需要的依赖的symbol, 跳转功能正常, 已经可以阅读代码了. 接下来的配置是为了更好地编写调试代码 配置代码开发环境 编译-&gt;打包-&gt;运行改动后快速编译 maven快速编译如果每次都使用之前的编译整个项目的mvn命令进行编译的话, 每天工作10小时大概能改20多次代码💩. 我想帮老板多干点活, 于是搬砖之前先去考察下有没有解决方案. 这里有一个能用的方案, 有更好的再更新. Intellij 提供了比较好的 maven GUI 集成, 所以我们可以通过鼠标和快捷键来运行maven lifecycle/goal. 在 Intellij 里面打开View -&gt; Tool Windows, 此时右侧会有Maven Projects按钮, 点击可以打开能够执行的 maven lifecycle/goal. 比如我更改了 examples/…/SparkPi.scala文件, 然后想重新编译打包运行, 那么我找到Maven Projects -&gt; Spark Project Examples -&gt; Plugins, 先双击运行scala -&gt; scala:compile, 完成之后再双击运行jar -&gt; jar:jar, 可以看到编译出来了对应的 jar (在console里面注意SUCCESS之前的信息, 这个 jar 是需要在运行spark-submit的时候指定的). 当然为了能够把这两件事一起做, 更重要的是跟下一步远程调试一起做掉, 最好的方式还是写进脚本在 CLI 运行. 我们在spark-1.6.1根目录下运行mvn -pl examples scala:compile jar:jar, 告诉 maven 只在 examples 这个 submodule 下运行scala:compile和jar:jar两个 maven goals, 能够看到类似输出 1234567891011121314151617181920~/work/spark-1.6.1 mvn -pl examples scala:compile jar:jar[INFO] Scanning for projects...[INFO][INFO] ------------------------------------------------------------------------[INFO] Building Spark Project Examples 1.6.1[INFO] ------------------------------------------------------------------------[INFO][INFO] --- scala-maven-plugin:3.2.2:compile (default-cli) @ spark-examples_2.10 ---[INFO] Using zinc server for incremental compilation[info] Compile success at May 11, 2016 11:42:13 PM [0.390s][INFO][INFO] --- maven-jar-plugin:2.6:jar (default-cli) @ spark-examples_2.10 ---[INFO] Building jar: /Users/dragonly/work/spark-1.6.1/examples/target/spark-examples_2.10-1.6.1.jar[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 10.931 s[INFO] Finished at: 2016-05-11T23:42:15+08:00[INFO] Final Memory: 54M/762M[INFO] ------------------------------------------------------------------------ 可以看到我们的改动后的 examples 的代码被编译打包到了spark-1.6.1/examples/target/spark-examples_2.10-1.6.1.jar. 远程调试废话 由于 spark job 的运行需要 spark 环境, 不同于以往的单机程序, 需要借助bin/spark-submit脚本提交给 spark 去运行 (查看脚本源码可以知道, 启动前要先做一些环境检查和初始化, 然后调用 launcher 和 deploy 相关 class 去加载和运行提交的 jar). 因此断点调试起来会比较麻烦, 因为这个 server 端跟我们写的代码是两个 process (广义), 所以需要类似 gdb 的远程调试一样的功能. 原理上讲是给出一堆运行参数, 让 jvm 运行 spark 的时候, 开一个调试端口, 然后在 Intellij 这边用 debugger 远程连过去进行调试. 虽然说是”远程”, 但是在这种情况下其实就是 localhost 上开的端口而已. Intellij 创建远程调试的 Debug Configuration 进入Run -&gt; Edit Configurations, 单击左上角+, 新建一个 Remote Configuration, 名字随便改一下, 其余留作默认. 点击OK之前, 复制Configurationtab下的Command line arguments for running remote JVM里面的内容, 之后会用到这个参数, 并做如下更改 -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005改成-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005然后点OK创建这个 Configuration. 命令行 先运行mvn -pl examples scala:compile jar:jar进行编译和打包, 然后运行1234bin/spark-submit \--driver-java-options "-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005" \--class org.apache.spark.examples.SparkPi \examples/target/spark-examples_2.10-1.6.1.jar注意到第二行的参数是告诉 spark 运行的时候设置额外的 java 参数, 就是上面复制出来的参数, 改成suspend=y是告诉 spark 启动之后暂停运行, 等待 debugger 连接之后才开始运行, 方便我们加断点. 然后 CLI 会显示Listening for transport dt_socket at address: 5005, 表示 spark 正在等待 debugger 连接. 此时只需要在 Intellij 上打开SparkPi.scala文件, 加上断点, 再点Run -&gt; Debug ‘Remote’, 就能开始单步追踪调试了.]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark本地调试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git一些指令的说明]]></title>
    <url>%2Fpost%2F2016%2F10%2Fgit%E4%B8%80%E4%BA%9B%E6%8C%87%E4%BB%A4%E7%9A%84%E8%AF%B4%E6%98%8E.html</url>
    <content type="text"><![CDATA[本文记录本人在使用git的过程中，学习到的一些指令的说明。本文不定期更新… 1.本地库关联远程库，在本地仓库目录运行命令：1$ git remote add origin git@github.com:username/repository.git origin: remote name 2.git fetch和git pull的区别Git从远程的分支获取最新的版本到本地可以用fetch和pull git fetch相当于是从远程获取最新版本到本地，不会自动merge123git fetch origin mastergit diff HEAD FETCH_HEAD git merge origin/master 以上命令的含义：首先从远程origin的master分支上下载最新的版本到本地的origin/master版本记录中然后比较本地的master和刚刚fetch的origin/master的区别最后进行合并git diff HEAD FETCH_HEADHEAD是本地仓库，不是本地工作区。查看git fetch后本地仓库和远程仓库的内容有哪些区别。 git pull 1git pull origin master 上述命令其实相当于git fetch和git merge在实际使用中，git fetch更安全一些，以为在merge之前，我们可以查看更新的情况，然后再决定是否合并 关于git fetch和git pull有更详细地原理分析，请见http://blog.csdn.net/a19881029/article/details/42245955 3. git clonegit clone自动创建了一个名为origin的远程连接，指向原有仓库。 4. ~字符~字符用户表示提交的父节点的相对引用。比如：3157e~1指向3157e前一个提交，HEAD~3是当前提交的回溯3个节点的提交。 5. git reset git reset &lt;file&gt;从缓存区移除特定文件，但不改变工作目录。它会取消这个文件的缓存，而不覆盖任何更改。git reset重设当前的整个缓冲区，匹配最近的一次提交，但工作目录不变。它会取消所有文件的缓存，而不会覆盖任何本地目录的修改，给你了一个重设缓存快照的机会。git reset --hard重设缓冲区和工作目录，匹配最近一次的提交。除了取消缓存之外，--hard标记告诉Git还要重写当前工作目录中的更改。 注意：上面所有的调用都是用来移除仓库中的修改。没有--hard标记时git reset通过取消缓存或取消一系列的提交，然后重新构建提交来清理仓库。而加上--hard标记对于作了大死之后想要重头再来尤其方便。 6. git rebasegit rebase &lt;base&gt;将当前分支rebase到，这里可以是任何类型的提交引用（ID、分支名、标签，或者是HEAD的相对引用）。 rebase的主要目的是：保持一个线性的项目历史。比如说，当你在feature分支工作时master分支取得了一些进展：要将你的feature分支整合进master分支，你有两个选择：直接merge，或者先rebase后merge。前者会产生一个三路合并(3-way merge)和一个合并提交，而后者产生的是一个快速向前的合并以及完美的线性历史。下图展示了为什么rebase到master分支会促成一个快速向前的合并。rebase是将上游更改合并进本地仓库的通常方法。你每次想查看上游进展时，用git merge拉取上游更新会导致一个多余的合并提交。在另一方面，rebase就好像是说：“我想将我的更改建立在其他人的进展之上。”注意:和git commit --amend,git reset一样，永远不应该rebase那些已经推送到公共仓库的提交。rebase会用新的提交替换旧的提交，你的项目历史会像突然消失了一样。 7.git pull1git pull &lt;remote&gt; 和下面的代码起着相同的效果1234git fetch origingit log --oneline master..origin/master //查看本地分支和远程分支的区别git checkout mastergit merge origin/master拉取当前分支对应的远程副本中的更改，并立即并入本地副本。效果和git fetch后接git merge origin/.一致。1git pull --rebase &lt;remote&gt;和上一个命令相同，但是使用git rebase合并远程分支和本地分支，而不是git merge. 基于rebase的Pull--rebase标记可以用来保证线性的项目历史，防止合并提交（merge commits）的产生。很多开发者倾向于使用rebase而不是merge，因为“我想要把我的更改放在其他人完成的工作之后”。这种情况下，使用带有–rebase标记的git pull甚至更像svn update，与普通的git pull相比而言。 事实上，使用–rebase的pull的工作流是如此普遍，以致于你可以直接在配置项中设置它： 1git config --global branch.autosetuprebase always 在运行这个命令之后，所有的git pull命令将使用git rebase,而不是git merge。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
</search>
